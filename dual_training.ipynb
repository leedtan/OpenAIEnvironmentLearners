{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import gym\n",
    "import roboschool\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "GAMMA = .95\n",
    "from functools import partial\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "import pickle\n",
    "\n",
    "if 0:\n",
    "    envname = \"RoboschoolInvertedPendulum-v1\"\n",
    "    SIZE_MULT = 1\n",
    "    REWARD_MULT = 1\n",
    "    N_LAYERS = 10\n",
    "    NUM_HISTORY = N_HISTORY = 2\n",
    "else:\n",
    "    envname = \"RoboschoolHumanoidFlagrun-v1\"\n",
    "    SIZE_MULT = 2\n",
    "    REWARD_MULT = 4\n",
    "    N_LAYERS = 10\n",
    "    NUM_HISTORY = N_HISTORY = 3\n",
    "N_EPISODES = 50\n",
    "N_STEPS = 100\n",
    "BATCH_SIZE = 16\n",
    "INIT_LEN = 50\n",
    "NUM_KEEP = 100\n",
    "dbg = 0\n",
    "env = gym.make(envname)\n",
    "if dbg:\n",
    "    \n",
    "    SIZE_MULT = 1\n",
    "    N_LAYERS = 2\n",
    "    NUM_HISTORY = N_HISTORY = 2\n",
    "    N_EPISODES = 100\n",
    "    N_STEPS = 20\n",
    "    BATCH_SIZE = 16\n",
    "    INIT_LEN = 100\n",
    "    NUM_KEEP = 200\n",
    "PERCENT_CHOOSE_OPTIMAL = 2\n",
    "N_OBS, N_ACT = [44, 17]\n",
    "N_STATE = (N_OBS + N_ACT) * 2\n",
    "REWARD_IN_STATE = 0\n",
    "if REWARD_IN_STATE:\n",
    "    N_STATE += 2\n",
    "INPUT_UNITS = N_STATE * NUM_HISTORY\n",
    "STATE_DECAY = .7\n",
    "TESTING_GRAD_NORMS = 0\n",
    "ADV_ENABLED = 0\n",
    "STORED_MODELS = {}\n",
    "FCNS = {'np':{\n",
    "    'concat':np.concatenate,\n",
    "    'reshape':np.reshape,\n",
    "    'expand':np.expand_dims\n",
    "},'tf':{\n",
    "    'concat':tf.concat,\n",
    "    'reshape':tf.reshape,\n",
    "    'expand':tf.expand_dims\n",
    "}}\n",
    "\n",
    "def normalize_adv(v, mag = 1, adv_type = 0):\n",
    "    size= np.sqrt(np.prod(v.shape))\n",
    "    if adv_type == 0:\n",
    "        return mag * np.sign(v).astype(float) * .0003\n",
    "    elif adv_type == 1:\n",
    "        return mag * np.sign(v).astype(float) * .001\n",
    "    elif adv_type == 2:\n",
    "        sign_v = np.sign(v)\n",
    "        v = np.abs(v)\n",
    "        v = v / np.sqrt(np.square(v).sum())\n",
    "        return mag * v * sign_v * 10.\n",
    "    elif adv_type == 3:\n",
    "        sign_v = np.sign(v)\n",
    "        v = np.sqrt(np.abs(v))\n",
    "        v = v / np.sqrt(np.square(v).sum())\n",
    "        return mag * v * sign_v * 10.\n",
    "\n",
    "\n",
    "def apply_clipped_optimizer(opt_fcn,\n",
    "                            loss,\n",
    "                            clip_norm=.1,\n",
    "                            clip_single=.03,\n",
    "                            clip_global_norm=False,\n",
    "                            var_list=None):\n",
    "    if var_list is None:\n",
    "        gvs = opt_fcn.compute_gradients(loss)\n",
    "    else:\n",
    "        gvs = opt_fcn.compute_gradients(loss, var_list = var_list)\n",
    "        \n",
    "\n",
    "    if clip_global_norm:\n",
    "        gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "        capped_gs, grad_norm_total = tf.clip_by_global_norm([g for g in gs],clip_norm)\n",
    "        capped_gvs = list(zip(capped_gs, vs))\n",
    "    else:\n",
    "        grad_norm_total = tf.sqrt(\n",
    "                tf.reduce_sum([\n",
    "                        tf.reduce_sum(tf.square(grad)) for grad, var in gvs\n",
    "                        if grad is not None\n",
    "                ]))\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single, clip_single), var)\n",
    "                                    for grad, var in gvs if grad is not None]\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var)\n",
    "                                    for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "    optimizer = opt_fcn.apply_gradients(capped_gvs)\n",
    "\n",
    "    return optimizer, grad_norm_total\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self,\n",
    "                x, lshapes, output_units, namebase, tanh_out = False, sigmoid = False, squeeze = True,\n",
    "                reuse = False,residual = None, use_bias = True, is_train = True\n",
    "                ):\n",
    "        x = x * 10 - 5\n",
    "        self.namebase = namebase\n",
    "        self.reuse = reuse\n",
    "        self.lidx = 0\n",
    "        name_fcn = self.get_name\n",
    "        trainable = True\n",
    "        momentum = .99\n",
    "        layer = tf.layers.dense(\n",
    "            x, lshapes[0], name=self.get_name(), reuse = self.reuse\n",
    "        )\n",
    "        layer = tf.contrib.layers.layer_norm(layer)\n",
    "#         layer = tf.layers.batch_normalization(layer, training = is_train,momentum = momentum, trainable = trainable)\n",
    "        self.h = [tf.nn.crelu(layer)]\n",
    "        h2 = self.h[-1]\n",
    "        for size in lshapes:\n",
    "            new_layer = self.h[-1]\n",
    "            new_layer = tf.layers.dense(new_layer, size, name = name_fcn(), reuse = reuse)\n",
    "            new_layer = tf.contrib.layers.layer_norm(new_layer)\n",
    "#             new_layer = tf.layers.batch_normalization(new_layer, training = is_train,\n",
    "#                     momentum = momentum, name = name_fcn(), reuse = reuse, trainable = trainable)\n",
    "            new_layer = tf.nn.crelu(new_layer)\n",
    "            new_layer = tf.layers.dense(new_layer, size, name = name_fcn(), reuse = reuse)\n",
    "            new_layer = tf.contrib.layers.layer_norm(new_layer)\n",
    "#             new_layer = tf.layers.batch_normalization(new_layer, training = is_train,\n",
    "#                     momentum = momentum, name = name_fcn(), reuse = reuse, trainable = trainable)\n",
    "            new_layer = tf.nn.crelu(new_layer)\n",
    "            new_layer = tf.layers.dense(new_layer, size * 2, name = name_fcn(), reuse = reuse)\n",
    "            new_layer = tf.contrib.layers.layer_norm(new_layer)\n",
    "#             new_layer = tf.layers.batch_normalization(new_layer, training = is_train,\n",
    "#                     momentum = momentum, name = name_fcn(), reuse = reuse, trainable = trainable)\n",
    "            \n",
    "            new_layer = new_layer + self.h[-1]\n",
    "            self.h.append(tf.nn.leaky_relu(new_layer, alpha = 0.3))\n",
    "            \n",
    "        h2 = tf.concat(self.h, -1)#self.h[-1]\n",
    "        if residual is not None:\n",
    "            self.res = [residual]\n",
    "            for hidden_layer in self.h:\n",
    "                blah = tf.concat((self.res[-1], hidden_layer), -1)\n",
    "                delta = tf.layers.dense(\n",
    "                            tf.nn.leaky_relu(tf.layers.dense(\n",
    "                                hidden_layer, lshapes[0]//2, name = name_fcn(), reuse = self.reuse), alpha = 0.3), \n",
    "                            output_units, name=name_fcn(), reuse = self.reuse)\n",
    "                self.res.append(self.res[-1] + delta)\n",
    "            h2 = tf.concat((h2, self.res[-1]), -1)\n",
    "            self.h = self.res[1:]\n",
    "        output = tf.layers.dense(tf.nn.leaky_relu(tf.layers.dense(\n",
    "            h2, lshapes[0]//2, name = name_fcn(), reuse = self.reuse)), \n",
    "                                  output_units, name=name_fcn(), reuse = self.reuse,\n",
    "            bias_initializer = tf.keras.initializers.zeros())\n",
    "        if residual is not None:\n",
    "            output = self.res[-1] + output\n",
    "#         output = tf.layers.dense(\n",
    "#             h2, output_units, name=name_fcn(), reuse = self.reuse,\n",
    "#             bias_initializer = tf.keras.initializers.zeros())\n",
    "        if output_units == 1 and squeeze:\n",
    "            output = tf.squeeze(output, -1)\n",
    "        self.raw_output = output\n",
    "        if tanh_out:\n",
    "            self.output = leaky_tanh(self.raw_output)\n",
    "        elif sigmoid:\n",
    "            self.output = tf.nn.sigmoid(self.raw_output) * 2. - 0.5\n",
    "        else:\n",
    "            self.output = self.raw_output\n",
    "            \n",
    "        h3 = tf.stack(self.h, 0)\n",
    "        if residual is None:\n",
    "            output = [tf.layers.dense(tf.nn.leaky_relu(tf.layers.dense(\n",
    "                h3, lshapes[0]//2, name = name_fcn(), reuse = self.reuse)), \n",
    "                                      output_units, name=name_fcn(), reuse = self.reuse,\n",
    "                bias_initializer = tf.keras.initializers.zeros()) for h3 in self.h[:-1]]\n",
    "            output = tf.stack(output, 0)\n",
    "        else:\n",
    "            output = tf.layers.dense(tf.nn.leaky_relu(tf.layers.dense(\n",
    "                h3, lshapes[0]//2, name = name_fcn(), reuse = self.reuse)), \n",
    "                                      output_units, name=name_fcn(), reuse = self.reuse,\n",
    "                bias_initializer = tf.keras.initializers.zeros())\n",
    "        if output_units == 1 and squeeze:\n",
    "            output = tf.squeeze(output, -1)\n",
    "        if tanh_out:\n",
    "            output = leaky_tanh(output)\n",
    "        elif sigmoid:\n",
    "            output = tf.nn.sigmoid(output) * 2. - 0.5\n",
    "        else:\n",
    "            output = output\n",
    "        self.hidden_output = output\n",
    "    def get_name(self):\n",
    "        self.lidx = self.lidx + 1\n",
    "        return self.namebase + str(self.lidx)\n",
    "\n",
    "\n",
    "def make_maxmin(x):\n",
    "    y = x.copy()\n",
    "    y[maskh==0] = np.nan\n",
    "    amin, amax = np.nanmin(y, (0, 1)), np.nanmax(y, (0,1))\n",
    "    return amin, amax\n",
    "\n",
    "def make_state(state, action, style = 'np'):\n",
    "    together = FCNS[style]['concat']((state, action), -1)\n",
    "    if style == 'tf':\n",
    "        return together\n",
    "#         expanded = FCNS[style]['expand'](\n",
    "#             together, 1)\n",
    "    else:\n",
    "        expanded = FCNS[style]['expand'](\n",
    "            together, 0)\n",
    "    return expanded\n",
    "    return FCNS[style]['reshape'](\n",
    "        together, (state.shape[0], -1, state.shape[-1] + action.shape[-1]))\n",
    "        \n",
    "    \n",
    "\n",
    "def accumulate_state(state, action, old_state, statedecay, style = 'np'):\n",
    "    new_state = make_state(state, action, style)\n",
    "    new_state_len = new_state.shape[-1]\n",
    "    if style == 'tf':\n",
    "        vel = new_state - old_state[:,:,0,:new_state_len]\n",
    "    else:\n",
    "        vel = new_state - old_state[0,:new_state_len]\n",
    "    new_state = FCNS[style]['concat']((new_state, vel), -1)\n",
    "    if style == 'tf':\n",
    "        return FCNS[style]['concat']((tf.expand_dims(new_state, 2), old_state[:,:,:-1,:]*statedecay), 2)\n",
    "    return FCNS[style]['concat']((new_state, old_state[:-1,:]*statedecay), 0)\n",
    "\n",
    "def leaky_tanh(x):\n",
    "    return tf.log(tf.abs(x)+1) * tf.sign(x)*.7\n",
    "    #return tf.nn.tanh(x*30)/10 + tf.nn.tanh(x*2)/2 + tf.nn.tanh(x/20) * 2 + x * 1e-3\n",
    "\n",
    "\n",
    "def get_grad_norm(optimizer, loss, optname):\n",
    "    gvs = optimizer.compute_gradients(loss)\n",
    "    grad_norm = tf.reduce_mean(\n",
    "        [tf.reduce_mean(tf.square(grad)) for\n",
    "         grad, var in gvs if grad is not None and optname in var.name])\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "\n",
    "class PolicyLearner(object):\n",
    "    def __init__(self, ob_space, ac_space, take_weights_here=None, \n",
    "                 lshapes = [32 * SIZE_MULT] * N_LAYERS, config = None, \n",
    "                 lshapes_big = [64 * SIZE_MULT] * N_LAYERS,\n",
    "                 lshapes_small = [16 * SIZE_MULT] * N_LAYERS,\n",
    "                reuse = False):\n",
    "        self.sess = tf.InteractiveSession(config=config)\n",
    "        self.returns = tf.placeholder(tf.float32, (None, None))\n",
    "        self.returnsdecayed = tf.placeholder(tf.float32, (None, None))\n",
    "        self.mask = tf.placeholder(tf.float32, (None, None))\n",
    "        self.lr = tf.placeholder_with_default(1e-3, (None))\n",
    "        self.statesraw = tf.placeholder(tf.float32, (None, None, N_OBS))\n",
    "        self.is_train = tf.placeholder_with_default(True, (None))\n",
    "        self.is_exploit = tf.placeholder(tf.float32, (None))\n",
    "        self.is_exploit2d = tf.expand_dims(self.is_exploit, -1)\n",
    "        self.is_exploit3d = tf.expand_dims(self.is_exploit2d, -1)\n",
    "        self.statesraw_expanded = tf.expand_dims(self.statesraw, -1)\n",
    "        self.maskexpanded = tf.expand_dims(self.mask, -1)\n",
    "        self.maskexpanded2 = tf.expand_dims(self.maskexpanded, -1)\n",
    "        self.obs = tf.placeholder(tf.float32, (None, None, INPUT_UNITS))\n",
    "        self.past_actions = tf.placeholder(tf.float32, (None, None, N_ACT))\n",
    "        self.past_states = tf.concat((tf.zeros_like(self.statesraw[:,:1,:]), self.statesraw[:,:-1,:]), 1)\n",
    "        is_train = self.is_train\n",
    "        self.actor = MLP(\n",
    "            self.obs, lshapes, N_ACT, 'a_', squeeze = False, reuse = reuse,\n",
    "            tanh_out = True, is_train = is_train, residual = self.past_actions)\n",
    "        self.explorer = MLP(\n",
    "            self.obs, lshapes, N_ACT, 'e_', squeeze = False, reuse = reuse,\n",
    "            tanh_out = True, is_train = is_train, residual = self.past_actions)\n",
    "        self.actions = self.actor.output\n",
    "        self.explorer_actions = self.explorer.output\n",
    "        self.state_value_estimator = MLP(\n",
    "            self.statesraw, lshapes, 1, 'v_', reuse = reuse, sigmoid = False, is_train = is_train)\n",
    "        self.state_value_estimate = self.state_value_estimator.output\n",
    "        self.advantage = ((\n",
    "            self.state_value_estimate[:,1:] * GAMMA + self.returns[:,:-1]) -\n",
    "            self.state_value_estimate[:,:-1])\n",
    "        \n",
    "        \n",
    "        self.actions_for_critic = tf.nn.tanh(self.actions * 10) + self.actions\n",
    "        self.explorer_actions_for_critic = tf.nn.tanh(self.explorer_actions * 10) + self.explorer_actions\n",
    "        self.critic_input = tf.concat((self.obs, self.actions_for_critic), -1)\n",
    "        self.explorer_critic_input = tf.concat((self.obs, self.explorer_actions_for_critic), -1)\n",
    "        \n",
    "        \n",
    "        self.advantage_estimator = MLP(\n",
    "            self.critic_input, lshapes, 1, 'c_', reuse = reuse, is_train = is_train)\n",
    "        self.model_estimator = MLP(\n",
    "            self.critic_input, lshapes_small, N_OBS, 'm_', reuse = reuse,\n",
    "            sigmoid = False, is_train = is_train, residual = self.past_states\n",
    "        )\n",
    "        self.explorer_advantage_estimator = MLP(\n",
    "            self.explorer_critic_input, lshapes, 1, 'c_', reuse = True, is_train = is_train)\n",
    "        self.explorer_model_estimator = MLP(\n",
    "            self.explorer_critic_input, lshapes_small, N_OBS, 'm_', reuse = True,\n",
    "            sigmoid = False, is_train = is_train, residual = self.past_states\n",
    "        )\n",
    "        self.advantage_estimate = self.advantage_estimator.output\n",
    "        self.explorer_advantage_estimate = self.explorer_advantage_estimator.output\n",
    "        self.model_estimate = self.model_estimator.output\n",
    "        \n",
    "        self.loss_input = tf.concat((\n",
    "            self.critic_input, self.model_estimator.output, \n",
    "            tf.expand_dims(self.advantage_estimator.output, -1)), -1)\n",
    "        self.loss_estimator = MLP(\n",
    "            self.loss_input, lshapes, 2, 'l_', reuse = reuse, is_train = is_train)\n",
    "        \n",
    "        self.explorer_loss_input = tf.concat((\n",
    "            self.explorer_critic_input, self.explorer_model_estimator.output, \n",
    "            tf.expand_dims(self.explorer_advantage_estimator.output, -1)), -1)\n",
    "        self.explorer_loss_estimator = MLP(\n",
    "            self.explorer_loss_input, lshapes, 2, 'l_', reuse = True, is_train = is_train)\n",
    "        \n",
    "        self.future_value = MLP(\n",
    "            self.model_estimate, lshapes, 1, 'v_', reuse = True, sigmoid = False, is_train = is_train)\n",
    "        \n",
    "        self.t_vars = tf.trainable_variables()\n",
    "        self.a_vars = [var for var in self.t_vars if 'a_' in var.name]\n",
    "        self.v_vars = [var for var in self.t_vars if 'v_' in var.name]\n",
    "        self.c_vars = [var for var in self.t_vars if 'c_' in var.name]\n",
    "        self.e_vars = [var for var in self.t_vars if 'e_' in var.name]\n",
    "        self.m_vars = [var for var in self.t_vars if 'm_' in var.name]\n",
    "        self.l_vars = [var for var in self.t_vars if 'l_' in var.name]\n",
    "        \n",
    "        self.creg, self.areg, self.vreg, self.mreg, self.ereg = [\n",
    "            tf.reduce_mean([tf.reduce_mean(tf.square(v)) for v in optvars]) * 1e-8\n",
    "            for optvars in \n",
    "            [self.c_vars, self.a_vars, self.v_vars, self.m_vars, self.e_vars]]\n",
    "        \n",
    "        self.frac_not_masked = tf.reduce_mean(self.mask)\n",
    "        \n",
    "\n",
    "        self.model_state_estimate = self.model_estimator.output\n",
    "        \n",
    "        \n",
    "        self.a_loss_critic = -tf.reduce_sum(\n",
    "            self.advantage_estimator.output * self.mask)/self.frac_not_masked\n",
    "        self.a_loss_model = -tf.reduce_sum(\n",
    "            self.future_value.output * self.mask)/self.frac_not_masked\n",
    "        \n",
    "        self.v_loss_ind = tf.square(\n",
    "            self.returnsdecayed - self.state_value_estimate) * self.mask * self.is_exploit2d\n",
    "        self.v_loss_raw = tf.reduce_mean(self.v_loss_ind) / tf.reduce_mean(\n",
    "            self.mask * self.is_exploit2d) * 1e1\n",
    "        \n",
    "        self.m_loss_ind = tf.square(\n",
    "            self.model_estimator.output[:,:-1] - self.statesraw[:,1:]\n",
    "        ) * self.maskexpanded[:,:-1]\n",
    "        self.m_loss_raw = tf.reduce_mean(self.m_loss_ind)\n",
    "        \n",
    "        self.e_m_loss_ind = tf.square(\n",
    "            self.explorer_model_estimator.output[:,:-1] - self.statesraw[:,1:]\n",
    "        ) * self.maskexpanded[:,:-1]\n",
    "        \n",
    "        self.e_c_loss_ind = tf.square(\n",
    "            self.explorer_advantage_estimate[:,:-1] - self.advantage) * self.mask[:,:-1]\n",
    "        self.e_c_loss_raw = tf.reduce_mean(self.e_c_loss_ind) * 1e1\n",
    "        \n",
    "        self.e_l_loss = tf.reduce_mean(\n",
    "            tf.sqrt(1e-4 + tf.abs(\n",
    "                self.explorer_loss_estimator.output[:,:-1,0] - tf.reduce_mean(self.e_m_loss_ind, -1))) *\n",
    "            self.mask[:,:-1]\n",
    "        ) + tf.reduce_mean(\n",
    "            tf.sqrt(1e-4 + tf.abs(\n",
    "                self.explorer_loss_estimator.output[:,:-1,1] - self.e_c_loss_ind\n",
    "            )) * self.mask[:,:-1]\n",
    "        )\n",
    "        \n",
    "        self.c_loss_ind = tf.square(\n",
    "            self.advantage_estimate[:,:-1] - self.advantage) * self.mask[:,:-1]\n",
    "        self.c_loss_raw = tf.reduce_mean(self.c_loss_ind) * 1e1\n",
    "        \n",
    "        self.l_loss = tf.reduce_mean(\n",
    "            tf.square(self.loss_estimator.output[:,:-1,0] - tf.reduce_mean(self.m_loss_ind, -1)) *\n",
    "            self.mask[:,:-1]\n",
    "        ) + tf.reduce_mean(\n",
    "            tf.square(\n",
    "                self.loss_estimator.output[:,:-1,1] - self.c_loss_ind\n",
    "            ) * self.mask[:,:-1]\n",
    "        )\n",
    "        \n",
    "        self.e_loss_critic = -tf.reduce_sum(\n",
    "            self.explorer_advantage_estimator.output * self.mask)/self.frac_not_masked\n",
    "        self.e_loss_loss = -self.l_loss * 10\n",
    "        \n",
    "        self.a_meanaction = tf.reduce_mean(\n",
    "            self.actions * self.maskexpanded, [0, 1])/self.frac_not_masked\n",
    "        self.a_meanactionloss = tf.reduce_mean(\n",
    "            tf.square(self.a_meanaction)\n",
    "        ) * 1e5 + tf.reduce_mean(\n",
    "            tf.square(tf.square(self.a_meanaction))\n",
    "        ) * 1e2 + tf.reduce_mean(\n",
    "            tf.abs(self.a_meanaction)\n",
    "        ) * 1e0\n",
    "        \n",
    "        self.e_meanaction = tf.reduce_mean(\n",
    "            self.explorer_actions * self.maskexpanded, [0, 1])/self.frac_not_masked\n",
    "        self.e_meanactionloss = tf.reduce_mean(\n",
    "            tf.square(self.e_meanaction)\n",
    "        ) * 1e5 + tf.reduce_mean(\n",
    "            tf.square(tf.square(self.e_meanaction))\n",
    "        ) * 1e2 + tf.reduce_mean(\n",
    "            tf.abs(self.e_meanaction)\n",
    "        ) * 1e0\n",
    "        \n",
    "        \n",
    "        self.a_mse = tf.reduce_mean((\n",
    "            tf.square(self.actor.raw_output) * 1e2 + tf.square(tf.square(self.actor.raw_output)) * 1e1\n",
    "            ) * self.maskexpanded)/tf.reduce_mean(self.maskexpanded)\n",
    "        \n",
    "        \n",
    "        self.e_mse = tf.reduce_mean((\n",
    "            tf.square(self.explorer.raw_output) * 1e2 + tf.square(tf.square(self.explorer.raw_output)) * 1e1\n",
    "            ) * self.maskexpanded)/tf.reduce_mean(self.maskexpanded)\n",
    "        \n",
    "        if 0:\n",
    "            optimizer = tf.train.RMSPropOptimizer\n",
    "            self.critic_opt = optimizer(self.lr, momentum= .8)\n",
    "            self.value_opt = optimizer(self.lr, momentum= .8)\n",
    "            self.actor_opt = optimizer(self.lr, momentum= .8)\n",
    "            self.model_opt = optimizer(self.lr, momentum= .8)\n",
    "            self.explorer_opt = optimizer(self.lr, momentum= .8)\n",
    "            self.loss_opt = optimizer(self.lr, momentum= .8)\n",
    "        else:\n",
    "            self.opt = tf.train.AdamOptimizer(self.lr, beta1= .8)\n",
    "        \n",
    "        \n",
    "        self.v_loss =  self.v_loss_raw + self.vreg\n",
    "        self.m_loss = self.m_loss_raw + self.mreg\n",
    "        self.c_loss =  self.c_loss_raw + self.creg\n",
    "        self.aregtotal = self.areg + self.a_mse + self.a_meanactionloss\n",
    "        self.a_loss_raw = self.a_loss_critic + self.a_loss_model\n",
    "        self.a_loss = self.a_loss_raw + self.aregtotal\n",
    "        self.e_loss = self.e_loss_critic - self.e_l_loss\n",
    "        self.l_loss_minimize = self.l_loss\n",
    "        self.e_loss_minimize = self.e_loss + \\\n",
    "            self.e_mse + self.ereg + self.e_meanactionloss\n",
    "        self.a_loss_minimize = self.a_loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.c_loss_minimize = self.c_loss + tf.reduce_mean(\n",
    "            tf.square(self.advantage_estimator.hidden_output[:,:,:-1] - tf.expand_dims(self.advantage, 0)\n",
    "                 )) * tf.expand_dims(self.mask[:,:-1], 0) * tf.reshape(\n",
    "            tf.range(N_LAYERS, dtype = tf.float32)/(N_LAYERS * 2) + .1, (-1, 1, 1))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.v_loss_minimize = self.v_loss + tf.reduce_mean(\n",
    "            tf.square(self.state_value_estimator.hidden_output - tf.expand_dims(self.returnsdecayed, 0)\n",
    "                 )) * tf.expand_dims(self.mask, 0) * tf.expand_dims(self.is_exploit2d, 0) * tf.reshape(\n",
    "            tf.range(N_LAYERS, dtype = tf.float32)/(N_LAYERS * 2) + .1, (-1, 1, 1))\n",
    "        \n",
    "        self.m_loss_minimize = self.m_loss + tf.reduce_mean(\n",
    "            tf.square(self.model_estimator.hidden_output[:,:,:-1] - tf.expand_dims(self.statesraw[:,1:], 0)\n",
    "                 )) * tf.expand_dims(self.maskexpanded[:,:-1], 0) * tf.reshape(\n",
    "            tf.range(N_LAYERS, dtype = tf.float32)/(N_LAYERS * 2) + .1, (-1, 1, 1, 1))\n",
    "        # + self.grad_norm_m + self.grad_m_1\n",
    "        \n",
    "        \n",
    "        self.v_obs_grad = tf.gradients(self.v_loss_minimize, self.statesraw)\n",
    "        self.c_obs_grad = tf.gradients(self.c_loss_minimize, self.critic_input)\n",
    "        self.m_obs_grad = tf.gradients(self.m_loss_minimize, self.critic_input)\n",
    "        self.a_obs_grad = tf.gradients(self.a_loss_minimize, self.obs)\n",
    "        self.e_obs_grad = tf.gradients(self.e_loss_minimize, self.obs)\n",
    "        self.l_obs_grad = tf.gradients(self.l_loss_minimize, self.loss_input)\n",
    "        \n",
    "        if TESTING_GRAD_NORMS:\n",
    "            self.a_grads = [\n",
    "                get_grad_norm(self.actor_opt, l) for l in [\n",
    "                self.a_loss_minimize, self.a_loss_raw, self.a_loss_critic,self.a_loss_model,\n",
    "                self.a_loss_secondaction,\n",
    "                self.areg, self.aregmeanbyaction, self.actstdpenalty, self.a_mse\n",
    "            ]]\n",
    "            self.v_grads = [get_grad_norm(self.value_opt, l) for l in [\n",
    "                self.v_loss_minimize, self.v_loss_raw, \n",
    "                self.vreg, self.grad_norm_v\n",
    "            ]]\n",
    "            self.c_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "                self.c_loss_minimize, self.c_loss_raw, \n",
    "                self.creg, self.grad_norm_c, self.grad_c_1\n",
    "\n",
    "            ]]\n",
    "            self.m_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "                self.m_loss_minimize, self.m_loss_raw, \n",
    "                self.mreg, self.grad_norm_m, self.grad_m_1\n",
    "            ]]\n",
    "        \n",
    "        self.copt, self.c_norm = apply_clipped_optimizer(\n",
    "            self.opt, self.c_loss_minimize, var_list = self.c_vars)\n",
    "        self.vopt, self.v_norm = apply_clipped_optimizer(\n",
    "            self.opt, self.v_loss_minimize, var_list = self.v_vars)\n",
    "        self.aopt, self.a_norm = apply_clipped_optimizer(\n",
    "            self.opt, self.a_loss_minimize, var_list = self.a_vars)\n",
    "        self.mopt, self.m_norm = apply_clipped_optimizer(\n",
    "            self.opt, self.m_loss_minimize, var_list = self.m_vars)\n",
    "        self.eopt, self.e_norm = apply_clipped_optimizer(\n",
    "            self.opt, self.e_loss_minimize, var_list = self.e_vars)\n",
    "        self.lopt, self.l_norm = apply_clipped_optimizer(\n",
    "            self.opt, self.l_loss_minimize, var_list = self.l_vars)\n",
    "\n",
    "    def load_weights(self):\n",
    "        feed_dict = {}\n",
    "        for (var, w), ph in zip(\n",
    "            self.assigns, self.weight_assignment_placeholders):\n",
    "            feed_dict[ph] = w\n",
    "        self.sess.run(self.weight_assignment_nodes, feed_dict=feed_dict)\n",
    "\n",
    "    def act(self, obs, past_action, exploit = True):\n",
    "        # Because we need batch dimension, \n",
    "        # data[None] changes shape from [A] to [1,A]\n",
    "        if exploit:\n",
    "            act = self.actions\n",
    "            adv = self.advantage_estimate\n",
    "        else:\n",
    "            act = self.explorer_actions\n",
    "            adv = self.explorer_advantage_estimate\n",
    "        obs = np.reshape(obs, (1, 1, -1))\n",
    "        a = self.sess.run(\n",
    "            act, feed_dict={\n",
    "                self.obs:obs,\n",
    "                self.is_train:False,\n",
    "                self.past_actions:past_action\n",
    "            })\n",
    "        a = np.tile(a, [16, 1, 1])\n",
    "        a = a * (np.random.randn(*a.shape) * .01 + 1)\n",
    "        x, y = np.random.choice(16), np.random.choice(a.shape[-1])\n",
    "        a[x, 0, y] = 0\n",
    "        x, y = np.random.choice(16), np.random.choice(a.shape[-1])\n",
    "        a[x, 0, y] = a[x, 0, y] * -1\n",
    "        obs = obs\n",
    "        obs = np.tile(obs, [16, 1, 1])\n",
    "        values = self.sess.run(adv, feed_dict = {\n",
    "            act: a,\n",
    "            self.obs:obs,\n",
    "            self.is_train:False\n",
    "        })\n",
    "        \n",
    "        return a[np.argmax(values),0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_feed_dict(batch_size, forced_hist, sample_flat):\n",
    "    num_hist = ah.shape[0]\n",
    "    if num_hist >  batch_size:\n",
    "        if sample_flat:\n",
    "            probability = np.ones(maskh.shape[0])\n",
    "        else:\n",
    "            probability = np.square(np.arange(maskh.shape[0])/1000 + .01)\n",
    "        if forced_hist:\n",
    "            probability = probability[:-forced_hist]\n",
    "        probability = np.square(probability)\n",
    "        probability = probability / probability.sum()\n",
    "        rnd = np.random.choice(\n",
    "                num_hist - forced_hist, batch_size - forced_hist, \n",
    "                replace=False, p=probability)\n",
    "        if forced_hist:\n",
    "            samples = np.concatenate((rnd,\n",
    "                np.arange( num_hist - forced_hist, num_hist)))\n",
    "        else:\n",
    "            samples=rnd\n",
    "    else:\n",
    "        samples = np.random.choice(num_hist, num_hist, replace=False)\n",
    "    actions, states, statesraw, returns, returnsdecayed, mask, exploit = [\n",
    "        v[samples] for v in [ah, sh, shraw, rh,rdecayedh, maskh, np.array(isexploit)]]\n",
    "    returns = returns\n",
    "    statesrawactive = statesraw\n",
    "    rd = returnsdecayed\n",
    "    obs = states\n",
    "\n",
    "    returnsmasked, rdmasked = [\n",
    "        v[mask.astype(bool)] for v in \n",
    "    [returns, rd]]\n",
    "\n",
    "    statesmasked, obsmasked = [\n",
    "        v[np.tile(np.expand_dims(mask, -1), [1,1,v.shape[-1]]).astype(bool)] for v in \n",
    "    [statesrawactive, obs]]\n",
    "\n",
    "    feed_dict={\n",
    "                self.returns:returns,\n",
    "                self.statesraw: statesrawactive,\n",
    "                self.returnsdecayed:rd,\n",
    "                self.lr: .001,# / np.power(ep + 20, .4),\n",
    "                self.mask:mask,\n",
    "                self.obs: obs,\n",
    "                self.is_exploit:exploit + 0.1,\n",
    "                self.actions:actions,\n",
    "                self.past_actions : np.concatenate((np.zeros_like(actions[:,:1,:]), actions[:,:-1,:]), 1)\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "import copy\n",
    "class Trainer:\n",
    "    def __init__(self, opt, grad, loss, inpt, pi, name = ''):\n",
    "        self.opt = opt\n",
    "        self.grad = grad\n",
    "        self.loss = loss\n",
    "        self.inpt = inpt\n",
    "        self.steps = 0\n",
    "        self.loss_mean = [0]\n",
    "        self.loss_pre_mean = [0]\n",
    "        self.diff_loss_mean = [0]\n",
    "        self.name = name\n",
    "        self.pi = pi\n",
    "        self.adv = 1\n",
    "    def step(self, feed_dict):\n",
    "        if self.name in ['a', 'e']:\n",
    "            del feed_dict[pi.actions]\n",
    "        if (self.name not in ['c', 'm', 'a', 'e']) or self.adv:\n",
    "            feed_dict_start = copy.copy(feed_dict)\n",
    "            grad, loss_pre, inpt = sess.run([self.grad, self.loss, self.inpt], feed_dict_start)\n",
    "            n_grad = normalize_adv(grad[0], mag = 1., adv_type = np.random.choice(4))\n",
    "            feed_dict[self.inpt] = inpt + n_grad\n",
    "        else:\n",
    "            feed_dict_start = copy.copy(feed_dict)\n",
    "            grad, loss_pre, inpt = sess.run([self.grad, self.loss, self.inpt], feed_dict_start)\n",
    "            feed_dict[self.inpt] = inpt + np.random.randn(*grad[0].shape) * 1e-4\n",
    "        _, loss = sess.run([self.opt, self.loss], feed_dict)\n",
    "        diff = loss - loss_pre\n",
    "        self.track_losses(loss, loss_pre, diff)\n",
    "        if self.steps % 10 == 9:\n",
    "            print(self.name, 'loss', np.mean(self.loss_mean).round(5), \n",
    "                  'loss_pre', np.mean(self.loss_pre_mean).round(5), \n",
    "                  'loss_diff', np.mean(self.diff_loss_mean).round(5), 'adv ', {0: 'disabled', 1:'enabled'}[self.adv])\n",
    "            if self.name in ['a', 'e']:\n",
    "                actions = sess.run({'a':pi.actions, 'e':pi.explorer_actions}[self.name], feed_dict)\n",
    "                actions_filterred = actions[np.tile(np.expand_dims(\n",
    "                    feed_dict[pi.mask], -1), [1,1,actions.shape[-1]]).astype(bool)]\n",
    "                print(self.name, 'mean', actions_filterred.mean(), \n",
    "                      'abs mean', np.abs(actions_filterred).mean(),\n",
    "                     'std', actions_filterred.std()\n",
    "                     )\n",
    "            if np.random.rand() > -2:\n",
    "                self.adv = 1\n",
    "            else:\n",
    "                self.adv = 0\n",
    "        self.steps += 1\n",
    "        return loss, loss_pre\n",
    "    def track_losses(self, loss, loss_pre, diff):\n",
    "        self.loss_mean.append(loss)\n",
    "        self.loss_pre_mean.append(loss_pre)\n",
    "        self.diff_loss_mean.append(diff)\n",
    "        self.loss_mean, self.loss_pre_mean, self.diff_loss_mean = [\n",
    "            v[-10:] for v in [self.loss_mean, self.loss_pre_mean, self.diff_loss_mean]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    inter_op_parallelism_threads=0,\n",
    "    intra_op_parallelism_threads=0,\n",
    "    device_count = { \"GPU\": 0 } )\n",
    "tf.reset_default_graph()\n",
    "\n",
    "pi = PolicyLearner(44, 17, config = None)\n",
    "\n",
    "sess = pi.sess\n",
    "self = pi\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-100, 100, .01)\n",
    "\n",
    "y = np.log(np.abs(x)+1) * np.sign(x)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ah, sh, shraw, rh, rdecayedh, maskh = [\n",
    "    np.zeros((0, 0, i)) for i in [N_ACT, INPUT_UNITS, N_OBS, 1, 1, 1]]\n",
    "isexploit = []\n",
    "globalframes = []\n",
    "localframes = []\n",
    "ep = 0\n",
    "trained = 0\n",
    "ongoing = 0\n",
    "exploit = 0\n",
    "printfreq = 20\n",
    "obj_fname = envname + str(PERCENT_CHOOSE_OPTIMAL) + 'saveobjs_unguided.pkl'\n",
    "tffile = \"tmp/\" + envname + str(PERCENT_CHOOSE_OPTIMAL) + \"unguided_trained.ckpt\"\n",
    "import pickle\n",
    "last_epoch_saved = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ongoing = 0\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = Trainer(pi.mopt, pi.m_obs_grad, pi.m_loss, pi.critic_input, pi, name = 'm')\n",
    "critic_trainer = Trainer(pi.copt, pi.c_obs_grad, pi.c_loss, pi.critic_input, pi, name = 'c')\n",
    "loss_trainer = Trainer(pi.lopt, pi.l_obs_grad, pi.l_loss, pi.loss_input, pi, name = 'l')\n",
    "value_trainer = Trainer(pi.vopt, pi.v_obs_grad, pi.v_loss, pi.statesraw, pi, name = 'v')\n",
    "actor_trainer = Trainer(pi.aopt, pi.a_obs_grad, pi.a_loss, pi.obs, pi, name = 'a')\n",
    "explorer_trainer = Trainer(pi.eopt, pi.e_obs_grad, pi.e_loss_minimize, pi.obs, pi, name = 'e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all(batch_size = None, forced_hist = 0, sample_flat = True, n_steps = None):\n",
    "    if n_steps is None:\n",
    "        n_steps = N_STEPS\n",
    "    if batch_size is None:\n",
    "        batch_size = BATCH_SIZE\n",
    "    for mdl in [model_trainer, value_trainer, critic_trainer, loss_trainer, actor_trainer, explorer_trainer]:\n",
    "        for _ in range(n_steps):\n",
    "            fd= get_feed_dict(batch_size, forced_hist, sample_flat)\n",
    "            mdl.step(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = 0\n",
    "ongoing = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trained, ongoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ongoing:\n",
    "    \n",
    "    save_path = saver.save(sess, tffile)\n",
    "    print('saved at epoch', ep)\n",
    "    with open(obj_fname,\"wb\") as f:\n",
    "        pickle.dump(\n",
    "            [ah, sh, shraw, rh, rdecayedh, maskh, ep, globalframes, isexploit,\n",
    "             shrawmin_g_e, shrawmax_g_e,shmin_g_e, shmax_g_e, vmin_g_e, vmax_g_e, shrawdiff, shdiff, vdiff\n",
    "\n",
    "            ], f)\n",
    "    trained = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAVE_ALONE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tffile = 'trainedoffline.ckpt'\n",
    "\n",
    "tfdir = 'tf_savefiles/'\n",
    "\n",
    "if not os.path.isdir(tfdir):\n",
    "    os.mkdir(tfdir)\n",
    "saver.restore(sess, os.path.join(tfdir, tffile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tffile = 'trainedoffline_retrained.ckpt'\n",
    "\n",
    "tfdir = 'tf_savefiles_retrained/'\n",
    "tffile = os.path.join(tfdir, 'trainedoffline_retrained.ckpt')\n",
    "\n",
    "if not os.path.isdir(tfdir):\n",
    "    os.mkdir(tfdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save():\n",
    "    print('saved at epoch', ep)\n",
    "    cur_obj_fname = 'objects2_at' + \"{:010d}\".format(int(ep)) + '.pkl'\n",
    "    with open(cur_obj_fname,\"wb\") as f:\n",
    "        pickle.dump(\n",
    "            [ah, sh, shraw,\n",
    "             rh, rdecayedh, maskh, ep, globalframes, isexploit, \n",
    "            shrawmin_g_e, shrawmax_g_e,shmin_g_e, shmax_g_e, vmin_g_e, \n",
    "            vmax_g_e, shrawdiff, shdiff, vdiff\n",
    "            ], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if not LEAVE_ALONE:\n",
    "    [[shrawmin_g, shrawmax_g],\n",
    "        [shmin_g, shmax_g],\n",
    "        [vmin_g, vmax_g]] = [[\n",
    "        np.ones(i) * 1000, np.zeros(i) * -1000] for i in [N_OBS, N_STATE * N_HISTORY, 1]]\n",
    "    if trained:\n",
    "        try:\n",
    "            saver.restore(sess, tffile)\n",
    "            with open(obj_fname, \"rb\") as f:\n",
    "                ah, sh, shraw, rh, rdecayedh, maskh, ep, globalframes, isexploit, \\\n",
    "                    shrawmin_g_e, shrawmax_g_e,shmin_g_e, shmax_g_e, vmin_g_e, \\\n",
    "                    vmax_g_e, shrawdiff, shdiff, vdiff = pickle.load(f)\n",
    "            print('restored from save file')\n",
    "            reinit = 1\n",
    "            last_epoch_saved = ep\n",
    "        except:\n",
    "            print('no save file detected')\n",
    "            trained = 0\n",
    "            reinit = 0\n",
    "    else:\n",
    "\n",
    "        reinit = 0\n",
    "        [[shrawmin_g, shrawmax_g],\n",
    "            [shmin_g, shmax_g],\n",
    "            [vmin_g, vmax_g]] = [[\n",
    "            np.ones(i) * 1000, np.zeros(i) * -1000] for i in [N_OBS, N_STATE * N_HISTORY, 1]]\n",
    "MAX_SEQ_LEN = 5000\n",
    "for ep in range(ep, 10000000):\n",
    "    if ep % 1000 == 0 and trained and ep > 0:\n",
    "        save_path = saver.save(sess, tffile)\n",
    "        print('saved at epoch', ep)\n",
    "        with open(obj_fname,\"wb\") as f:\n",
    "            pickle.dump(\n",
    "                [ah[-NUM_KEEP:], sh[-NUM_KEEP:], shraw[-NUM_KEEP:],\n",
    "                 rh[-NUM_KEEP:], rdecayedh[-NUM_KEEP:], maskh[-NUM_KEEP:], ep, globalframes, isexploit[-NUM_KEEP:], \n",
    "                shrawmin_g_e, shrawmax_g_e,shmin_g_e, shmax_g_e, vmin_g_e, \n",
    "                vmax_g_e, shrawdiff, shdiff, vdiff\n",
    "                ], f)\n",
    "    if ep < 10000 and ep > 1:\n",
    "        reinit = 0\n",
    "        shrawmin, shrawmax = make_maxmin(shraw)\n",
    "        shmin, shmax = make_maxmin(sh)\n",
    "        vmin, vmax = make_maxmin(rdecayedh)\n",
    "        shrawmin_g, shmin_g, vmin_g = [\n",
    "            np.minimum(g, n) for g, n in zip(\n",
    "                [shrawmin_g, shmin_g, vmin_g], [shrawmin, shmin, vmin])]\n",
    "        shrawmax_g, shmax_g, vmax_g = [\n",
    "            np.maximum(g, n) for g, n in zip(\n",
    "                [shrawmax_g, shmax_g, vmax_g], [shrawmax, shmax, vmax])]\n",
    "\n",
    "        [shrawmin_g_e, shrawmax_g_e,shmin_g_e, shmax_g_e] = [\n",
    "            np.reshape(v, (1, 1, -1)) for v in [shrawmin_g, shrawmax_g,shmin_g, shmax_g]]\n",
    "        vmin_g_e, vmax_g_e = [np.reshape(v, (1, 1)) for v in [vmin_g, vmax_g]]\n",
    "\n",
    "        shrawdiff = shrawmax_g_e - shrawmin_g_e\n",
    "        shdiff = shmax_g_e - shmin_g_e\n",
    "        vdiff = vmax_g_e - vmin_g_e\n",
    "    if ep % NUM_KEEP == 0 and ep > last_epoch_saved + 10:\n",
    "        save()\n",
    "    trained = 1\n",
    "    ongoing = 1\n",
    "    \n",
    "    an, sn, snraw, rn, rdecayedn, maskn = [\n",
    "        np.zeros((0, i)) for i in [N_ACT, INPUT_UNITS, N_OBS, 1, 1, 1]]\n",
    "    frame = 0\n",
    "    score = 0\n",
    "    restart_delay = 5\n",
    "    obs = env.reset()\n",
    "    obsraw = obs\n",
    "    snraw = np.concatenate((snraw, obs.reshape(1, -1)), 0)\n",
    "    obs = np.concatenate((obs, np.zeros(N_ACT)))\n",
    "    obs = np.concatenate((obs, np.zeros_like(obs)))\n",
    "    obs_mat = np.concatenate((\n",
    "        obs[None,:],np.zeros((NUM_HISTORY-1, N_STATE))), 0)\n",
    "    rn = [0]\n",
    "    done_ctr = 0\n",
    "    sn = np.concatenate((sn, obs_mat.reshape(1, -1)), 0)\n",
    "    step_num = 0\n",
    "    step_shifted = -1\n",
    "    show_shift = 0\n",
    "    if exploit or ep < 5:\n",
    "        exploit = 0\n",
    "    else:\n",
    "        exploit = 1\n",
    "#         if np.random.rand() > .5:\n",
    "#             exploit = 0\n",
    "    explore_type_random = np.random.rand() < 0.5\n",
    "    #print({0:'exploring', 1:'acting'}[exploit])\n",
    "    explore_random_mag = np.random.rand() * .5 + 1e-6\n",
    "    if not exploit:\n",
    "        pass#print('explore type ', {0:'focused', 1:'random'}[explore_type_random])\n",
    "    while 1:\n",
    "        step_num += 1\n",
    "        if ep < N_EPISODES:\n",
    "            a = np.random.randn(N_ACT) * explore_random_mag\n",
    "        else:\n",
    "            if exploit:\n",
    "                if np.random.rand() < .9:\n",
    "                    a = pi.act(obs_mat.flatten(), exploit = exploit, past_action = np.reshape(a, (1, 1, -1)))\n",
    "                elif np.random.rand() < .5:\n",
    "                    a = pi.act(obs_mat.flatten(), exploit = 0, past_action = np.reshape(a, (1, 1, -1)))\n",
    "                else:\n",
    "                    a = np.random.randn(N_ACT) * 1. * explore_random_mag\n",
    "            else:\n",
    "                if np.random.rand() < .5 and not explore_type_random:\n",
    "                    a = pi.act(obs_mat.flatten(), exploit = exploit, past_action = np.reshape(a, (1, 1, -1)))\n",
    "                elif np.random.rand() < .5 and not explore_type_random:\n",
    "                    a = pi.act(obs_mat.flatten(), exploit = 1, past_action = np.reshape(a, (1, 1, -1)))\n",
    "                else:\n",
    "                    a = np.random.randn(N_ACT) * 1. * explore_random_mag\n",
    "        a = np.clip(a, -2, 2)\n",
    "        an = np.concatenate((an, a[None,:]), 0)\n",
    "        snraw = np.concatenate((snraw, obsraw[None,:]), 0)\n",
    "        last_obs = obs\n",
    "        obs, r, done, _ = env.step(a + .2)\n",
    "        obsraw = obs\n",
    "        r = r + 1 * REWARD_MULT\n",
    "\n",
    "        obs_mat = accumulate_state(\n",
    "            obs, a, obs_mat, STATE_DECAY, style = 'np')\n",
    "        \n",
    "        rn.append(r)\n",
    "        sn = np.concatenate((sn, obs_mat.reshape(1, -1)), 0)\n",
    "        score += r\n",
    "        frame += 1\n",
    "        if 1:#ep > INIT_LEN:\n",
    "            still_open = env.render(\"human\")\n",
    "        else:\n",
    "            still_open = 1\n",
    "        if done:\n",
    "            done_ctr += 1\n",
    "            if done_ctr > 3:\n",
    "                if ep % MAX_SEQ_LEN == 0:\n",
    "                    print('score', score, ' frames', frame)\n",
    "                break\n",
    "        if still_open==False:\n",
    "            crashhere\n",
    "        if not done: continue\n",
    "        if restart_delay==0:\n",
    "            print(\"score=%0.2f in %i frames\" % (score, frame))\n",
    "            if still_open!=True:      # not True in multiplayer or non-Roboschool \n",
    "                break\n",
    "            restart_delay = 2000*2  # 2 sec at 60 fps\n",
    "        restart_delay -= 1\n",
    "        if restart_delay==0: \n",
    "            break\n",
    "    if 0:#ep < INIT_LEN:\n",
    "        a = np.random.randn(N_ACT)\n",
    "    else:\n",
    "        a = pi.act(obs_mat.flatten(), exploit = exploit, past_action = np.reshape(a, (1, 1, -1)))\n",
    "    an = np.concatenate((an, a[None,:]), 0)\n",
    "    localframes.append(frame)\n",
    "    rn = np.array(rn)/10\n",
    "    rn[-1] = rn[-1] - 100 * REWARD_MULT\n",
    "    rn[-2] = rn[-2] - 5 * REWARD_MULT\n",
    "    rn[-3] = rn[-3] - 1 * REWARD_MULT\n",
    "    rewards = [0]\n",
    "    for ir in rn[::-1]:\n",
    "        rewards.append(rewards[-1] * GAMMA + ir)\n",
    "    rdecayedn = np.array(rewards)[:0:-1]\n",
    "    lenrdec = len(rdecayedn)\n",
    "#     rdecayedn = rdecayedn + lenrdec\n",
    "#     rdecayedn = rdecayedn - np.arange(lenrdec) * 1.8\n",
    "    maskn = np.ones_like(rn)\n",
    "    if step_shifted > -1:\n",
    "        if step_shifted == 0:\n",
    "            raise ValueError('step shifted not allowed 0')\n",
    "        maskn[:step_shifted - 1] = 0\n",
    "    if ep == 0:\n",
    "        ah, sh, shraw, rh, rdecayedh, maskh = [\n",
    "            np.expand_dims(v, 0) for v in [an, sn, snraw, rn,rdecayedn, maskn]]\n",
    "        isexploit = [exploit]\n",
    "    else:\n",
    "        def get_updated_h(h, n, third_dim):\n",
    "            hshape = h.shape[1]\n",
    "            nshape = n.shape[0]\n",
    "            if third_dim:\n",
    "                if hshape > nshape:\n",
    "                    n = np.concatenate((n, np.zeros((hshape - nshape, n.shape[-1]))), 0)\n",
    "                if nshape > hshape:\n",
    "                    h = np.concatenate((h, np.zeros((\n",
    "                        h.shape[0], nshape - hshape, h.shape[-1]))), 1)\n",
    "            else:\n",
    "                if hshape > nshape:\n",
    "                    n = np.concatenate((n, np.zeros((hshape - nshape))), 0)\n",
    "                if nshape > hshape:\n",
    "                    h = np.concatenate((h, np.zeros((h.shape[0], nshape - hshape))), 1)\n",
    "            h = np.concatenate((h, np.expand_dims(n, 0)), 0)\n",
    "            return h\n",
    "            \n",
    "        ah, sh, shraw = [get_updated_h(h, n, 1) for  h, n in zip(\n",
    "            [ah, sh, shraw], [an, sn, snraw])]\n",
    "        \n",
    "        rh, rdecayedh, maskh = [\n",
    "            get_updated_h(h, n, 0) for h, n in zip(\n",
    "                [rh, rdecayedh, maskh], [rn, rdecayedn, maskn])]\n",
    "        isexploit.append(exploit)\n",
    "    if ep % 1 == 0 and ep > INIT_LEN:\n",
    "        ah, sh, shraw, rh,rdecayedh, maskh, isexploit = [\n",
    "            v[-NUM_KEEP:] for v in [ah, sh, shraw, rh,rdecayedh, maskh, isexploit]]\n",
    "        globalframes.append(np.mean(localframes))\n",
    "        localframes = []\n",
    "        batch_size = BATCH_SIZE\n",
    "        if ep < batch_size:\n",
    "            batch_size = ep\n",
    "        num_hist = ah.shape[0]\n",
    "    if ep % N_EPISODES == 0 and ep > 0:\n",
    "        print('average global framnes', np.mean(globalframes[-100:]))\n",
    "        train_all()\n",
    "        ysmoothed = gaussian_filter1d(globalframes, sigma=4)\n",
    "        plt.plot(ysmoothed)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.actions, {self.obs: np.reshape((obs_mat.flatten()/shdiff) - shmin_g_e, [1, 1, -1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_mat.flatten().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.act(obs_mat.flatten(), exploit = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(obs/shdiff) - shmin_g_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd= get_feed_dict(16, 0, 1)\n",
    "\n",
    "del fd[self.actions]\n",
    "sess.run(self.actions, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess.run(self.actions, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explore_type_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah[:,:10][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_EPISODES = 4\n",
    "N_STEPS = 1000\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.model_estimator.hidden_output[:,:-1], feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(\n",
    "            tf.square(self.model_estimator.hidden_output[:,:-1] - tf.expand_dims(self.statesraw[:,1:], -1)\n",
    "                 )) * tf.expand_dims(self.maskexpanded[:,:-1], -1) * tf.reshape(\n",
    "            tf.range(N_LAYERS, dtype = tf.float32)/(N_LAYERS * 2) + .1, (1, 1, 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 1002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = train(actor = True, value = True, n_steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feed_dict[self.obs][:,:7,:].min(0).min(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.critic_input[:,:7,0], feed_dict).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah[-11::2,5:10].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isexploit[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.loss_estimator.h[0], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gradients(self.loss_estimator.output, self.loss_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        self.l_loss = tf.reduce_mean(\n",
    "            tf.square(self.loss_estimator.output[:,:-1,0] - tf.reduce_mean(self.m_loss_ind, -1)) * self.mask[:,:-1]\n",
    "        ) + tf.reduce_mean(\n",
    "            tf.square(\n",
    "                self.loss_estimator.output[:,:-1,1] - self.c_loss_ind\n",
    "            ) * self.mask[:,:-1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.reduce_mean(\n",
    "            tf.square(self.loss_estimator.output[:,:-1,0] - tf.reduce_mean(self.m_loss_ind, -1)) * self.mask[:,:-1]\n",
    "        ) + tf.reduce_mean(\n",
    "            tf.square(\n",
    "                self.loss_estimator.output[:,:-1,1] - self.c_loss_ind\n",
    "            ) * self.mask[:,:-1]\n",
    "        ), feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.reduce_mean(self.c_loss_ind, -1), feed_dict).shape\n",
    "        self.l_loss = tf.reduce_mean(\n",
    "            tf.square(self.loss_estimator.output[:,:-1,0] - tf.reduce_mean(self.m_loss_ind, -1)) * self.mask[:,:-1]\n",
    "        ) + tf.reduce_mean(\n",
    "            tf.square(\n",
    "                self.loss_estimator.output[:,:-1,1] - self.c_loss_ind\n",
    "            ) * self.mask[:,:-1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvars2 = sess.run(self.m_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m1, m2 in zip(mvars, mvars2):\n",
    "    print(np.square(m1 - m2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATE * N_HISTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.m_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.model_estimate, feed_dict)[0,:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.statesraw[:,1:], feed_dict)[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.critic_input, feed_dict)[:,5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict[self.lr] = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = get_feed_dict(4, 0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    print(sess.run([self.mopt, self.m_loss], feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.statesraw[:,1:], feed_dict)[:,:10, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_adv(sess.run(self.m_obs_grad, feed_dict), adv_type = 1)[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.model_estimate, feed_dict)[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_ACT + N_OBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskh.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.critic_input, feed_dict)[:,:7][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.model_estimate, feed_dict)[2,5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(self.statesraw[:,1:], feed_dict)[2,5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.model_estimate, feed_dict)[2,5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.actions, feed_dict)[:,5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict[self.lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict[self.lr]# = .0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(200):\n",
    "    print(sess.run([self.mopt, self.m_loss], feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(30):\n",
    "    train(actor = True, value = True, n_steps = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
