{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os.path, gym\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import roboschool\n",
    "import pdb\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "GAMMA = .97\n",
    "from functools import partial\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "if 1:\n",
    "    envname = \"RoboschoolInvertedPendulum-v1\"\n",
    "    SIZE_MULT = 2\n",
    "    REWARD_MULT = 1\n",
    "    N_LAYERS = 10\n",
    "    N_DROP = 4\n",
    "    NUM_DROP_PARALLEL = 8\n",
    "    NUM_HISTORY = N_HISTORY = 2\n",
    "else:\n",
    "    envname = \"RoboschoolHumanoidFlagrun-v1\"\n",
    "    SIZE_MULT = 8\n",
    "    REWARD_MULT = 4\n",
    "    N_LAYERS = 20\n",
    "    N_DROP = 3\n",
    "    NUM_DROP_PARALLEL = 6\n",
    "    NUM_HISTORY = N_HISTORY = 6\n",
    "DROP_RATE = .3\n",
    "INIT_LEN = 200\n",
    "N_PRETRAIN = 1\n",
    "NUM_KEEP = 300\n",
    "PERCENT_CHOOSE_OPTIMAL = 2\n",
    "env = gym.make(envname)\n",
    "N_OBS, N_ACT = [v.shape[0] for v in [env.observation_space, env.action_space]]\n",
    "N_STATE = (N_OBS + N_ACT) * 2\n",
    "REWARD_IN_STATE = 0\n",
    "if REWARD_IN_STATE:\n",
    "    N_STATE += 2\n",
    "INPUT_UNITS = N_STATE * NUM_HISTORY\n",
    "STATE_DECAY = .7\n",
    "TESTING_GRAD_NORMS = 1\n",
    "ADV_ENABLED = 0\n",
    "STORED_MODELS = {}\n",
    "FCNS = {'np':{\n",
    "    'concat':np.concatenate,\n",
    "    'reshape':np.reshape,\n",
    "    'expand':np.expand_dims\n",
    "},'tf':{\n",
    "    'concat':tf.concat,\n",
    "    'reshape':tf.reshape,\n",
    "    'expand':tf.expand_dims\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb2d2a4518>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAHm9JREFUeJzt3Xt4XXWd7/H3L7emubbJzq1Jc2vTpndaQqFAASkoVCiIcptR8HjpI8ooD+OMMKij43Ee1OMc5TijDyOOgnhUBAErKlBkgENbaNM2vaRt2qRpbs39unPf+3f+yG4NNekl2dlrXz6v59lPdtZeWevb31795Jff/q21jLUWEREJH1FOFyAiIv6lYBcRCTMKdhGRMKNgFxEJMwp2EZEwo2AXEQkzCnYRkTCjYBcRCTMKdhGRMBPjxE5dLpctLCx0YtciIiFr165dbdbajHOt50iwFxYWsnPnTid2LSISsowxteeznoZiRETCjIJdRCTMKNhFRMKMgl1EJMwo2EVEwoyCXUQkzCjYRUTCjCPz2EVEIoG1lg73MDVtbo6391Pb7uaOsvnMT0uY0f0q2EVEpsk9NEpNm/s9j+o2NzWtffQMjp5eL8rAmvy5CnYRkWAw4vFS297vC+6+sfBudXO83U1zz9B71s2dM5siVyKbLppHkSuJIlcCRa4kcufMJi5m5kfAFewiIuMMjXqoaXNT1dxHVUsfR1t6qWoeC/JRrz29XnpiHIWuRNaXZFDkSqTYlUhRRiIFaYnMjot28F+gYBeRCDUw7OFYax9VvuAeC/E+atvdnMrvKAMF6YkszEzi+qVZLMxMojgjiaL0RFITYp39B5yFgl1EwprXa6nr7KeyqZdDJ3s41NRL5ckeTnT0Y30BHhNlKHIlsiQnmZtXzaMkM4mSrCQK0xOJj3W29z0VCnYRCRu9gyMcPtlL5cleKpt6ONTUw+GTvbiHPQAYA0XpiSyfl8ptq/NYlDUW4AXpicRGh8/sbwW7iISkDvcw+xq62d/QTUV9FwebeqjrGDj9ekp8DKU5KXzk4jyW5KRQmpPC4qxkx8e/A0HBLiJBr9MX4vsautlXP/a1oesvIV6QnsDK3DncWTb/dIjPS43HGONg1c5RsItIUBkc8bCvoZvy2k721ndRUd9Nfed7Q3x1/hzuWVfAitxUluWmkjo7eD/IdIKCXUQcY62lvnOA8hOd7D7Rxe4TnRxo7Dk9rTA/LYFV8+fwscsU4hfCb8FujIkGdgIN1tqb/LVdEQkfIx4v+xq6ebemg121neyu66K1d+zkntmx0ayan8rmq4pZnT+X1flzcCXNcrji0OTPHvsXgEogxY/bFJEQNjTqYW9dN+/UtLPDF+b9vhkqhekJrF/oYnX+HFbnz6U0O5mYMJqZ4iS/BLsxJg/4IPBN4EF/bFNEQs/QqIfy2i62V7ezo6ad3Se6GBr1AlCancztF+dxaXE6lxSmkZGs3vhM8VeP/XvAPwLJk61gjNkMbAbIz8/3025FxEnWWo629PFGVRtvVrWyo7qDgREPxsDSnBT+9tICLi1OY21hGnMT45wuN2JMO9iNMTcBLdbaXcaYayZbz1r7OPA4QFlZmZ1sPREJbu19Q7x1tI03fWF+6gJYxa5E7ijL48qSDNYWpelDTgf5o8d+BbDJGLMRiAdSjDE/t9Z+1A/bFhGHWWs53NzL1soWXjnYzN76LqyFOQmxXLHAxfoSF1eWuMibO7OXopXzN+1gt9Y+DDwM4Ouxf1GhLhLahkY97KjuYGtlM69Wtpw+GWhVXioPbFjENYszWJ6bSnRUZJ4AFOw0j11EgLETg14/3MpL+5p47VALfUOjxMdGceVCF/dfu5ANpZlkpsQ7XaacB78Gu7X2deB1f25TRGbO+DDfWtmMe9jD3IRYblqZw/VLs7h8gSsirq0SbtRjF4kwHq/lraNtPFdez6sH/xLmmy6ax8YVOawrTtd88hCnYBeJEEeae3l2Vz2/3d1AS+8QKfEx3LxqHh9cmcNlxelhddnaSKdgFwljPYMj/La8gd/sqmdfQzfRUYb3Lc7gtjV5bFiSyawYDbOEIwW7SBja39DNz7fX8sKeRgZGPCzNSeErNy3llovm6forEUDBLhImBkc8bKlo4ufba9lT10V8bBS3rMrlo5cVsCIv1enyJIAU7CIhrqt/mKe21fKzbcdp6xtmQUYi/3zzUm5bk6ezPyOUgl0kRNV19PPEWzX86t06BkY8XLM4g0+vL+byBekRe+cgGaNgFwkxx9vcPPZaFc/vbiA6yrBpVS6brypmcfak1+CTCKNgFwkRJ9r7+T+vVfHc7gZiow2fuKKIT60vJjtVZ4PKeynYRYJcS+8g33u1il+/W0d0lOHedYV85ppiMpMV6DIxBbtIkBoc8fDEWzX8x5+PMuzx8reX5vPZ9y0kS9drkXNQsIsEGWstWyqaePQPh2joGuD9S7N4eOMSilyJTpcmIULBLhJEatvdfPn5/bxZ1cbSnBS+c/tKLl/gcrosCTEKdpEgMOLx8uM3a/jeq0eIjY7i65uW8dHLCnS9c5kSBbuIw6qae3ngV3s40NjDB5Zl8bVNy8hJne10WRLCFOwiDrHW8uS2Wv71pUoSZ8Xwo4+u4YblOU6XJWFAwS7igPa+If7+mb28friVaxZn8O2PrNT0RfEbBbtIgFXUd3Hfz8tp7RviX25ZxscuK9AlAMSvFOwiAfTrnXV8+fn9ZCTN4tnPXK6rLsqMULCLBIDXa3n0j4d4/I1qrlzo4rG7V5OWGOd0WRKmFOwiM2xo1MMXn6ngd3sbuWddAV+9aanuKSozSsEuMoP6hkb51M/eZXt1Bw/fWMrmq4o1ni4zTsEuMkN6B0f4+H+9y566Lr5/10XcclGu0yVJhFCwi8yA3sER7v3JO1TUd/ODu1dz4wrNT5fAUbCL+NngiIdP/nTnWKj/zWqddCQBp2AX8SOP1/Lgr/fwzvEOHrtboS7O0EfzIn70jS0HeWnfSb78wSVsWjXP6XIkQinYRfzkV++e4KdvHz99yzoRpyjYRfygor6Lr7xwgCsXunjkg0ucLkcinIJdZJq6+oe57+flZCTN4rG7V+sa6uI4fXgqMk1ffeEAzT2D/Oa+y3WZAAkK6rGLTMOWikZe3NvIFzaUcNH8OU6XIwIo2EWmrLV3iC8/v59V8+dw3zULnC5H5DQFu8gUPfqHQ7iHRvnu7at0US8JKtM+Go0x840xfzbGVBpjDhhjvuCPwkSC2a7aDp4tr+fT64tZmJnkdDki7+GPD09Hgb+31pYbY5KBXcaYV6y1B/2wbZGg4/FavvL8AXJS47n/2oVOlyPyV6bdY7fWNllry33Pe4FKQJexk7C1paKRg009PHRjKQlxmlgmwcevA4PGmEJgNbBjgtc2G2N2GmN2tra2+nO3IgEz6vHyvVerKM1O5uaVumSABCe/BbsxJgl4FnjAWttz5uvW2settWXW2rKMjAx/7VYkoJ7b3UBNm5sHr19ElE5EkiDll2A3xsQyFupPW2uf88c2RYKN12v54evHWJGbyvVLs5wuR2RS/pgVY4AngEpr7b9NvySR4PTnwy3UtLn5tG5vJ0HOHz32K4CPAdcaY/b4Hhv9sF2RoPLEWzXkpMZz4/Jsp0sROatpf6RvrX0LUPdFwtrhk728faydL91QSqxORpIgpyNU5Dw8s7OO2GjDnZfMd7oUkXNSsIucw4jHy/N7Grm2NFNXb5SQoGAXOYc3jrTS1jfEh9fkOV2KyHlRsIucw/N7GklLjOOaxZlOlyJyXhTsImcxPOrl9UMtXL8ki7gY/XeR0KAjVeQstle30zs0qhOSJKQo2EXO4pWDzcyOjebKEpfTpYicNwW7yCSstWytbGZ9iYv42GinyxE5bwp2kUnUtvfT2D3I+kW6aJ2EFgW7yCS2VbcDsK443eFKRC6Mgl1kEm8faycjeRYLMhKdLkXkgijYRSZgrWV7dTvritN1JUcJOQp2kQnUdw7Q2jvEJUVpTpcicsEU7CITqKjvBmBVXqrDlYhcOAW7yAQqGrqIi45icXay06WIXDAFu8gEKuq6Kc1JZlaM5q9L6FGwi5zB67Xsb+hmRa6GYSQ0KdhFztDYPUDv0ChL56U4XYrIlCjYRc5wtKUPgJJMja9LaFKwi5zhVLDrxCQJVQp2kTMca+1jbkIs6UmznC5FZEoU7CJnONrSx8LMJKfLEJkyBbvIGapb3RS7FOwSuhTsIuP0D4/S7h4mPz3B6VJEpkzBLjJOY9cAALlzZjtcicjUKdhFxqnv9AX7XAW7hC4Fu8g4DeqxSxhQsIuM09g1QEyUISsl3ulSRKZMwS4yTkPnANmp8URH6eYaEroU7CLjnOwZJFu9dQlxCnaRcdr7hnHpjFMJcQp2kXHa3cOkJ8U5XYbItCjYRXxGPV46+4d1jRgJeX4JdmPMDcaYw8aYo8aYh/yxTZFA6+wfwVrIUI9dQty0g90YEw38O3AjsBS42xizdLrbFQm0tr4hAPXYJeT5o8e+Fjhqra221g4DvwRu8cN2RQKqvW8YgPRE9dgltPkj2HOBunHf1/uWiYSUdvepHruCXUKbP4J9ojM57F+tZMxmY8xOY8zO1tZWP+xWxL/aTvfYNRQjoc0fwV4PzB/3fR7QeOZK1trHrbVl1tqyjIwMP+xWxL863ENERxlSZ8c6XYrItPgj2N8FSowxRcaYOOAu4EU/bFckoDrcw8xNiCNKlxOQEBcz3Q1Ya0eNMfcDfwKigZ9Yaw9MuzKRAGvrG8al8XUJA9MOdgBr7UvAS/7YlohTOtzDpGlGjIQBnXkq4tPeN6Q57BIWFOwiPu3uYc1hl7CgYBcBBkc89A6OkpGsHruEPgW7CH+5iXVOqq7FLqFPwS4CNHYNAjBP9zqVMKBgFwEau8d67PNSFewS+hTsIowNxRgDWakaY5fQp2AXYSzYXUmzmBUT7XQpItOmYBcBGroGNL4uYUPBLgLUtLopdiU6XYaIXyjYJeL1D4/S2D1IkYJdwoSCXSJeTZsbgOIMBbuEBwW7RLzTwe5KcrgSEf9QsEvEq24dC3YNxUi4ULBLxDt8spf5abOZHaepjhIeFOwS8fY3drN8XqrTZYj4jYJdIlrP4Ai17f0sz1WwS/hQsEtEO9jYA8DSeSkOVyLiPwp2iWj7G7oBNBQjYUXBLhGt/EQnuXNm6wYbElYU7BKxrLW8U9PB2qI0p0sR8SsFu0SsmjY3bX3DXFKoYJfwomCXiPVOTQeAeuwSdhTsErHePtaOKymOBbpGjIQZBbtEpFGPl/8+0srVizIxxjhdjohfKdglIu2u66J7YIRrSzOdLkXE7xTsEpFeO9RCTJRh/SKX06WI+J2CXSKOtZaXD5ykrHAuKfGxTpcj4ncKdok4B5t6ONbq5uZV85wuRWRGKNgl4ry4p5GYKMPG5TlOlyIyIxTsElG8XsuLexu5elEGcxPjnC5HZEYo2CWivFHVSlP3ILeuznW6FJEZo2CXiPLUtlpcSbP4wLJsp0sRmTEKdokYdR39vHa4hbvXzicuRoe+hK9pHd3GmO8YYw4ZYyqMMb81xszxV2Ei/vbktuMY4G8uzXe6FJEZNd1uyyvAcmvtSuAI8PD0SxLxvw73ME/vOMGmVfPISZ3tdDkiM2pawW6tfdlaO+r7djuQN/2SRPzvJ2/VMDDi4XPvW+h0KSIzzp8DjZ8A/uDH7Yn4RVf/MD97+zg3Ls+mJCvZ6XJEZlzMuVYwxrwKTDSF4BFr7Qu+dR4BRoGnz7KdzcBmgPx8jXFK4Hx/axXu4VE+v6HE6VJEAuKcwW6tve5srxtj7gVuAjZYa+1ZtvM48DhAWVnZpOuJ+NOx1j6e2lbLnZfkU5qd4nQ5IgFxzmA/G2PMDcCXgKuttf3+KUnEP6y1/OvvK4mPjebB6xc5XY5IwEx3jP0HQDLwijFmjzHmR36oScQvXtp3kq2HWvj8hoVkJM9yuhyRgJlWj91aqykGEpQ63cP884v7WZGbyieuKHK6HJGAmlawiwSrr/3uAF39Izz1yUuJidZZphJZdMRL2HlmZx0v7Gnk764tYUmOPjCVyKNgl7BS1dzLV184wLridO6/ViOFEpkU7BI2egZH+OzT5STOiub7d11EdJRxuiQRR2iMXcLCqMfL/b/YTU2bmyc/sZbMlHinSxJxjIJdwsK/bDnIG0daefS2FVy+0OV0OSKO0lCMhLwfvFbFk9tq2XxVMXet1eUqRBTsEtJ+/GY1/+vlI9y2OpeHbih1uhyRoKBgl5D11Lbj/M/fV7JxRTbf/shKovRhqQigMXYJQdZa/uP1Y3znT4e5bkkm37tztU5CEhlHwS4hxeu1fPOlSp54q4YPrc7l2x9ZSaxCXeQ9FOwSMgaGPfzDb/aypaKJj19eyFdvWqrhF5EJKNglJDR2DbD5qZ0caOzhSzeU8pmrizFGoS4yEQW7BL13ajr47NPlDI54+PE9ZWxYkuV0SSJBTcEuQcvjtfzgtaN8f+sR8tMS+OXmS1mYqXuWipyLgl2CUlP3AA/8cg87ajq49aJ5fOPW5STHxzpdlkhIULBLULHW8mx5A9/YcpARj5fv3r6KD1+c53RZIiFFwS5Bo6FrgIef28cbR1q5pHAu3/rwSoozkpwuSyTkKNjFcSMeL09tq+W7Lx/GAl/ftIyPXVagqYwiU6RgF0e9VdXG1393gKqWPq5alME3b13O/LQEp8sSCWkKdnFEbbubb/6+kpcPNpOflsB/3lPGdUsyNTddxA8U7BJQJ7sHeey1Kn79bh1xMVH8wwcW88kri4iPjXa6NJGwoWCXgOhwD/PD14/y5LZavNZy99p8/u7ahbrTkcgMULDLjGruGeSJt2p4enstAyMePrQ6jweuK9E4usgMUrDLjKhu7ePxN6p5rryBUa+XD66cx+evXUhJls4cFZlpCnbxG2st7x7v5L/+Xw1/PHCS2Ogo7rgkj83rF5Cfrh66SKAo2GXa+odHeX53I09uO86hk72kxMdw39UL+B9XFJGRPMvp8kQijoJdpuxoSx+/2HGCZ3bV0Ts4ypKcFB69bQW3XJTL7DjNchFxioJdLkjP4Ahb9jbxzK46dp/oIibKcOOKHO5dV8DFBXM1D10kCCjY5Zy8Xsvbx9p5Zlcdf9x/kqFRLyWZSfzTxlJuXZ1LZrKmLIoEEwW7TMhaS/mJLrZUNPLSviaae4ZIiY/h9rI8br94PivzUtU7FwlSCnY5zVpLRX03v9/XxO8rmmjoGiAuOoqrF2ewadU8rl+apTNERUKAgj3CjXq87Krt5NXKZv50oJkTHf3ERBnWl7h48PpFXL8sixTd4EIkpCjYI1Df0ChvHmnllYPNvHa4ha7+EWKjDesWuPjc+xbwgWXZzEmIc7pMEZkivwS7MeaLwHeADGttmz+2Kf5jreV4ez9vVrWytbKFbcfaGfZ4mZMQy7WLM7luaRbrS1y69ZxImJh2sBtj5gPXAyemX474S8/gCG8fbeONqjberGqlrmMAgIL0BO5ZV8B1S7MoK5hLTHSUw5WKiL/5o8f+v4F/BF7ww7ZkikY8Xirqu3mrqo03qlrZU9eFx2tJjItm3QIXm9cXs74kg0JXotOlisgMm1awG2M2AQ3W2r2a+hZYw6NeKuq72F7dzo6aDnbVdtI/7MEYWJmbyn1XL+CqRRmszp9DrHrlIhHlnMFujHkVyJ7gpUeAfwLefz47MsZsBjYD5OfnX0CJAjA44mFvXRfbqzvYUdNO+YlOBke8AJRmJ3P7xXlcWpzOZcXppCXqg0+RSGastVP7QWNWAFuBft+iPKARWGutPXm2ny0rK7M7d+6c0n4jxcnuQcpPdFJe20n5iU72N/YwPOrFGCjNTuGy4jQuLUpnbVGaglwkQhhjdllry8613pSHYqy1+4DMcTs8DpRpVsyFGx71cqCxm/ITXZSf6GR3bSeN3YMAxMVEsTI3lY9fXsglhWmsLUwjNUGzV0RkcprHHmAer6W6tY99Dd1U1Hezr2HsMTw6NqySO2c2awrm8qn8uawpmMvSnBTiYjRGLiLnz2/Bbq0t9Ne2woXHa6lp6zsd4PsbujnQ2EP/sAeA2bHRLJuXwr3rCljjC/Is3QNURKZJPXY/GRzxcLSlj8qmHg429Uwa4neUzWdFbior81IpzkgiOkqziUTEvxTsF8hay8meQQ419XKwqYdDJ3s51NRDdZsbj3fsg+j42CiWzUvljrL5LPeF+AKFuIgEiIL9LHoHRzja0seR5l4qm3o5dHIsyLv6R06vkzd3NqXZKdywPJvS7BSW5CRTkJ6oEBcRxyjYgU73MFUtfRxt6aOqpZejvudNvpkpAAlx0SzOTmbjihyWZCdTmpPC4uxkXflQRIJOxAS7tZbW3iFfeI8FeFVzH8da+2jrGz69XkJcNAszk1hXnM7CrCRKMpMpyUwiPy2BKPXCRSQEhF2wd/ePUN3Wx/F2NzWtbmra+6lp6+N4Wz99Q6On10uJj6EkK5kNpVmUZCWxMHPsMS91tgJcREJaSAa7e2h0LLjb3Bxvc1Pt+1rT5qZz3Ph3lIG8uQkUuRIpK0ijyJVIiS/AM5Jn6dZuIhKWQirYH9taxdM7amnuGXrP8uyUeIpcidywPIdiVyJFrkQKXYnkpyXo5B4RiTghFexZKbNYX5JB0anwTk+k0JVAQlxI/TNERGZUSCXinZfkc+clujKkiMjZaJxCRCTMKNhFRMKMgl1EJMwo2EVEwoyCXUQkzCjYRUTCjIJdRCTMKNhFRMKMsdYGfqfGtAK1U/xxFxCMN8xWXRdGdV0Y1XVhgrUumF5tBdbajHOt5EiwT4cxZqe1tszpOs6kui6M6rowquvCBGtdEJjaNBQjIhJmFOwiImEmFIP9cacLmITqujCq68KorgsTrHVBAGoLuTF2ERE5u1DssYuIyFkEdbAbY243xhwwxniNMWVnvPawMeaoMeawMeYD45bf4Ft21BjzUABq/JUxZo/vcdwYs8e3vNAYMzDutR/NdC1n1PU1Y0zDuP1vHPfahG0XoLq+Y4w5ZIypMMb81hgzx7fc0fby1RDQY+csdcw3xvzZGFPpO/6/4Fs+6XsawNqOG2P2+fa/07cszRjzijGmyvd1boBrWjyuTfYYY3qMMQ840V7GmJ8YY1qMMfvHLZuwfcyYx3zHW4UxZo3fCrHWBu0DWAIsBl4HysYtXwrsBWYBRcAxINr3OAYUA3G+dZYGsN7vAl/1PS8E9jvYdl8DvjjB8gnbLoB1vR+I8T3/FvCtIGkvR4+dM2rJAdb4nicDR3zv24TvaYBrOw64zlj2beAh3/OHTr2nDr6PJ4ECJ9oLuApYM/5Ynqx9gI3AHwADXAbs8FcdQd1jt9ZWWmsPT/DSLcAvrbVD1toa4Ciw1vc4aq2tttYOA7/0rTvjzNidse8A/m8g9jcNk7VdQFhrX7bWjvq+3Q7kBWrf5+DYsXMma22Ttbbc97wXqARynajlPN0C/Mz3/GfArQ7WsgE4Zq2d6gmQ02KtfQPoOGPxZO1zC/CkHbMdmGOMyfFHHUEd7GeRC9SN+77et2yy5YGwHmi21laNW1ZkjNltjPlvY8z6ANUx3v2+P/F+Mu7PYyfb6EyfYKzHcoqT7RVM7XKaMaYQWA3s8C2a6D0NJAu8bIzZZYzZ7FuWZa1tgrFfSkCmA3Wdchfv7Vw53V4wefvM2DHneLAbY141xuyf4HG23pKZYJk9y/JA1Hg37z2gmoB8a+1q4EHgF8aYlOnWcgF1/RBYAFzkq+W7p35sgk35dWrU+bSXMeYRYBR42rdoxtvrXGVPsMzRKWPGmCTgWeABa20Pk7+ngXSFtXYNcCPwOWPMVQ7UMCFjTBywCXjGtygY2utsZuyYc/xm1tba66bwY/XA/HHf5wGNvueTLZ+yc9VojIkBbgMuHvczQ8CQ7/kuY8wxYBGwc7r1nG9d4+r7T2CL79uztV1A6jLG3AvcBGywvsHGQLTXOcx4u1wIY0wsY6H+tLX2OQBrbfO418e/pwFjrW30fW0xxvyWsSGsZmNMjrW2yTeU0BLounxuBMpPtVMwtJfPZO0zY8ec4z32KXoRuMsYM8sYUwSUAO8A7wIlxpgi32/vu3zrzrTrgEPW2vpTC4wxGcaYaN/zYl+N1QGo5dT+x4/VfQg49Sn9ZG0XqLpuAL4EbLLW9o9b7mh74dyx81d8n9c8AVRaa/9t3PLJ3tNA1ZVojEk+9ZyxD8L3M9ZO9/pWuxd4IZB1jfOev5qdbq9xJmufF4F7fLNjLgO6Tw3ZTFsgPzGewifMH2Lst9oQ0Az8adxrjzA2i+EwcOO45RsZm0VwDHgkQHX+FPjMGcs+DBxgbHZFOXBzgNvuKWAfUOE7gHLO1XYBqusoY+OKe3yPHwVDezl17ExSx5WM/UleMa6dNp7tPQ1QXcW+92ev7716xLc8HdgKVPm+pjnQZglAO5A6blnA24uxXyxNwIgvuz45WfswNhTz777jbR/jZv5N96EzT0VEwkyoDsWIiMgkFOwiImFGwS4iEmYU7CIiYUbBLiISZhTsIiJhRsEuIhJmFOwiImHm/wMVafgUtTT4nAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-100, 100, .01)\n",
    "\n",
    "y = np.log(np.abs(x)+1) * np.sign(x)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_clipped_optimizer(opt_fcn,\n",
    "                            loss,\n",
    "                            clip_norm=.1,\n",
    "                            clip_single=.03,\n",
    "                            clip_global_norm=False,\n",
    "                            var_list=None):\n",
    "    if var_list is None:\n",
    "        gvs = opt_fcn.compute_gradients(loss)\n",
    "    else:\n",
    "        gvs = opt_fcn.compute_gradients(loss, var_list = var_list)\n",
    "        \n",
    "\n",
    "    if clip_global_norm:\n",
    "        gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "        capped_gs, grad_norm_total = tf.clip_by_global_norm([g for g in gs],clip_norm)\n",
    "        capped_gvs = list(zip(capped_gs, vs))\n",
    "    else:\n",
    "        grad_norm_total = tf.sqrt(\n",
    "                tf.reduce_sum([\n",
    "                        tf.reduce_sum(tf.square(grad)) for grad, var in gvs\n",
    "                        if grad is not None\n",
    "                ]))\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single, clip_single), var)\n",
    "                                    for grad, var in gvs if grad is not None]\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var)\n",
    "                                    for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "    optimizer = opt_fcn.apply_gradients(capped_gvs)\n",
    "\n",
    "    return optimizer, grad_norm_total\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self,\n",
    "                x, lshapes, output_units, namebase, squeeze = True,\n",
    "                lshapes_drop = None, drop_rate = DROP_RATE, reuse = False,\n",
    "                residual = None\n",
    "                ):\n",
    "        self.namebase = namebase\n",
    "        self.reuse = reuse\n",
    "        self.lidx = 0\n",
    "        name_fcn = self.get_name\n",
    "        h = [tf.nn.leaky_relu(tf.layers.dense(\n",
    "            x, lshapes[0], name=self.get_name(), reuse = self.reuse))]\n",
    "        h2 = h[-1]\n",
    "        for size in lshapes:\n",
    "            h.append(tf.nn.leaky_relu(h[-1] + tf.layers.dense(\n",
    "                h2, size, name=name_fcn(), reuse = self.reuse)))\n",
    "            h2 = h[-1]#tf.concat((x, h[-1]), -1)\n",
    "        if lshapes_drop is None:\n",
    "            hout = h[-1]\n",
    "            output = tf.layers.dense(\n",
    "                hout, output_units, name=name_fcn(), reuse = self.reuse)\n",
    "            if output_units == 1 and squeeze:\n",
    "                output = tf.squeeze(output, -1)\n",
    "            self.raw_output = output\n",
    "            self.output = leaky_tanh(self.raw_output)\n",
    "            if residual is not None:\n",
    "                self.output = self.output + residual\n",
    "        else:\n",
    "            drop_outs = []\n",
    "            x_drop = h[-1]\n",
    "            name_fcn = self.get_name_drop\n",
    "            self.lidx_drop = 0\n",
    "            for drop_idx, drop_try in enumerate(range(NUM_DROP_PARALLEL)):\n",
    "                h_drop = [x_drop]\n",
    "                for lsize in lshapes_drop:\n",
    "                    new_layer = tf.nn.dropout(tf.nn.leaky_relu(tf.layers.dense(\n",
    "                        h_drop[-1], size, name=name_fcn(), reuse = self.reuse)), keep_prob = 1-drop_rate)\n",
    "                    h_drop.append(new_layer)\n",
    "                    new_layer = tf.nn.leaky_relu(tf.layers.dense(\n",
    "                        h_drop[-1], size, name=name_fcn(), reuse = self.reuse))\n",
    "                    h_drop.append(new_layer)\n",
    "                output = tf.layers.dense(h_drop[-1], output_units)\n",
    "                if output_units == 1 and squeeze:\n",
    "                    output = tf.squeeze(output, -1)\n",
    "                output = output\n",
    "                if residual is not None:\n",
    "                    output = output + residual\n",
    "                drop_outs.append(output)\n",
    "                self.reuse = True\n",
    "                self.lidx_drop = 0\n",
    "            self.output = tf.stack(drop_outs, -1)\n",
    "            self.output_mean, self.variance = tf.nn.moments(self.output, -1)\n",
    "    def get_name(self):\n",
    "        self.lidx = self.lidx + 1\n",
    "        return self.namebase + str(self.lidx)\n",
    "    def get_name_drop(self):\n",
    "        self.lidx_drop = self.lidx_drop + 1\n",
    "        return self.namebase + '_drop_' + str(self.lidx_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_state(state, action, style = 'np'):\n",
    "    together = FCNS[style]['concat']((state, action), -1)\n",
    "    if style == 'tf':\n",
    "        return together\n",
    "#         expanded = FCNS[style]['expand'](\n",
    "#             together, 1)\n",
    "    else:\n",
    "        expanded = FCNS[style]['expand'](\n",
    "            together, 0)\n",
    "    return expanded\n",
    "    return FCNS[style]['reshape'](\n",
    "        together, (state.shape[0], -1, state.shape[-1] + action.shape[-1]))\n",
    "        \n",
    "    \n",
    "\n",
    "def accumulate_state(state, action, old_state, statedecay, style = 'np'):\n",
    "    new_state = make_state(state, action, style)\n",
    "    new_state_len = new_state.shape[-1]\n",
    "    if style == 'tf':\n",
    "        vel = new_state - old_state[:,:,0,:new_state_len]\n",
    "    else:\n",
    "        vel = new_state - old_state[0,:new_state_len]\n",
    "    new_state = FCNS[style]['concat']((new_state, vel), -1)\n",
    "    if style == 'tf':\n",
    "        return FCNS[style]['concat']((tf.expand_dims(new_state, 2), old_state[:,:,:-1,:]*statedecay), 2)\n",
    "    return FCNS[style]['concat']((new_state, old_state[:-1,:]*statedecay), 0)\n",
    "\n",
    "def leaky_tanh(x):\n",
    "    return tf.log(tf.abs(x)+1) * tf.sign(x)*.7\n",
    "    #return tf.nn.tanh(x*30)/10 + tf.nn.tanh(x*2)/2 + tf.nn.tanh(x/20) * 2 + x * 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_grad_norm(optimizer, loss, optname):\n",
    "    gvs = optimizer.compute_gradients(loss)\n",
    "    grad_norm = tf.reduce_mean(\n",
    "        [tf.reduce_mean(tf.square(grad)) for\n",
    "         grad, var in gvs if grad is not None and optname in var.name])\n",
    "    return grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PolicyLearner(object):\n",
    "    def __init__(self, ob_space, ac_space, take_weights_here=None, \n",
    "                 lshapes = [32 * SIZE_MULT] * N_LAYERS, config = None, \n",
    "                 lshapes_small = [16 * SIZE_MULT] * (N_LAYERS//2),\n",
    "                 lshapes_smaller = [16 * SIZE_MULT] * (N_LAYERS//2),\n",
    "                 lshapes_drop = [16 * SIZE_MULT] * N_DROP,\n",
    "                reuse = False):\n",
    "        self.sess = tf.InteractiveSession(config=config)\n",
    "        self.obs_raw = tf.placeholder(tf.float32, (None, None, NUM_HISTORY, N_STATE))\n",
    "        self.returns = tf.placeholder(tf.float32, (None, None))\n",
    "        self.returnsdecayed = tf.placeholder(tf.float32, (None, None))\n",
    "        self.mask = tf.placeholder(tf.float32, (None, None))\n",
    "        self.lr = tf.placeholder_with_default(1e-3, (None))\n",
    "        self.statesraw = tf.placeholder(tf.float32, (None, None, N_OBS))\n",
    "        self.is_train = tf.placeholder_with_default(True, (None))\n",
    "        self.is_exploit = tf.placeholder(tf.float32, (None))\n",
    "        self.is_exploit2d = tf.expand_dims(self.is_exploit, -1)\n",
    "        self.is_exploit3d = tf.expand_dims(self.is_exploit2d, -1)\n",
    "        self.statesraw_expanded = tf.expand_dims(self.statesraw, -1)\n",
    "        self.obs_raw_expanded = tf.expand_dims(self.obs_raw, -1)\n",
    "        self.maskexpanded = tf.expand_dims(self.mask, -1)\n",
    "        self.maskexpanded2 = tf.expand_dims(self.maskexpanded, -1)\n",
    "        self.obs = tf.reshape(\n",
    "            self.obs_raw, (\n",
    "                tf.shape(self.obs_raw)[0], tf.shape(self.obs_raw)[1], N_STATE * NUM_HISTORY))\n",
    "        self.actor = MLP(\n",
    "            self.obs, lshapes, N_ACT, 'a_', squeeze = False, reuse = reuse)\n",
    "        self.explorer = MLP(\n",
    "            self.obs, lshapes, N_ACT, 'e_', squeeze = False, reuse = reuse, residual = self.actor.output)\n",
    "        self.actions = self.actor.output\n",
    "        self.explorer_actions = self.explorer.output\n",
    "        self.state_value_estimator = MLP(\n",
    "            self.obs, lshapes_smaller, 1, 'v_', reuse = reuse)\n",
    "        self.state_value_estimate = self.state_value_estimator.output\n",
    "        \n",
    "        self.advantage = ((\n",
    "            self.state_value_estimate[:,1:] * GAMMA + self.returns[:,:-1]) -\n",
    "            self.state_value_estimate[:,:-1])\n",
    "        \n",
    "        self.critic_input = tf.concat((self.obs, self.actions), -1)\n",
    "        self.explorer_critic_input = tf.concat((self.obs, self.explorer.output), -1)\n",
    "        \n",
    "        self.advantage_estimator = MLP(\n",
    "            self.critic_input, lshapes_small, 1, 'c_', lshapes_drop = lshapes_drop, reuse = reuse)\n",
    "        self.model_estimator = MLP(\n",
    "            self.critic_input, lshapes_small, N_OBS, 'm_', lshapes_drop = lshapes_drop, reuse = reuse,\n",
    "            residual = self.statesraw\n",
    "        )\n",
    "        self.explorer_advantage_estimator = MLP(\n",
    "            self.explorer_critic_input, lshapes_small, 1, 'c_', lshapes_drop = lshapes_drop, reuse = True)\n",
    "        self.explorer_model_estimator = MLP(\n",
    "            self.explorer_critic_input, lshapes_small, N_OBS, 'm_', lshapes_drop = lshapes_drop, reuse = True,\n",
    "            residual = self.statesraw\n",
    "        )\n",
    "        self.advantage_estimate = self.advantage_estimator.output_mean\n",
    "        self.model_estimate = self.model_estimator.output_mean\n",
    "        self.future_obs_raw = accumulate_state(\n",
    "            self.model_estimate, self.actions, self.obs_raw, STATE_DECAY, style = 'tf')\n",
    "        self.explorer_future_obs_raw = accumulate_state(\n",
    "            self.explorer_model_estimator.output_mean, self.explorer_actions,\n",
    "            self.obs_raw, STATE_DECAY, style = 'tf')\n",
    "        \n",
    "        self.future_obs = tf.reshape(self.future_obs_raw, \n",
    "            (tf.shape(self.future_obs_raw)[0],tf.shape(self.future_obs_raw)[1], N_STATE * NUM_HISTORY))\n",
    "        self.explorer_future_obs = tf.reshape(self.explorer_future_obs_raw, \n",
    "            (tf.shape(self.explorer_future_obs_raw)[0],\n",
    "             tf.shape(self.explorer_future_obs_raw)[1], N_STATE * NUM_HISTORY))\n",
    "        \n",
    "        self.future_value = MLP(\n",
    "            self.future_obs, lshapes_smaller, 1, 'v_', reuse = True)\n",
    "        \n",
    "        self.future_actor = MLP(\n",
    "            self.future_obs, lshapes, N_ACT, 'a_', reuse = True, squeeze = False)\n",
    "        self.future_actions = self.future_actor.output\n",
    "        \n",
    "        self.future_critic_input = tf.concat((self.future_obs, self.future_actions), -1)\n",
    "        self.future_advantage_estimator = MLP(\n",
    "            self.future_critic_input, lshapes_small, 1, 'c_', lshapes_drop = lshapes_drop, reuse = True)\n",
    "        \n",
    "        if len(self.future_actions.shape) == 2:\n",
    "            self.future_actions = tf.expand_dims(self.future_actions, -1)\n",
    "        \n",
    "        self.t_vars = tf.trainable_variables()\n",
    "        self.a_vars = [var for var in self.t_vars if 'a_' in var.name]\n",
    "        self.v_vars = [var for var in self.t_vars if 'v_' in var.name]\n",
    "        self.c_vars = [var for var in self.t_vars if 'c_' in var.name]\n",
    "        self.e_vars = [var for var in self.t_vars if 'e_' in var.name]\n",
    "        self.m_vars = [var for var in self.t_vars if 'm_' in var.name]\n",
    "        \n",
    "        self.creg, self.areg, self.vreg, self.mreg, self.ereg = [\n",
    "            tf.reduce_mean([tf.reduce_mean(tf.square(v)) for v in optvars]) * 1e0\n",
    "            for optvars in \n",
    "            [self.c_vars, self.a_vars, self.v_vars, self.m_vars, self.e_vars]]\n",
    "        \n",
    "        def mplusv(x):\n",
    "            return x.variance + x.output_mean\n",
    "        \n",
    "        self.explorer_adv_loss = -tf.reduce_mean(mplusv(\n",
    "            self.explorer_advantage_estimator) * self.mask)\n",
    "        self.explorer_mdl_loss = -tf.reduce_mean(\n",
    "            self.explorer_model_estimator.variance * self.maskexpanded)\n",
    "        self.e_mse = tf.reduce_mean(\n",
    "            tf.square(self.explorer.raw_output) * self.maskexpanded) * 1e-1\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model_state_estimate = self.model_estimator.output_mean\n",
    "        \n",
    "        self.v_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.returnsdecayed - self.state_value_estimate) * self.mask * self.is_exploit2d) * 3\n",
    "        \n",
    "        self.m_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.model_estimator.output[:,:-1] - self.statesraw_expanded[:,1:]\n",
    "        ) * self.maskexpanded2[:,:-1]) * 10000\n",
    "        \n",
    "        self.c_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.advantage_estimate[:,:-1] - self.advantage) * self.mask[:,:-1]) * 3\n",
    "        \n",
    "        \n",
    "        self.actionmean = tf.reduce_sum(\n",
    "            self.actions * self.maskexpanded, 1) / tf.reduce_sum(\n",
    "            self.maskexpanded, 1) + tf.reduce_sum(\n",
    "            self.future_actions * self.maskexpanded, 1) / tf.reduce_sum(\n",
    "            self.maskexpanded, 1) \n",
    "        \n",
    "        \n",
    "        self.actionmean = tf.reduce_sum(\n",
    "            self.actions * self.maskexpanded, 1) / tf.reduce_sum(\n",
    "            self.maskexpanded, 1)\n",
    "        self.actdiffsquared = tf.square(\n",
    "            self.actions - tf.expand_dims(self.actionmean, 1))\n",
    "        self.actvar = tf.reduce_sum(\n",
    "            self.actdiffsquared * self.maskexpanded, 1)/ tf.reduce_sum(\n",
    "            self.maskexpanded, 1)\n",
    "        self.actstd = self.actvar\n",
    "        self.actstdpenalty = -tf.reduce_mean(\n",
    "            tf.log(self.actstd + .00001)* .00001 + tf.square(self.actstd)) * .001\n",
    "        \n",
    "        self.aregmeanbyaction = tf.reduce_mean(\n",
    "            tf.square(self.actionmean)) * 1e3\n",
    "        self.a_mse = tf.reduce_mean(\n",
    "            tf.square(self.actor.raw_output) * 1e2 + \n",
    "            tf.square(self.future_actor.raw_output) * 1e-1 +\n",
    "            tf.square(tf.square(self.actor.raw_output)) * 1e1 + \n",
    "            tf.square(tf.square(self.future_actor.raw_output)) * 1e-3\n",
    "            )\n",
    "            \n",
    "        self.frac_not_masked = tf.reduce_mean(self.mask)\n",
    "        \n",
    "        self.a_loss_critic = -tf.reduce_mean(\n",
    "            self.advantage_estimator.output_mean * self.mask)/self.frac_not_masked * 2000\n",
    "        self.a_loss_model = -tf.reduce_mean(\n",
    "            self.future_value.output * self.mask)/self.frac_not_masked * 1.\n",
    "        self.a_loss_secondaction = -tf.reduce_mean(\n",
    "            self.future_advantage_estimator.output_mean * self.mask)/self.frac_not_masked * .1\n",
    "        \n",
    "        self.grad_v = tf.square(tf.gradients(\n",
    "            self.state_value_estimator.output * self.mask, self.obs)[0])\n",
    "        self.grad_m = tf.square(tf.gradients(\n",
    "            self.model_estimator.output * self.maskexpanded2, self.critic_input)[0])\n",
    "        self.grad_c = tf.square(tf.gradients(\n",
    "            self.advantage_estimator.output * self.maskexpanded, self.critic_input)[0])\n",
    "        \n",
    "        self.grad_norm_m = tf.reduce_mean(self.grad_m) * 1e-2\n",
    "        self.grad_norm_v = tf.reduce_mean(self.grad_v) * 1e-4\n",
    "        self.grad_norm_c = tf.reduce_mean(self.grad_c) * 1e-2\n",
    "    \n",
    "        slopes = tf.reduce_sum(tf.square(self.grad_c), reduction_indices=[2])\n",
    "        self.grad_c_1 = tf.reduce_mean(self.mask * (slopes - .2) ** 2) * 1e-3\n",
    "        slopes = tf.reduce_sum(tf.square(self.grad_m), reduction_indices=[2])\n",
    "        self.grad_m_1 = tf.reduce_mean(self.mask * (slopes - .2) ** 2) * 1e-5\n",
    "        \n",
    "        \n",
    "        self.critic_opt = tf.train.AdamOptimizer(self.lr, beta1= .8)\n",
    "        self.value_opt = tf.train.AdamOptimizer(self.lr, beta1= .8)\n",
    "        self.actor_opt = tf.train.AdamOptimizer(self.lr, beta1= .8)\n",
    "        self.model_opt = tf.train.AdamOptimizer(self.lr, beta1= .8)\n",
    "        self.explorer_opt = tf.train.AdamOptimizer(self.lr, beta1= .8)\n",
    "        \n",
    "        \n",
    "        self.actor_current_mult = []\n",
    "        self.critic_current_mult = []\n",
    "        self.actor_loss_mults = []\n",
    "        self.critic_loss_mults = []\n",
    "        self.critic_loss_grads = []\n",
    "        self.actor_loss_grads = []\n",
    "        self.actor_loss_targets = np.array([30., 1., .1,1e-6, .1, .01, .01,\n",
    "                                           10., 1., 1e-6, .01])\n",
    "        self.actor_loss_names = ['a_loss_critic', 'a_loss_model', 'a_loss_secondaction', \n",
    "                    'areg', 'aregmeanbyaction', 'actstdpenalty', 'a_mse',\n",
    "                    'explorer_adv_loss', 'explorer_mdl_loss', 'ereg',\n",
    "                     'e_mse'\n",
    "                                ]\n",
    "        self.critic_loss_targets = np.array(\n",
    "            [10, 10, 10,\n",
    "             .001, .001, .001, \n",
    "            .001, .001,\n",
    "            1e-8, 1e-8, 1e-8])\n",
    "        self.critic_loss_names = [\n",
    "            'v_loss_raw','m_loss_raw', 'c_loss_raw',\n",
    "            'grad_norm_c',  'grad_norm_v', 'grad_norm_m', \n",
    "            'grad_m_1', 'grad_c_1',\n",
    "             'creg','vreg', 'mreg']\n",
    "        self.aopts = [self.actor_opt] * 7 + [self.explorer_opt] * 4\n",
    "        self.aoptnames = ['a_'] * 7 + ['e_'] * 4\n",
    "        self.copts = [self.value_opt, self.model_opt, self.critic_opt, \n",
    "                      self.critic_opt, self.value_opt,\n",
    "                      self.model_opt, self.model_opt, self.critic_opt, \n",
    "                      self.critic_opt, self.value_opt,self.model_opt]\n",
    "        self.coptnames = [v + '_' for v in ['v', 'm', 'c', 'c', 'v', 'm', 'm', 'c', 'c', 'v', 'm']]\n",
    "        for lidx, loss in enumerate(self.actor_loss_names):\n",
    "            opt = self.aopts[lidx]\n",
    "            optname = self.aoptnames[lidx]\n",
    "            loss_mult = tf.placeholder_with_default(1., (None))\n",
    "            self.actor_loss_mults.append(loss_mult)\n",
    "            setattr(self, loss, getattr(self, loss) * loss_mult)\n",
    "            self.actor_loss_grads.append(get_grad_norm(opt, getattr(self, loss), optname))\n",
    "            self.actor_current_mult.append(1)\n",
    "            if loss == 'a_mse':\n",
    "                self.amse_mult = loss_mult\n",
    "            if loss == 'e_mse':\n",
    "                self.emse_mult = loss_mult\n",
    "        for lidx, loss in enumerate(self.critic_loss_names):\n",
    "            opt = self.copts[lidx]\n",
    "            optname = self.coptnames[lidx]\n",
    "            loss_mult = tf.placeholder_with_default(1., (None))\n",
    "            self.critic_loss_mults.append(loss_mult)\n",
    "            setattr(self, loss, getattr(self, loss) * loss_mult)\n",
    "            self.critic_loss_grads.append(get_grad_norm(opt, getattr(self, loss), optname))\n",
    "            self.critic_current_mult.append(1)\n",
    "        self.actor_current_mult = np.array(self.actor_current_mult)\n",
    "        self.critic_current_mult = np.array(self.critic_current_mult)\n",
    "        \n",
    "        self.v_loss =  self.v_loss_raw + self.vreg\n",
    "        self.m_loss = self.m_loss_raw + self.mreg\n",
    "        self.c_loss =  self.c_loss_raw + self.creg\n",
    "        self.aregtotal = self.areg + self.aregmeanbyaction + self.actstdpenalty + self.a_mse\n",
    "        self.a_loss_raw = self.a_loss_critic + self.a_loss_model + \\\n",
    "            self.a_loss_secondaction\n",
    "        self.a_loss = self.a_loss_raw + self.aregtotal\n",
    "        \n",
    "        \n",
    "        self.e_loss_minimize = self.explorer_adv_loss + self.explorer_mdl_loss + self.e_mse + self.ereg\n",
    "        self.a_loss_minimize = self.a_loss\n",
    "        self.c_loss_minimize = self.c_loss + self.grad_norm_c + self.grad_c_1\n",
    "        self.v_loss_minimize = self.v_loss + self.grad_norm_v\n",
    "        self.m_loss_minimize = self.m_loss + self.grad_norm_m + self.grad_m_1\n",
    "        \n",
    "        \n",
    "#         if TESTING_GRAD_NORMS:\n",
    "#             self.a_grads = [\n",
    "#                 get_grad_norm(self.actor_opt, l) for l in [\n",
    "#                 self.a_loss_minimize, self.a_loss_raw, self.a_loss_critic,self.a_loss_model,\n",
    "#                 self.a_loss_secondaction,\n",
    "#                 self.areg, self.aregmeanbyaction, self.actstdpenalty, self.a_mse\n",
    "#             ]]\n",
    "#             self.v_grads = [get_grad_norm(self.value_opt, l) for l in [\n",
    "#                 self.v_loss_minimize, self.v_loss_raw, \n",
    "#                 self.vreg, self.grad_norm_v\n",
    "#             ]]\n",
    "#             self.c_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "#                 self.c_loss_minimize, self.c_loss_raw, \n",
    "#                 self.creg, self.grad_norm_c, self.grad_c_1\n",
    "\n",
    "#             ]]\n",
    "#             self.m_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "#                 self.m_loss_minimize, self.m_loss_raw, \n",
    "#                 self.mreg, self.grad_norm_m, self.grad_m_1\n",
    "#             ]]\n",
    "        \n",
    "        self.copt, self.c_norm = apply_clipped_optimizer(\n",
    "            self.critic_opt, self.c_loss_minimize, var_list = self.c_vars)\n",
    "        self.vopt, self.v_norm = apply_clipped_optimizer(\n",
    "            self.value_opt, self.v_loss_minimize, var_list = self.v_vars)\n",
    "        self.aopt, self.a_norm = apply_clipped_optimizer(\n",
    "            self.actor_opt, self.a_loss_minimize, var_list = self.a_vars)\n",
    "        self.mopt, self.m_norm = apply_clipped_optimizer(\n",
    "            self.model_opt, self.m_loss_minimize, var_list = self.m_vars)\n",
    "        self.eopt, self.e_norm = apply_clipped_optimizer(\n",
    "            self.explorer_opt, self.e_loss_minimize, var_list = self.e_vars)\n",
    "\n",
    "    def a_name(self):\n",
    "        self.a_idx += 1\n",
    "        return 'a_' + str(self.a_idx)\n",
    "    def c_name(self):\n",
    "        self.c_idx += 1\n",
    "        return 'c_' + str(self.c_idx)\n",
    "    def v_name(self):\n",
    "        self.v_idx += 1\n",
    "        return 'v_' + str(self.v_idx)\n",
    "    def m_name(self):\n",
    "        self.m_idx += 1\n",
    "        return 'm_' + str(self.m_idx)\n",
    "    def e_name(self):\n",
    "        self.e_idx += 1\n",
    "        return 'e_' + str(self.e_idx)\n",
    "    \n",
    "    def load_weights(self):\n",
    "        feed_dict = {}\n",
    "        for (var, w), ph in zip(\n",
    "            self.assigns, self.weight_assignment_placeholders):\n",
    "            feed_dict[ph] = w\n",
    "        self.sess.run(self.weight_assignment_nodes, feed_dict=feed_dict)\n",
    "\n",
    "    def act(self, obs, exploit = True):\n",
    "        # Because we need batch dimension, \n",
    "        # data[None] changes shape from [A] to [1,A]\n",
    "        if exploit:\n",
    "            act = self.actions\n",
    "        else:\n",
    "            act = self.explorer_actions\n",
    "        a = self.sess.run(\n",
    "            act, feed_dict={\n",
    "                self.obs_raw:np.reshape(obs, (1, 1, N_HISTORY, N_STATE)),\n",
    "                self.is_train:False\n",
    "            })\n",
    "        return a[0][0]  # return first in batch\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    inter_op_parallelism_threads=0,\n",
    "    intra_op_parallelism_threads=0,\n",
    "    device_count = { \"GPU\": 0 } )\n",
    "tf.reset_default_graph()\n",
    "\n",
    "pi = PolicyLearner(env.observation_space, env.action_space, config = config)\n",
    "\n",
    "sess = pi.sess\n",
    "self = pi\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#trainer = ZooPolicyTensorflow(\n",
    "# \"mymodel1\", env.observation_space, env.action_space)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()\n",
    "\n",
    "\n",
    "# env = gym.make(envname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.actor_current_mult = np.ones_like(self.actor_current_mult,dtype=np.float32)\n",
    "self.critic_current_mult = np.ones_like(self.critic_current_mult,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#         self.advantage = ((\n",
    "#             self.stateraw_value_estimate[:,1:] * GAMMA + self.returns) -\n",
    "#             self.state_value_estimate[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ah, sh, shraw, rh, rdecayedh, maskh = [\n",
    "    np.zeros((0, 0, i)) for i in [N_ACT, INPUT_UNITS, N_OBS, 1, 1, 1]]\n",
    "isexploit = []\n",
    "globalframes = []\n",
    "localframes = []\n",
    "ep = 0\n",
    "trained = 0\n",
    "ongoing = 0\n",
    "printfreq = 20\n",
    "obj_fname = envname + str(PERCENT_CHOOSE_OPTIMAL) + 'saveobjs_unguided.pkl'\n",
    "tffile = \"tmp/\" + envname + str(PERCENT_CHOOSE_OPTIMAL) + \"unguided_trained.ckpt\"\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ongoing = 0\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ongoing:\n",
    "    \n",
    "    save_path = saver.save(sess, tffile)\n",
    "    print('saved at epoch', ep)\n",
    "    with open(obj_fname,\"wb\") as f:\n",
    "        pickle.dump(\n",
    "            [ah, sh, rh, rdecayedh, maskh, ep, globalframes, isexploit\n",
    "            ], f)\n",
    "    trained = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_actor(self, feed_dict, printon):\n",
    "    _, _, aloss = sess.run(\n",
    "        [self.aopt, self.eopt, self.a_loss],\n",
    "        feed_dict = feed_dict\n",
    "            )\n",
    "    if TESTING_GRAD_NORMS and ep % printfreq == 0 and printon and ep > INIT_LEN:\n",
    "        cur_grad_norms = sess.run(self.actor_loss_grads, feed_dict)\n",
    "        target = self.actor_loss_targets\n",
    "        problem_factor = cur_grad_norms / target\n",
    "        for fidx, factor in enumerate(problem_factor):\n",
    "            if fidx == len(problem_factor) - 1:\n",
    "                pass#pdb.set_trace()\n",
    "            if factor > 100:\n",
    "                self.actor_current_mult[fidx] = self.actor_current_mult[fidx] / 10\n",
    "            elif factor > 10:\n",
    "                self.actor_current_mult[fidx] = self.actor_current_mult[fidx] / 2\n",
    "            elif factor < .01:\n",
    "                self.actor_current_mult[fidx] = self.actor_current_mult[fidx] * 10\n",
    "            elif factor < .1:\n",
    "                self.actor_current_mult[fidx] = self.actor_current_mult[fidx] * 2\n",
    "            self.actor_current_mult[fidx] = np.clip(self.actor_current_mult[fidx], 1e-5, 1e5)\n",
    "        print(list(zip(self.actor_loss_names, cur_grad_norms, target, self.actor_current_mult)))\n",
    "    return aloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(actor = True, value = True, n_steps = 1, forced_hist = 2):\n",
    "    for itr in range(n_steps):\n",
    "        if num_hist >  batch_size:\n",
    "            #probability = np.arange(num_hist - forced_hist)\n",
    "            probability = num_steps_per_run = (\n",
    "                maskh.shape[1] - maskh[:,::-1].argmax(1))\n",
    "            if forced_hist:\n",
    "                probability = probability[:-forced_hist]\n",
    "            probability = np.square(probability)\n",
    "            probability = probability / probability.sum()\n",
    "            rnd = np.random.choice(\n",
    "                    num_hist - forced_hist, batch_size - forced_hist, \n",
    "                    replace=False, p=probability)\n",
    "            if forced_hist:\n",
    "                samples = np.concatenate((rnd,\n",
    "                    np.arange( num_hist - forced_hist, num_hist)))\n",
    "            else:\n",
    "                samples=rnd\n",
    "        else:\n",
    "            samples = np.random.choice(num_hist, num_hist, replace=False)\n",
    "        #samples = np.array([-2, -1])\n",
    "        actions, states, statesraw, returns, returnsdecayed, mask, exploit = [\n",
    "            v[samples] for v in [ah, sh, shraw, rh,rdecayedh, maskh, np.array(isexploit)]]\n",
    "        \n",
    "        feed_dict={\n",
    "                    self.returns:returns,\n",
    "                    self.statesraw: statesraw,\n",
    "                    self.returnsdecayed:returnsdecayed,\n",
    "                    self.lr: .02 / np.power(ep + 20, .4),\n",
    "                    self.mask:mask,\n",
    "                    self.obs_raw: states.reshape(*states.shape[:2], N_HISTORY, N_STATE),\n",
    "                    self.is_exploit:exploit,\n",
    "                    self.actions:actions\n",
    "        }\n",
    "        for lph, lmult in zip(self.actor_loss_mults, self.actor_current_mult):\n",
    "            feed_dict[lph] = lmult\n",
    "        for lph, lmult in zip(self.critic_loss_mults, self.critic_current_mult):\n",
    "            feed_dict[lph] = lmult\n",
    "        self.amse_mult = 1.\n",
    "        self.emse_mult = 1.\n",
    "        if value:\n",
    "            if exploit.sum() > 1:\n",
    "                _,_, _, mloss, closs, vloss = sess.run(\n",
    "                    [self.mopt, self.copt,self.vopt, self.m_loss, self.c_loss, self.v_loss],\n",
    "                        feed_dict=feed_dict)\n",
    "            else:\n",
    "                _, mloss = sess.run(\n",
    "                    [self.mopt, self.m_loss],\n",
    "                        feed_dict=feed_dict)\n",
    "                closs = vloss = 0\n",
    "        else:\n",
    "            mloss = closs =  vloss = 0\n",
    "        del feed_dict[self.actions]\n",
    "        if actor and exploit.sum() > 1:\n",
    "            aloss = train_actor(self, feed_dict, printon = itr == 0)\n",
    "        else:\n",
    "            aloss = 0\n",
    "    if 1:\n",
    "        if ep % printfreq == 0 and exploit.sum() > 1:\n",
    "            print('aloss', aloss, 'closs', closs, 'vloss', vloss, 'mloss', mloss)\n",
    "            if TESTING_GRAD_NORMS and ep > INIT_LEN:\n",
    "                \n",
    "                cur_grad_norms = sess.run(self.critic_loss_grads, feed_dict)\n",
    "                target = self.critic_loss_targets\n",
    "                problem_factor = cur_grad_norms / target\n",
    "                for fidx, factor in enumerate(problem_factor):\n",
    "                    if factor > 100:\n",
    "                        self.critic_current_mult[fidx] = self.critic_current_mult[fidx] / 10\n",
    "                    elif factor > 10:\n",
    "                        self.critic_current_mult[fidx] = self.critic_current_mult[fidx] / 2\n",
    "                    elif factor < .01:\n",
    "                        self.critic_current_mult[fidx] = self.critic_current_mult[fidx] * 10\n",
    "                    elif factor < .1:\n",
    "                        self.critic_current_mult[fidx] = self.critic_current_mult[fidx] * 2\n",
    "                print(list(zip(\n",
    "                    self.critic_loss_names, cur_grad_norms, target, self.critic_current_mult)))\n",
    "    if ep % printfreq == 0:\n",
    "        print(' ep, ', ep, ' avg frames',np.mean(globalframes[-20:]))\n",
    "        print('abs action',np.abs(ah)[-1,0,:].shape, np.abs(ah)[-1,0,:].mean())\n",
    "        print('max reward',np.max(rh[-10:,10:]))\n",
    "    if ep % 10000 == 0:\n",
    "        clear_output()\n",
    "    if ep % 100 == 0:\n",
    "        ysmoothed = gaussian_filter1d(globalframes, sigma=4)\n",
    "        plt.plot(ysmoothed)\n",
    "        plt.show()\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(1):\n",
    "#     feed_dict = train(actor = True, value = True, n_steps = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploit = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 44.0  frames 22\n",
      "saved at epoch 100\n",
      "saved at epoch 200\n",
      "aloss -1056.8217 closs 13.939738 vloss 9.380328 mloss 70.26559\n",
      "[('v_loss_raw', 0.0017752616, 10.0, 10.0), ('m_loss_raw', 7.412723, 10.0, 1.0), ('c_loss_raw', 0.0013809875, 10.0, 10.0), ('grad_norm_c', 3.2501362e-09, 0.001, 10.0), ('grad_norm_v', 3.5187773e-13, 0.001, 10.0), ('grad_norm_m', 1.4013674e-09, 0.001, 10.0), ('grad_m_1', 0.036250807, 0.001, 0.5), ('grad_c_1', 0.000112994945, 0.001, 1.0), ('creg', 1.5132101e-09, 1e-08, 1.0), ('vreg', 1.1386301e-07, 1e-08, 0.5), ('mreg', 2.1077213e-09, 1e-08, 1.0)]\n",
      " ep,  220  avg frames 16.360148514851485\n",
      "abs action (1,) 0.35146987438201904\n",
      "max reward 2.0\n",
      "aloss -2482.2063 closs 126.49638 vloss 2407.6174 mloss 77.38941\n",
      "[('v_loss_raw', 7.5709662, 10.0, 10.0), ('m_loss_raw', 6.764461, 10.0, 1.0), ('c_loss_raw', 0.034719177, 10.0, 100.0), ('grad_norm_c', 3.2931218e-06, 0.001, 100.0), ('grad_norm_v', 0.35295826, 0.001, 1.0), ('grad_norm_m', 1.7659373e-07, 0.001, 100.0), ('grad_m_1', 0.0037187284, 0.001, 0.5), ('grad_c_1', 0.15846889, 0.001, 0.1), ('creg', 2.9420258e-08, 1e-08, 1.0), ('vreg', 4.54388e-06, 1e-08, 0.05), ('mreg', 3.4847767e-09, 1e-08, 1.0)]\n",
      " ep,  240  avg frames 32.6\n",
      "abs action (1,) 1.3640358448028564\n",
      "max reward 2.0\n",
      "[('a_loss_critic', 611.596, 30.0, 0.5), ('a_loss_model', 0.0030987293, 1.0, 10.0), ('a_loss_secondaction', 5.153516e-07, 0.1, 10.0), ('areg', 1.02503286e-07, 1e-06, 1.0), ('aregmeanbyaction', 311.66623, 0.1, 0.1), ('actstdpenalty', 1.148761e-10, 0.01, 10.0), ('a_mse', 202.18762, 0.01, 0.1), ('explorer_adv_loss', 127.810486, 10.0, 0.5), ('explorer_mdl_loss', 0.014580578, 1.0, 2.0), ('ereg', 5.0739895e-06, 1e-06, 1.0), ('e_mse', 3.0631084, 0.01, 0.1)]\n",
      "aloss -1202.7059 closs 1427.699 vloss 3289.3162 mloss 50.334602\n",
      "[('v_loss_raw', 4.4203463, 10.0, 10.0), ('m_loss_raw', 2.7472458, 10.0, 1.0), ('c_loss_raw', 4.6912994, 10.0, 100.0), ('grad_norm_c', 0.0007609117, 0.001, 100.0), ('grad_norm_v', 0.012199553, 0.001, 0.5), ('grad_norm_m', 2.5266245e-05, 0.001, 200.0), ('grad_m_1', 0.01771638, 0.001, 0.25), ('grad_c_1', 0.7380857, 0.001, 0.01), ('creg', 3.4299497e-08, 1e-08, 1.0), ('vreg', 4.9852652e-08, 1e-08, 0.05), ('mreg', 5.5503944e-09, 1e-08, 1.0)]\n",
      " ep,  260  avg frames 38.8\n",
      "abs action (1,) 2.3789501190185547\n",
      "max reward 2.0\n",
      "[('a_loss_critic', 31.818567, 30.0, 0.5), ('a_loss_model', 0.013740656, 1.0, 20.0), ('a_loss_secondaction', 1.7471912e-06, 0.1, 100.0), ('areg', 1.893648e-07, 1e-06, 1.0), ('aregmeanbyaction', 20.305223, 0.1, 0.01), ('actstdpenalty', 4.2013085e-08, 0.01, 100.0), ('a_mse', 30.005327, 0.01, 0.01), ('explorer_adv_loss', 824.23, 10.0, 0.25), ('explorer_mdl_loss', 190.8669, 1.0, 0.2), ('ereg', 1.1468059e-05, 1e-06, 0.5), ('e_mse', 22.023544, 0.01, 0.01)]\n",
      "aloss -1502.721 closs 1891.751 vloss 4056.475 mloss 41.907356\n",
      "[('v_loss_raw', 3.8887525, 10.0, 10.0), ('m_loss_raw', 1.3143815, 10.0, 1.0), ('c_loss_raw', 33.688545, 10.0, 100.0), ('grad_norm_c', 0.006600072, 0.001, 100.0), ('grad_norm_v', 1.227687e-09, 0.001, 5.0), ('grad_norm_m', 5.4259497e-05, 0.001, 400.0), ('grad_m_1', 0.0002985083, 0.001, 0.25), ('grad_c_1', 119.471085, 0.001, 0.0009999999), ('creg', 3.973927e-08, 1e-08, 1.0), ('vreg', 4.488128e-08, 1e-08, 0.05), ('mreg', 1.0397841e-08, 1e-08, 1.0)]\n",
      " ep,  280  avg frames 38.05\n",
      "abs action (1,) 4.17834997177124\n",
      "max reward 2.0\n",
      "saved at epoch 300\n",
      "[('a_loss_critic', 206.35176, 30.0, 0.5), ('a_loss_model', 0.017835269, 1.0, 40.0), ('a_loss_secondaction', 3.553331e-05, 0.1, 1000.0), ('areg', 1.9128633e-07, 1e-06, 1.0), ('aregmeanbyaction', 0.82746077, 0.1, 0.01), ('actstdpenalty', 0.00014370079, 0.01, 200.0), ('a_mse', 490.92603, 0.01, 0.0009999999), ('explorer_adv_loss', 41529.6, 10.0, 0.025), ('explorer_mdl_loss', 9.369066, 1.0, 0.2), ('ereg', 5.0226536e-06, 1e-06, 0.5), ('e_mse', 3871.1729, 0.01, 0.0009999999)]\n",
      "aloss -2386.447 closs 2528.8928 vloss 1593.1488 mloss 45.18063\n",
      "[('v_loss_raw', 0.21826012, 10.0, 20.0), ('m_loss_raw', 0.5188887, 10.0, 2.0), ('c_loss_raw', 650.11786, 10.0, 50.0), ('grad_norm_c', 0.026198601, 0.001, 50.0), ('grad_norm_v', 0.000500848, 0.001, 5.0), ('grad_norm_m', 7.144556e-05, 0.001, 800.0), ('grad_m_1', 0.0006179341, 0.001, 0.25), ('grad_c_1', 128.4996, 0.001, 9.999999e-05), ('creg', 3.89229e-08, 1e-08, 1.0), ('vreg', 3.6359033e-08, 1e-08, 0.05), ('mreg', 1.9364004e-08, 1e-08, 1.0)]\n",
      " ep,  300  avg frames 24.95\n",
      "abs action (1,) 0.5205812523609695\n",
      "max reward 2.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a_loss_critic', 368.98044, 30.0, 0.25), ('a_loss_model', 9.634305, 1.0, 40.0), ('a_loss_secondaction', 0.004400201, 0.1, 2000.0), ('areg', 5.0397693e-07, 1e-06, 1.0), ('aregmeanbyaction', 1.543263, 0.1, 0.005), ('actstdpenalty', 0.0023566391, 0.01, 200.0), ('a_mse', 436.3297, 0.01, 9.999999e-05), ('explorer_adv_loss', 39556.03, 10.0, 0.0025), ('explorer_mdl_loss', 11850.719, 1.0, 0.02), ('ereg', 7.74093e-06, 1e-06, 0.5), ('e_mse', 7363.1616, 0.01, 9.999999e-05)]\n",
      "aloss -1350.4484 closs 1400.5237 vloss 6464.8813 mloss 107.139305\n",
      "[('v_loss_raw', 2.6021385, 10.0, 20.0), ('m_loss_raw', 4.487883, 10.0, 2.0), ('c_loss_raw', 578.3931, 10.0, 25.0), ('grad_norm_c', 0.018005222, 0.001, 25.0), ('grad_norm_v', 1.857762e-09, 0.001, 50.0), ('grad_norm_m', 0.00013630472, 0.001, 800.0), ('grad_m_1', 0.000100057405, 0.001, 0.25), ('grad_c_1', 76.08788, 0.001, 9.999999e-06), ('creg', 4.3260645e-08, 1e-08, 1.0), ('vreg', 3.3585525e-08, 1e-08, 0.05), ('mreg', 3.4208945e-08, 1e-08, 1.0)]\n",
      " ep,  320  avg frames 23.85\n",
      "abs action (1,) 0.7919937213743009\n",
      "max reward 2.0\n",
      "[('a_loss_critic', 146.5696, 30.0, 0.25), ('a_loss_model', 0.09339043, 1.0, 80.0), ('a_loss_secondaction', 0.004263963, 0.1, 4000.0), ('areg', 6.9954194e-07, 1e-06, 1.0), ('aregmeanbyaction', 0.39769268, 0.1, 0.005), ('actstdpenalty', 0.006466236, 0.01, 200.0), ('a_mse', 71.1746, 0.01, 1e-05), ('explorer_adv_loss', 1951.9277, 10.0, 0.00024999998), ('explorer_mdl_loss', 20.752268, 1.0, 0.01), ('ereg', 1.0953759e-05, 1e-06, 0.25), ('e_mse', 1011.12494, 0.01, 1e-05)]\n",
      "aloss -2308.7415 closs 728.76135 vloss 6478.7314 mloss 90.61362\n",
      "[('v_loss_raw', 3.7819393, 10.0, 20.0), ('m_loss_raw', 6.660885, 10.0, 2.0), ('c_loss_raw', 227.42342, 10.0, 12.5), ('grad_norm_c', 0.011632302, 0.001, 12.5), ('grad_norm_v', 0.31834716, 0.001, 5.0), ('grad_norm_m', 0.000144431, 0.001, 800.0), ('grad_m_1', 6.4824395e-05, 0.001, 0.5), ('grad_c_1', 52.613777, 0.001, 9.999999e-07), ('creg', 4.9361233e-08, 1e-08, 1.0), ('vreg', 2.9464e-08, 1e-08, 0.05), ('mreg', 5.730747e-08, 1e-08, 1.0)]\n",
      " ep,  340  avg frames 23.0\n",
      "abs action (1,) 4.283105850219727\n",
      "max reward 2.0\n",
      "[('a_loss_critic', 83.580315, 30.0, 0.25), ('a_loss_model', 0.45007968, 1.0, 80.0), ('a_loss_secondaction', 0.008239967, 0.1, 8000.0), ('areg', 1.1206852e-06, 1e-06, 1.0), ('aregmeanbyaction', 0.5186999, 0.1, 0.005), ('actstdpenalty', 0.0265027, 0.01, 200.0), ('a_mse', 61.50546, 0.01, 1e-05), ('explorer_adv_loss', 57.911205, 10.0, 0.00024999998), ('explorer_mdl_loss', 2902.4617, 1.0, 0.0009999999), ('ereg', 3.630965e-06, 1e-06, 0.25), ('e_mse', 47.322742, 0.01, 1e-05)]\n",
      "aloss -2607.2217 closs 442.91107 vloss 7239.295 mloss 98.10838\n",
      "[('v_loss_raw', 6.508559, 10.0, 20.0), ('m_loss_raw', 17.42023, 10.0, 2.0), ('c_loss_raw', 116.51299, 10.0, 6.25), ('grad_norm_c', 0.0051070475, 0.001, 12.5), ('grad_norm_v', 1.9484584, 0.001, 0.5), ('grad_norm_m', 0.0007318013, 0.001, 800.0), ('grad_m_1', 2.8042023, 0.001, 0.05), ('grad_c_1', 6.735443, 0.001, 9.999999e-08), ('creg', 5.5154803e-08, 1e-08, 1.0), ('vreg', 2.7902066e-08, 1e-08, 0.05), ('mreg', 8.143473e-08, 1e-08, 1.0)]\n",
      " ep,  360  avg frames 17.3\n",
      "abs action (1,) 4.130815505981445\n",
      "max reward 2.0\n",
      "[('a_loss_critic', 182.69757, 30.0, 0.25), ('a_loss_model', 0.27222374, 1.0, 80.0), ('a_loss_secondaction', 0.0070533925, 0.1, 16000.0), ('areg', 1.3038201e-06, 1e-06, 1.0), ('aregmeanbyaction', 0.8715549, 0.1, 0.005), ('actstdpenalty', 0.011231057, 0.01, 200.0), ('a_mse', 137.01088, 0.01, 1e-05), ('explorer_adv_loss', 79.58462, 10.0, 0.00024999998), ('explorer_mdl_loss', 0.5209021, 1.0, 0.0009999999), ('ereg', 4.6557234e-06, 1e-06, 0.25), ('e_mse', 40.59953, 0.01, 1e-05)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-69dcde1b8482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m#train(value = False, n_steps = 20, forced_hist=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-7d4da6f38f0d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(actor, value, n_steps, forced_hist)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 _,_, _, mloss, closs, vloss = sess.run(\n\u001b[1;32m     44\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                         feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 _, mloss = sess.run(\n",
      "\u001b[0;32m/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if trained:\n",
    "    try:\n",
    "        saver.restore(sess, tffile)\n",
    "        with open(obj_fname, \"rb\") as f:\n",
    "            ah, sh, shraw, rh, rdecayedh, maskh, ep, globalframes, isexploit = pickle.load(f)\n",
    "        print('restored from save file')\n",
    "    except:\n",
    "        print('no save file detected')\n",
    "        trained = 0\n",
    "MAX_SEQ_LEN = 5000\n",
    "for ep in range(ep, 10000000):\n",
    "    if ep % 100 == 0 and trained and ep > 0:\n",
    "        save_path = saver.save(sess, tffile)\n",
    "        print('saved at epoch', ep)\n",
    "        with open(obj_fname,\"wb\") as f:\n",
    "            pickle.dump(\n",
    "                [ah[-NUM_KEEP:], sh[-NUM_KEEP:], shraw[-NUM_KEEP:],\n",
    "                 rh[-NUM_KEEP:], rdecayedh[-NUM_KEEP:], maskh[-NUM_KEEP:], ep, globalframes, isexploit[-NUM_KEEP:]\n",
    "                ], f)\n",
    "    trained = 1\n",
    "    ongoing = 1\n",
    "    \n",
    "    an, sn, snraw, rn, rdecayedn, maskn = [\n",
    "        np.zeros((0, i)) for i in [N_ACT, INPUT_UNITS, N_OBS, 1, 1, 1]]\n",
    "    frame = 0\n",
    "    score = 0\n",
    "    restart_delay = 5\n",
    "    obs = env.reset()\n",
    "    obsraw = obs\n",
    "    snraw = np.concatenate((snraw, obs.reshape(1, -1)), 0)\n",
    "    obs = np.concatenate((obs, np.zeros(N_ACT)))\n",
    "    obs = np.concatenate((obs, np.zeros_like(obs)))\n",
    "    obs_mat = np.concatenate((\n",
    "        obs[None,:],np.zeros((NUM_HISTORY-1, N_STATE))), 0)\n",
    "    rn = [0]\n",
    "    done_ctr = 0\n",
    "    sn = np.concatenate((sn, obs_mat.reshape(1, -1)), 0)\n",
    "    step_num = 0\n",
    "    step_shifted = -1\n",
    "    show_shift = 0\n",
    "    if ep <= INIT_LEN or exploit:\n",
    "        exploit = 0\n",
    "    else:\n",
    "        exploit = 1\n",
    "    if np.random.rand() > .5:\n",
    "        prob_random = PERCENT_CHOOSE_OPTIMAL#2#.95#1 - 1/np.sqrt(ep+1)\n",
    "    else:\n",
    "        prob_random = 2\n",
    "    while 1:\n",
    "        step_num += 1\n",
    "        if ep < INIT_LEN:\n",
    "            a = np.random.randn(N_ACT)\n",
    "        else:\n",
    "            a = pi.act(obs_mat.flatten(), exploit = exploit)\n",
    "            if np.random.rand() < 0.01:\n",
    "                a = np.random.randn(*a.shape)\n",
    "            if (not exploit) and np.random.rand() < .1:\n",
    "                a = np.random.randn(*a.shape)\n",
    "        an = np.concatenate((an, a[None,:]), 0)\n",
    "        snraw = np.concatenate((snraw, obsraw[None,:]), 0)\n",
    "        last_obs = obs\n",
    "#         last_obsraw = obsraw\n",
    "        obs, r, done, _ = env.step(a)\n",
    "        obsraw = obs\n",
    "        r = r + 1 * REWARD_MULT\n",
    "\n",
    "        obs_mat = accumulate_state(\n",
    "            obs, a, obs_mat, STATE_DECAY, style = 'np')\n",
    "        \n",
    "        rn.append(r)\n",
    "        sn = np.concatenate((sn, obs_mat.reshape(1, -1)), 0)\n",
    "        score += r\n",
    "        frame += 1\n",
    "        if ep > INIT_LEN:\n",
    "            still_open = env.render(\"human\")\n",
    "        else:\n",
    "            still_open = 1\n",
    "        if done:\n",
    "            done_ctr += 1\n",
    "            if done_ctr > 3:\n",
    "                if ep % MAX_SEQ_LEN == 0:\n",
    "                    print('score', score, ' frames', frame)\n",
    "                break\n",
    "        if still_open==False:\n",
    "            crashhere\n",
    "        if not done: continue\n",
    "        if restart_delay==0:\n",
    "            print(\"score=%0.2f in %i frames\" % (score, frame))\n",
    "            if still_open!=True:      # not True in multiplayer or non-Roboschool \n",
    "                break\n",
    "            restart_delay = 2000*2  # 2 sec at 60 fps\n",
    "        restart_delay -= 1\n",
    "        if restart_delay==0: \n",
    "            break\n",
    "    a = pi.act(obs_mat.flatten())\n",
    "    an = np.concatenate((an, a[None,:]), 0)\n",
    "    localframes.append(frame)\n",
    "    rn = np.array(rn)\n",
    "    rn[-1] = rn[-1] - 20 * REWARD_MULT\n",
    "    rewards = [0]\n",
    "    for ir in rn[::-1]:\n",
    "        rewards.append(rewards[-1] * GAMMA + ir)\n",
    "    rdecayedn = np.array(rewards)[:0:-1]\n",
    "    lenrdec = len(rdecayedn)\n",
    "#     rdecayedn = rdecayedn + lenrdec\n",
    "#     rdecayedn = rdecayedn - np.arange(lenrdec) * 1.8\n",
    "    maskn = np.ones_like(rn)\n",
    "    if step_shifted > -1:\n",
    "        if step_shifted == 0:\n",
    "            raise ValueError('step shifted not allowed 0')\n",
    "        maskn[:step_shifted - 1] = 0\n",
    "    if ep == 0:\n",
    "        ah, sh, shraw, rh, rdecayedh, maskh = [\n",
    "            np.expand_dims(v, 0) for v in [an, sn, snraw, rn,rdecayedn, maskn]]\n",
    "        isexploit = [exploit]\n",
    "    else:\n",
    "        def get_updated_h(h, n, third_dim):\n",
    "            hshape = h.shape[1]\n",
    "            nshape = n.shape[0]\n",
    "            if third_dim:\n",
    "                if hshape > nshape:\n",
    "                    n = np.concatenate((n, np.zeros((hshape - nshape, n.shape[-1]))), 0)\n",
    "                if nshape > hshape:\n",
    "                    h = np.concatenate((h, np.zeros((\n",
    "                        h.shape[0], nshape - hshape, h.shape[-1]))), 1)\n",
    "            else:\n",
    "                if hshape > nshape:\n",
    "                    n = np.concatenate((n, np.zeros((hshape - nshape))), 0)\n",
    "                if nshape > hshape:\n",
    "                    h = np.concatenate((h, np.zeros((h.shape[0], nshape - hshape))), 1)\n",
    "            h = np.concatenate((h, np.expand_dims(n, 0)), 0)\n",
    "            return h\n",
    "            \n",
    "        ah, sh, shraw = [get_updated_h(h, n, 1) for  h, n in zip(\n",
    "            [ah, sh, shraw], [an, sn, snraw])]\n",
    "        \n",
    "        rh, rdecayedh, maskh = [\n",
    "            get_updated_h(h, n, 0) for h, n in zip(\n",
    "                [rh, rdecayedh, maskh], [rn, rdecayedn, maskn])]\n",
    "        isexploit.append(exploit)\n",
    "    if ep % 1 == 0 and ep > INIT_LEN:\n",
    "        ah, sh, shraw, rh,rdecayedh, maskh, isexploit = [\n",
    "            v[-NUM_KEEP:] for v in [ah, sh, shraw, rh,rdecayedh, maskh, isexploit]]\n",
    "        globalframes.append(np.mean(localframes))\n",
    "        localframes = []\n",
    "        batch_size = 32\n",
    "        if ep < batch_size:\n",
    "            batch_size = ep\n",
    "        num_hist = ah.shape[0]\n",
    "        if ep == INIT_LEN + 1 or sum(isexploit) < 2:\n",
    "            train(actor = False, n_steps = N_PRETRAIN, forced_hist=0)\n",
    "            #train(value = False, n_steps = 20, forced_hist=0)\n",
    "        else:\n",
    "            feed_dict = train(actor = True, value = True, n_steps = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.702507710220319"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ah[-20::2,:10].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17113504108092786"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ah[-21::2,:10].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isexploit[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        self.v_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.returnsdecayed - self.state_value_estimate) * self.mask * self.is_exploit2d)\n",
    "        self.v_loss =  self.v_loss_raw + self.vreg\n",
    "        \n",
    "        self.m_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.model_estimator.output[:,:-1] - self.statesraw_expanded[:,1:]\n",
    "        ) * self.maskexpanded[:,:-1]) * 100\n",
    "        self.m_loss = self.m_loss_raw + self.mreg\n",
    "        \n",
    "        self.c_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.advantage_estimate[:,:-1] - self.advantage) * self.mask[:,:-1])\n",
    "        self.c_loss =  self.c_loss_raw + self.creg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            self.a_grads = [\n",
    "                get_grad_norm(self.actor_opt, l) for l in [\n",
    "                self.a_loss_minimize, self.a_loss_raw, self.a_loss_critic,self.a_loss_model,\n",
    "                self.a_loss_secondaction,\n",
    "                self.areg, self.aregmeanbyaction, self.actstdpenalty\n",
    "            ]]\n",
    "            self.v_grads = [get_grad_norm(self.value_opt, l) for l in [\n",
    "                self.v_loss_minimize, self.v_loss_raw, \n",
    "                self.vreg, self.grad_norm_v\n",
    "            ]]\n",
    "            self.c_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "                self.c_loss_minimize, self.c_loss_raw, \n",
    "                self.creg, self.grad_norm_c, self.grad_c_1\n",
    "\n",
    "            ]]\n",
    "            self.m_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "                self.m_loss_minimize, self.m_loss_raw, \n",
    "                self.mreg, self.grad_norm_m, self.grad_m_1\n",
    "            ]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
