{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os.path, gym\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import roboschool\n",
    "import pdb\n",
    "GAMMA = .98\n",
    "\n",
    "NUM_HISTORY = 3\n",
    "N_STATE = (44 + 17) * 2\n",
    "INPUT_UNITS = N_STATE * NUM_HISTORY\n",
    "ADV_ENABLED = True\n",
    "\n",
    "\n",
    "class ZooPolicyTensorflow(object):\n",
    "    def __init__(self, name, ob_space, ac_space, take_weights_here=None):\n",
    "        self.name = name\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            obs_tuple = [\n",
    "                tf.placeholder(tf.float32,         (None, 1), name=\"obs0\"),\n",
    "                tf.placeholder(tf.float32,        (None, 44), name=\"obs1\"),\n",
    "            ]\n",
    "            self.obs_tuple = obs_tuple\n",
    "\n",
    "            actions_input = []\n",
    "            actions_input.append(obs_tuple[1])\n",
    "\n",
    "            x = tf.concat( actions_input, axis=1 )\n",
    "            dense1_w = tf.get_variable(\"dense1_w\", [44,256])\n",
    "            dense1_b = tf.get_variable(\"dense1_b\", [256])\n",
    "            x = tf.matmul(x, dense1_w) + dense1_b\n",
    "            x = tf.nn.relu(x)\n",
    "            dense2_w = tf.get_variable(\"dense2_w\", [256,128])\n",
    "            dense2_b = tf.get_variable(\"dense2_b\", [128])\n",
    "            x = tf.matmul(x, dense2_w) + dense2_b\n",
    "            x = tf.nn.relu(x)\n",
    "            final_w = tf.get_variable(\"final_w\", [128,17])\n",
    "            final_b = tf.get_variable(\"final_b\", [17])\n",
    "            x = tf.matmul(x, final_w) + final_b\n",
    "            pi = x\n",
    "            self.pi = pi\n",
    "\n",
    "        if take_weights_here is None:\n",
    "            take_weights_here = {}\n",
    "            exec(open(\n",
    "                \"RoboschoolHumanoidFlagrun_v1_2017jul.weights\").read(), take_weights_here)\n",
    "        self.assigns = [\n",
    "            (  dense1_w, take_weights_here[\"weights_dense1_w\"]),\n",
    "            (  dense1_b, take_weights_here[\"weights_dense1_b\"]),\n",
    "            (  dense2_w, take_weights_here[\"weights_dense2_w\"]),\n",
    "            (  dense2_b, take_weights_here[\"weights_dense2_b\"]),\n",
    "            (   final_w, take_weights_here[\"weights_final_w\"]),\n",
    "            (   final_b, take_weights_here[\"weights_final_b\"]),\n",
    "        ]\n",
    "\n",
    "        self.weight_assignment_placeholders = []\n",
    "        self.weight_assignment_nodes = []\n",
    "        for var, w in self.assigns:\n",
    "            ph = tf.placeholder(tf.float32, w.shape)\n",
    "            self.weight_assignment_placeholders.append(ph)\n",
    "            self.weight_assignment_nodes.append( tf.assign(var, ph) )\n",
    "\n",
    "        self.load_weights()\n",
    "\n",
    "    def load_weights(self):\n",
    "        feed_dict = {}\n",
    "        for (var, w), ph in zip(self.assigns, self.weight_assignment_placeholders):\n",
    "            feed_dict[ph] = w\n",
    "        tf.get_default_session().run(self.weight_assignment_nodes, feed_dict=feed_dict)\n",
    "\n",
    "    def act(self, obs_data, cx):\n",
    "        obs_data = [np.ones((1,)), obs_data]\n",
    "        obs_data = [obs_data[0], obs_data[1]]\n",
    "        # Because we need batch dimension, data[None] changes shape from [A] to [1,A]\n",
    "        a = tf.get_default_session().run(\n",
    "            self.pi, feed_dict=dict(\n",
    "                (ph,data[None]) for ph,data in zip(self.obs_tuple, obs_data) ))\n",
    "        return a[0]  # return first in batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def apply_clipped_optimizer(opt_fcn,\n",
    "                            loss,\n",
    "                            clip_norm=.1,\n",
    "                            clip_single=.03,\n",
    "                            clip_global_norm=False,\n",
    "                            var_list=None):\n",
    "    if var_list is None:\n",
    "        gvs = opt_fcn.compute_gradients(loss)\n",
    "    else:\n",
    "        gvs = opt_fcn.compute_gradients(loss, var_list = var_list)\n",
    "        \n",
    "\n",
    "    if clip_global_norm:\n",
    "        gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "        capped_gs, grad_norm_total = tf.clip_by_global_norm([g for g in gs],clip_norm)\n",
    "        capped_gvs = list(zip(capped_gs, vs))\n",
    "    else:\n",
    "        grad_norm_total = tf.sqrt(\n",
    "                tf.reduce_sum([\n",
    "                        tf.reduce_sum(tf.square(grad)) for grad, var in gvs\n",
    "                        if grad is not None\n",
    "                ]))\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single, clip_single), var)\n",
    "                                    for grad, var in gvs if grad is not None]\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var)\n",
    "                                    for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "    optimizer = opt_fcn.apply_gradients(capped_gvs)\n",
    "\n",
    "    return optimizer, grad_norm_total\n",
    "\n",
    "def MLP(x, lshapes, output_units, name_fcn, is_train):\n",
    "    h = [x]\n",
    "    h.append(tf.nn.leaky_relu(tf.layers.dense(h[-1], lshapes[0], name=name_fcn())))\n",
    "    init = 0\n",
    "    for size in lshapes:\n",
    "        if not init:\n",
    "            init = 1\n",
    "            h2 = h[-1]\n",
    "        h.append(tf.nn.leaky_relu(h[-1] + tf.layers.dense(\n",
    "            h2, size, name=name_fcn())))\n",
    "#         h[-1] = tf.layers.batch_normalization(h[-1], training=is_train, momentum=.9)\n",
    "        h2 = tf.concat((h[-1], x), -1)\n",
    "    hout = tf.concat((h), -1)\n",
    "    output = tf.layers.dense(hout, output_units, name=name_fcn())\n",
    "    if output_units == 1:\n",
    "        output = tf.squeeze(output, -1)\n",
    "    return h, output\n",
    "class PolicyLearner(object):\n",
    "    def __init__(self, ob_space, ac_space, take_weights_here=None, \n",
    "                 lshapes = [256] * 16, config = None, lshapes_small = [256] * 16):\n",
    "        self.a_idx = 0\n",
    "        self.c_idx = 0\n",
    "        self.v_idx = 0\n",
    "        self.sess = tf.InteractiveSession(config=config)\n",
    "        self.obs = tf.placeholder(tf.float32, (None, None, INPUT_UNITS))\n",
    "        self.metaobs = tf.placeholder(tf.float32, (None, None, 1))\n",
    "        self.returns = tf.placeholder(tf.float32, (None, None))\n",
    "        self.returnsdecayed = tf.placeholder(tf.float32, (None, None))\n",
    "        self.mask = tf.placeholder(tf.float32, (None, None))\n",
    "        self.lr = tf.placeholder_with_default(1e-3, (None))\n",
    "\n",
    "        self.is_train = tf.placeholder_with_default(True, (None))\n",
    "\n",
    "        \n",
    "        self.actions_input = tf.concat((self.obs, self.metaobs), axis=-1)\n",
    "        \n",
    "        self.h, pi = MLP(self.actions_input, lshapes, 17, self.a_name, self.is_train)\n",
    "        self.pi = tf.nn.tanh(pi/10) * 3\n",
    "\n",
    "        self.hs, self.state_value_estimate = MLP(\n",
    "            self.actions_input, lshapes_small, 1, self.v_name, self.is_train)\n",
    "        \n",
    "        self.critic_input = tf.concat((self.actions_input, self.pi), -1)\n",
    "        \n",
    "        self.advantage = ((\n",
    "            self.state_value_estimate[:,1:] * GAMMA + self.returns) -\n",
    "            self.state_value_estimate[:,:-1])\n",
    "        \n",
    "        self.hae, self.advantage_estimator = MLP(\n",
    "            self.critic_input, lshapes_small, 1, self.c_name, self.is_train)\n",
    "        \n",
    "        self.t_vars = tf.trainable_variables()\n",
    "        self.c_vars = [var for var in self.t_vars if 'c_' in var.name]\n",
    "        self.a_vars = [var for var in self.t_vars if 'a_' in var.name]\n",
    "        self.v_vars = [var for var in self.t_vars if 'v_' in var.name]\n",
    "        \n",
    "        self.creg, self.areg, self.vreg = [\n",
    "            tf.reduce_mean([tf.reduce_mean(tf.square(v)) for v in optvars]) * 1e-4\n",
    "            for optvars in [self.c_vars, self.a_vars, self.v_vars]]\n",
    "        \n",
    "        self.v_loss = tf.reduce_mean(tf.square(\n",
    "            self.returnsdecayed - self.state_value_estimate[:,1:]) * self.mask) + self.vreg\n",
    "        self.c_loss = tf.reduce_mean(tf.square(\n",
    "            self.advantage_estimator[:,:-1] - self.advantage) * self.mask) + self.creg\n",
    "        self.a_loss = -tf.reduce_mean(\n",
    "            self.advantage_estimator[:,:-1] * self.mask) + tf.reduce_mean(\n",
    "            tf.square(pi[:,:-1,:]) * tf.expand_dims(self.mask, -1))/1000 + self.areg\n",
    "        \n",
    "        self.v_obs_grad = tf.gradients(self.v_loss, self.obs)\n",
    "        self.c_obs_grad = tf.gradients(self.c_loss, self.obs)\n",
    "        self.c_pi_grad = tf.gradients(self.c_loss, self.pi)\n",
    "        self.a_obs_grad = tf.gradients(self.a_loss, self.obs)\n",
    "        \n",
    "        self.total_loss = self.c_loss + self.a_loss/100\n",
    "        self.critic_opt = tf.train.AdamOptimizer(self.lr)\n",
    "        self.value_opt = tf.train.AdamOptimizer(self.lr)\n",
    "        self.actor_opt = tf.train.AdamOptimizer(self.lr)\n",
    "        self.copt, self.c_norm = apply_clipped_optimizer(\n",
    "            self.critic_opt, self.c_loss, var_list = self.c_vars)\n",
    "        self.vopt, self.v_norm = apply_clipped_optimizer(\n",
    "            self.value_opt, self.v_loss, var_list = self.v_vars)\n",
    "        self.aopt, self.a_norm = apply_clipped_optimizer(\n",
    "            self.actor_opt, self.a_loss, var_list = self.a_vars)\n",
    "\n",
    "    def a_name(self):\n",
    "        self.a_idx += 1\n",
    "        return 'a_' + str(self.a_idx)\n",
    "    \n",
    "    def c_name(self):\n",
    "        self.c_idx += 1\n",
    "        return 'c_' + str(self.c_idx)\n",
    "    def v_name(self):\n",
    "        self.v_idx += 1\n",
    "        return 'v_' + str(self.v_idx)\n",
    "    \n",
    "    def load_weights(self):\n",
    "        feed_dict = {}\n",
    "        for (var, w), ph in zip(self.assigns, self.weight_assignment_placeholders):\n",
    "            feed_dict[ph] = w\n",
    "        self.sess.run(self.weight_assignment_nodes, feed_dict=feed_dict)\n",
    "\n",
    "    def act(self, obs, metaobs, cx):\n",
    "        # Because we need batch dimension, data[None] changes shape from [A] to [1,A]\n",
    "        a = self.sess.run(\n",
    "            self.pi, feed_dict={\n",
    "                self.obs:np.reshape(obs, (1, 1, INPUT_UNITS)),\n",
    "                self.metaobs:np.reshape(metaobs, (1, 1, 1)),\n",
    "                self.is_train:False\n",
    "            })\n",
    "        return a[0][0]  # return first in batch\n",
    "\n",
    "    \n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    inter_op_parallelism_threads=0,\n",
    "    intra_op_parallelism_threads=0,\n",
    "    device_count = { \"GPU\": 0 } )\n",
    "tf.reset_default_graph()\n",
    "\n",
    "env = gym.make(\"RoboschoolHumanoidFlagrun-v1\")\n",
    "pi = PolicyLearner(env.observation_space, env.action_space, config = config)\n",
    "\n",
    "sess = pi.sess\n",
    "self = pi\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#trainer = ZooPolicyTensorflow(\"mymodel1\", env.observation_space, env.action_space)\n",
    "saver = tf.train.Saver()\n",
    "ah, sh = [np.zeros((0, 0, i)) for i in [17, INPUT_UNITS]]\n",
    "mh, rh, rdecayedh = [np.zeros((0, 0)) for i in [None, None, None]]\n",
    "globalframes = []\n",
    "localframes = []\n",
    "ep = 0\n",
    "trained = 1\n",
    "obj_fname = 'saveobjs.pkl'\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tffile = \"tmp/unguided_trained.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adv(v, mag = 1, adv_type = 0):\n",
    "    if adv_type == 0:\n",
    "        return mag * v / np.abs(v).sum() * 10\n",
    "    elif adv_type == 1:\n",
    "        return mag * np.sign(v).astype(float) * .002\n",
    "    elif adv_type == 2:\n",
    "        sign_v = np.sign(v)\n",
    "        v = np.sqrt(np.abs(v))\n",
    "        v = v / v.sum() * 100\n",
    "        return mag * v * sign_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/unguided_trained.ckpt\n",
      "restored from save file\n"
     ]
    }
   ],
   "source": [
    "if trained:\n",
    "    try:\n",
    "        saver.restore(sess, tffile)\n",
    "        with open(obj_fname, \"rb\") as f:\n",
    "            ah, sh, mh, rh, rdecayedh, maskh, ep = pickle.load(f)\n",
    "        print('restored from save file')\n",
    "    except:\n",
    "        print('no save file detected')\n",
    "MAX_SEQ_LEN = 5000\n",
    "for ep in range(ep, 10000000):\n",
    "    if ep % 100 == 0 and trained:\n",
    "        save_path = saver.save(sess, tffile)\n",
    "        print('saved at epoch', ep)\n",
    "        with open(obj_fname,\"wb\") as f:\n",
    "            pickle.dump(\n",
    "                [ah[-1000:], sh[-1000:], mh[-1000:],\n",
    "                 rh[-1000:], rdecayedh[-1000:], maskh[-1000:], ep\n",
    "                ], f)\n",
    "    trained = 1\n",
    "    an, sn = [np.zeros((0, i)) for i in [17, INPUT_UNITS]]\n",
    "    mn, rn, maskn = [], [], []\n",
    "    frame = 0\n",
    "    score = 0\n",
    "    restart_delay = 0\n",
    "    obs = env.reset()\n",
    "    obs = np.concatenate((obs, np.zeros((17))))\n",
    "    obs = np.concatenate((obs, np.zeros_like(obs)))\n",
    "    obs_mat = np.concatenate((\n",
    "        obs[None,:],np.zeros((NUM_HISTORY-1, N_STATE))), 0)\n",
    "    metaobs = .00\n",
    "    mn.append(metaobs)\n",
    "    sn = np.concatenate((sn, obs_mat.reshape(1, -1)), 0)\n",
    "    while 1:\n",
    "        prob_random = 1 - 1/np.sqrt(ep+1)\n",
    "        if ep % 5 != 20: #always act on your own\n",
    "            a = pi.act(obs_mat.flatten(), metaobs, env)\n",
    "            if np.random.rand() > prob_random:\n",
    "                a = np.random.randn(*a.shape)*.1\n",
    "            elif np.random.rand() > prob_random:\n",
    "                a = a + np.random.randn(*a.shape) * .2\n",
    "                #a = a + np.random.randn(*a.shape)/np.sqrt(np.sqrt(ep + 2))*4\n",
    "            elif np.random.rand() > prob_random:\n",
    "                impact = np.random.binomial(size=a.shape, n=1, p= 0.3) * 2\n",
    "                if np.random.rand() > .5:\n",
    "                    a = a + impact\n",
    "                else:\n",
    "                    a = a - impact\n",
    "            elif np.random.rand() > prob_random:\n",
    "                impact = np.random.binomial(size=a.shape, n=1, p= 0.5) * 2 - 1\n",
    "                a = a + impact\n",
    "            elif np.random.rand() > prob_random:\n",
    "                a = a * .7\n",
    "            elif np.random.rand() > prob_random:\n",
    "                a = a * 1.5\n",
    "#             t = trainer.act(obs, env)\n",
    "        else:\n",
    "            pass\n",
    "#             a = trainer.act(obs, env)\n",
    "            t = a\n",
    "        an = np.concatenate((an, a[None,:]), 0)\n",
    "        last_obs = obs\n",
    "        obs, r, done, _ = env.step(a)\n",
    "        obs = np.concatenate((obs, a))\n",
    "        obs = np.concatenate((obs, obs - last_obs[:last_obs.shape[0]//2]))\n",
    "        r = r + 2\n",
    "        obs_mat = np.concatenate((obs[None,:], obs_mat[:-1,:]/1.3), 0)\n",
    "        metaobs = metaobs + .01\n",
    "        mn.append(metaobs)\n",
    "        rn.append(r)\n",
    "        sn = np.concatenate((sn, obs_mat.reshape(1, -1)), 0)\n",
    "        score += r\n",
    "        frame += 1\n",
    "        still_open = env.render(\"human\")\n",
    "        if done:\n",
    "            if ep % MAX_SEQ_LEN == 0:\n",
    "                print('score', score, ' frames', frame)\n",
    "            break\n",
    "        if still_open==False:\n",
    "            crashhere\n",
    "        if not done: continue\n",
    "        if restart_delay==0:\n",
    "            print(\"score=%0.2f in %i frames\" % (score, frame))\n",
    "            if still_open!=True:      # not True in multiplayer or non-Roboschool \n",
    "                break\n",
    "            restart_delay = 2000*2  # 2 sec at 60 fps\n",
    "        restart_delay -= 1\n",
    "        if restart_delay==0: \n",
    "            break\n",
    "    localframes.append(frame)\n",
    "    rn = np.array(rn)\n",
    "    second_half_run = len(rn)//2\n",
    "    subtract_fail = np.power(np.arange(len(rn)), 2)\n",
    "    subtract_fail = subtract_fail / subtract_fail.sum()\n",
    "    rn = .1 + rn - subtract_fail * 10\n",
    "    rn[-1] = rn[-1] - 10\n",
    "    rn[-2] = rn[-2] - 5\n",
    "    rewards = [0]\n",
    "    for ir in rn[::-1]:\n",
    "        rewards.append(rewards[-1] * GAMMA + ir)\n",
    "    rdecayedn = np.array(rewards)[:0:-1]\n",
    "    rdecayedn[-1] = rdecayedn[-1] - 100\n",
    "    rdecayedn[-2] = rdecayedn[-2] - 70\n",
    "    rdecayedn[-3] = rdecayedn[-3] - 40\n",
    "    rdecayedn[-4] = rdecayedn[-4] - 20\n",
    "    rdecayedn[-5] = rdecayedn[-5] - 10\n",
    "    mn = np.array(mn)\n",
    "    maskn = np.ones_like(rn)\n",
    "    if ep == 0:\n",
    "        ah, sh, mh, rh, rdecayedh, maskh = [\n",
    "            np.expand_dims(v, 0) for v in [an, sn, mn, rn,rdecayedn, maskn]]\n",
    "    else:\n",
    "        def get_updated_h(h, n, third_dim):\n",
    "            hshape = h.shape[1]\n",
    "            nshape = n.shape[0]\n",
    "            if third_dim:\n",
    "                if hshape > nshape:\n",
    "                    n = np.concatenate((n, np.zeros((hshape - nshape, n.shape[-1]))), 0)\n",
    "                if nshape > hshape:\n",
    "                    h = np.concatenate((h, np.zeros((\n",
    "                        h.shape[0], nshape - hshape, h.shape[-1]))), 1)\n",
    "            else:\n",
    "                if hshape > nshape:\n",
    "                    n = np.concatenate((n, np.zeros((hshape - nshape))), 0)\n",
    "                if nshape > hshape:\n",
    "                    h = np.concatenate((h, np.zeros((h.shape[0], nshape - hshape))), 1)\n",
    "            h = np.concatenate((h, np.expand_dims(n, 0)), 0)\n",
    "            return h\n",
    "            \n",
    "        ah, sh = [get_updated_h(h, n, 1) for  h, n in zip([ah, sh], [an, sn])]\n",
    "        \n",
    "        mh, rh, rdecayedh, maskh = [\n",
    "            get_updated_h(h, n, 0) for h, n in zip(\n",
    "                [mh, rh, rdecayedh, maskh], [mn, rn, rdecayedn, maskn])]\n",
    "        \n",
    "    if ep % 1 == 0 and ep > 5:\n",
    "        ah, sh, mh, rh,rdecayedh, maskh = [\n",
    "            v[-400:] for v in [ah, sh, mh, rh,rdecayedh, maskh]]\n",
    "        globalframes.append(np.mean(localframes))\n",
    "        localframes = []\n",
    "        batch_size = 32\n",
    "        if ep < batch_size:\n",
    "            batch_size = ep\n",
    "        num_hist = ah.shape[0]\n",
    "        total_aloss = 0\n",
    "        total_closs = 0\n",
    "        for itr in range(1):\n",
    "            if num_hist >  batch_size:\n",
    "                forced_hist = 4\n",
    "                probability = np.arange(num_hist - forced_hist)\n",
    "                probability = probability / probability.sum()\n",
    "                samples = np.concatenate((\n",
    "                    np.random.choice(\n",
    "                        num_hist - forced_hist, batch_size - forced_hist, \n",
    "                        replace=False, p=probability),\n",
    "                    np.arange(\n",
    "                        num_hist - forced_hist, num_hist)))\n",
    "            else:\n",
    "                samples = np.random.choice(num_hist, num_hist, replace=False)\n",
    "            actions, states, meta, returns, returnsdecayed, mask = [\n",
    "                v[samples] for v in [ah, sh, mh, rh,rdecayedh, maskh]]\n",
    "            feed_dict={\n",
    "                        self.metaobs:meta[:,:,None],\n",
    "                        self.returns:returns,\n",
    "                        self.returnsdecayed:returnsdecayed,\n",
    "                        self.lr: .01 / np.sqrt(ep + 1),\n",
    "                        self.mask:mask}\n",
    "            if ADV_ENABLED:\n",
    "                a_type = np.random.choice(3)\n",
    "                feed_dict[self.obs] = states\n",
    "                a_obs_grad, aloss_pre = sess.run(\n",
    "                    [self.a_obs_grad, self.a_loss], feed_dict)\n",
    "                a_obs_grad = normalize_adv(a_obs_grad[0], mag = 3, adv_type = a_type)\n",
    "                feed_dict[self.obs] = states + a_obs_grad\n",
    "            else:\n",
    "                feed_dict[self.obs] = states + np.random.rand(*states.shape) * .1\n",
    "            _, aloss = sess.run(\n",
    "                [self.aopt, self.a_loss],\n",
    "                feed_dict = feed_dict\n",
    "                    )\n",
    "            if ADV_ENABLED:\n",
    "                adv_sample = np.random.rand() > .3\n",
    "            else:\n",
    "                adv_sample = 0\n",
    "            if adv_sample:\n",
    "                feed_dict[self.pi] = actions\n",
    "                feed_dict[self.obs] = states[:,:-1,:]\n",
    "            else:\n",
    "                feed_dict[self.pi] = actions + np.random.rand(*actions.shape) * .1\n",
    "                feed_dict[self.obs] = states[:,:-1,:] + np.random.rand(\n",
    "                    *states[:,:-1,:].shape) * .1\n",
    "            feed_dict[self.metaobs] = meta[:,:-1,None]\n",
    "            feed_dict[self.returns] = returns[:,:-1]\n",
    "            feed_dict[self.returnsdecayed] = returnsdecayed[:,:-1]\n",
    "            feed_dict[self.mask] = mask[:,:-1]\n",
    "            if adv_sample:\n",
    "                feed_dict[self.pi] = actions\n",
    "                feed_dict[self.obs] = states[:,:-1,:]\n",
    "                v_obs_grad, vloss_pre = sess.run(\n",
    "                    [self.v_obs_grad, self.v_loss], feed_dict)\n",
    "                v_type = np.random.choice(3)\n",
    "                v_obs_grad = normalize_adv(v_obs_grad[0], mag = .1, adv_type = v_type)\n",
    "                feed_dict[self.obs] = states[:,:-1,:] + v_obs_grad\n",
    "                _, vloss = sess.run(\n",
    "                    [self.vopt, self.v_loss],\n",
    "                        feed_dict=feed_dict)\n",
    "                \n",
    "                feed_dict[self.pi] = actions\n",
    "                feed_dict[self.obs] = states[:,:-1,:]\n",
    "                c_obs_grad, c_pi_grad, closs_pre = sess.run(\n",
    "                    [self.c_obs_grad, self.c_pi_grad, self.c_loss], feed_dict)\n",
    "                c_type = np.random.choice(3)\n",
    "                c_obs_grad, c_pi_grad = [\n",
    "                    normalize_adv(v[0], mag = .1, adv_type = c_type) \n",
    "                    for v in [c_obs_grad, c_pi_grad]]\n",
    "                feed_dict[self.pi] = actions + c_pi_grad\n",
    "                feed_dict[self.obs] = states[:,:-1,:] + c_obs_grad\n",
    "                \n",
    "                _,closs = sess.run(\n",
    "                    [self.copt, self.c_loss],\n",
    "                        feed_dict=feed_dict)\n",
    "                \n",
    "            else:\n",
    "                _,_, closs, vloss = sess.run(\n",
    "                    [self.copt,self.vopt, self.c_loss, self.v_loss],\n",
    "                        feed_dict=feed_dict)\n",
    "            if 0:\n",
    "                if ep % 10 == 0:('aloss', aloss, 'closs', closs, 'vloss', vloss)\n",
    "        if ep % 1 == 0:\n",
    "            if adv_sample:\n",
    "                print('aloss', aloss - aloss_pre, 'atype', a_type,\n",
    "                      'closs', closs - closs_pre,  'ctype', c_type,\n",
    "                      'vloss', vloss - vloss_pre,  'vtype', v_type,\n",
    "                      ' ep, ',\n",
    "                      ep, ' avg frames',np.mean(globalframes[-20:]))\n",
    "            print('abs action',np.abs(ah)[-1,0,:].shape, np.abs(ah)[-1,0,:].mean())\n",
    "            print('max reward',np.max(rh[-10:]))\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.run(self.state_value_estimate,feed_dict)[1][np.where(mask[1])].shape\n",
    "\n",
    "# sess.run(self.state_value_estimate,feed_dict)[1][np.where(mask[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at epoch 1955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_path = saver.save(sess, tffile)\n",
    "print('saved at epoch', ep)\n",
    "with open(obj_fname,\"wb\") as f:\n",
    "    pickle.dump(\n",
    "        [ah[-1000:], sh[-1000:], mh[-1000:],\n",
    "         rh[-1000:], rdecayedh[-1000:], maskh[-1000:], ep\n",
    "        ], f)\n",
    "trained = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah[-2][:5].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
