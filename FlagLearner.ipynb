{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os.path, gym\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import roboschool\n",
    "import pdb\n",
    "\n",
    "def apply_clipped_optimizer(opt_fcn,\n",
    "                            loss,\n",
    "                            clip_norm=.1,\n",
    "                            clip_single=.03,\n",
    "                            clip_global_norm=False,\n",
    "                            var_list=None):\n",
    "    if var_list is None:\n",
    "        gvs = opt_fcn.compute_gradients(loss)\n",
    "    else:\n",
    "        gvs = opt_fcn.compute_gradients(loss, var_list = var_list)\n",
    "        \n",
    "\n",
    "    if clip_global_norm:\n",
    "        gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "        capped_gs, grad_norm_total = tf.clip_by_global_norm([g for g in gs],clip_norm)\n",
    "        capped_gvs = list(zip(capped_gs, vs))\n",
    "    else:\n",
    "        grad_norm_total = tf.sqrt(\n",
    "                tf.reduce_sum([\n",
    "                        tf.reduce_sum(tf.square(grad)) for grad, var in gvs\n",
    "                        if grad is not None\n",
    "                ]))\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single, clip_single), var)\n",
    "                                    for grad, var in gvs if grad is not None]\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var)\n",
    "                                    for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "    optimizer = opt_fcn.apply_gradients(capped_gvs)\n",
    "\n",
    "    return optimizer, grad_norm_total\n",
    "\n",
    "def MLP(x, lshapes, output_units, name_fcn):\n",
    "    h = [x]\n",
    "    h.append(tf.nn.leaky_relu(tf.layers.dense(h[-1], lshapes[0], name=name_fcn())))\n",
    "    for size in lshapes:\n",
    "        h.append(tf.nn.leaky_relu(h[-1] + tf.layers.dense(h[-1], size, name=name_fcn())))\n",
    "    output = tf.layers.dense(h[-1], output_units, name=name_fcn())\n",
    "    if output_units == 1:\n",
    "        output = tf.squeeze(output, -1)\n",
    "    return h, output\n",
    "NUM_HISTORY = 6\n",
    "INPUT_UNITS = 44 * NUM_HISTORY\n",
    "class PolicyLearner(object):\n",
    "    def __init__(self, ob_space, ac_space, take_weights_here=None, \n",
    "                 lshapes = [128]*4, config = None):\n",
    "        self.a_idx = 0\n",
    "        self.c_idx = 0\n",
    "        self.sess = tf.InteractiveSession(config=config)\n",
    "        self.obs = tf.placeholder(tf.float32, (None, None, INPUT_UNITS))\n",
    "        self.metaobs = tf.placeholder(tf.float32, (None, None, 1))\n",
    "        self.returns = tf.placeholder(tf.float32, (None, None))\n",
    "        self.mask = tf.placeholder(tf.float32, (None, None))\n",
    "        self.lr = tf.placeholder_with_default(1e-5, (None))\n",
    "\n",
    "        self.actions_input = tf.concat((self.obs, self.metaobs), axis=-1)\n",
    "        \n",
    "        self.h, pi = MLP(self.actions_input, lshapes, 17, self.a_name)\n",
    "        self.pi = tf.nn.tanh(pi/20) * 5\n",
    "\n",
    "        self.hs, self.state_value_estimate = MLP(self.actions_input, lshapes, 1, self.c_name)\n",
    "        \n",
    "        self.critic_input = tf.concat((self.actions_input, self.pi), -1)\n",
    "        \n",
    "        self.advantage = ((\n",
    "            self.state_value_estimate[:,1:] + self.returns) -\n",
    "            self.state_value_estimate[:,:-1])\n",
    "        \n",
    "        self.hae, self.advantage_estimator = MLP(self.critic_input, lshapes, 1, self.c_name)\n",
    "        \n",
    "        self.t_vars = tf.trainable_variables()\n",
    "        self.c_vars = [var for var in self.t_vars if 'c_' in var.name]\n",
    "        self.a_vars = [var for var in self.t_vars if 'a_' in var.name]\n",
    "        \n",
    "        self.critic_loss = tf.reduce_mean(tf.square(\n",
    "            self.advantage_estimator[:,:-1] - self.advantage) * self.mask)\n",
    "        self.actor_loss = -tf.reduce_mean(\n",
    "            self.advantage_estimator[:,:-1] * self.mask) + tf.reduce_mean(\n",
    "            tf.square(pi[:,:-1,:]) * tf.expand_dims(self.mask, -1))/100\n",
    "        self.total_loss = self.critic_loss + self.actor_loss/10\n",
    "        self.critic_opt = tf.train.AdamOptimizer(self.lr)\n",
    "        self.actor_opt = tf.train.AdamOptimizer(self.lr)\n",
    "        self.copt, self.c_norm = apply_clipped_optimizer(\n",
    "            self.critic_opt, self.critic_loss, var_list = self.c_vars)\n",
    "        self.aopt, self.a_norm = apply_clipped_optimizer(\n",
    "            self.actor_opt, self.actor_loss, var_list = self.a_vars)\n",
    "\n",
    "    def a_name(self):\n",
    "        self.a_idx += 1\n",
    "        return 'a_' + str(self.a_idx)\n",
    "    \n",
    "    def c_name(self):\n",
    "        self.c_idx += 1\n",
    "        return 'c_' + str(self.c_idx)\n",
    "    \n",
    "    def load_weights(self):\n",
    "        feed_dict = {}\n",
    "        for (var, w), ph in zip(self.assigns, self.weight_assignment_placeholders):\n",
    "            feed_dict[ph] = w\n",
    "        self.sess.run(self.weight_assignment_nodes, feed_dict=feed_dict)\n",
    "\n",
    "    def act(self, obs, metaobs, cx):\n",
    "        # Because we need batch dimension, data[None] changes shape from [A] to [1,A]\n",
    "        a = self.sess.run(\n",
    "            self.pi, feed_dict={self.obs:np.reshape(obs, (-1, 1, INPUT_UNITS)),\n",
    "                                self.metaobs:np.reshape(metaobs, (-1, 1, 1))\n",
    "            })\n",
    "        return a[0][0]  # return first in batch\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    inter_op_parallelism_threads=0,\n",
    "    intra_op_parallelism_threads=0,\n",
    "    device_count = { \"GPU\": 0 } )\n",
    "tf.reset_default_graph()\n",
    "\n",
    "env = gym.make(\"RoboschoolHumanoidFlagrun-v1\")\n",
    "pi = PolicyLearner(env.observation_space, env.action_space, config = config)\n",
    "sess = pi.sess\n",
    "self = pi\n",
    "sess.run(tf.global_variables_initializer())\n",
    "ah, sh = [np.zeros((0, 0, i)) for i in [17, INPUT_UNITS]]\n",
    "mh, rh = [np.zeros((0, 0)) for i in [1, 1]]\n",
    "globalframes = []\n",
    "localframes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ep in range(1000):\n",
    "    an, sn = [np.zeros((0, i)) for i in [17, INPUT_UNITS]]\n",
    "    mn, rn, maskn = [], [], []\n",
    "    frame = 0\n",
    "    score = 0\n",
    "    restart_delay = 0\n",
    "    obs = env.reset()\n",
    "    obs_mat = np.concatenate((obs[None,:],np.zeros((NUM_HISTORY-1, 44))), 0)\n",
    "    metaobs = .000\n",
    "    mn.append(metaobs)\n",
    "    sn = np.concatenate((sn, obs_mat.reshape(1, -1)), 0)\n",
    "    while 1:\n",
    "        a = pi.act(obs_mat.flatten(), metaobs, env)\n",
    "        a = a + np.random.randn(*a.shape)/2\n",
    "        an = np.concatenate((an, a[None,:]), 0)\n",
    "        \n",
    "        obs, r, done, _ = env.step(a)\n",
    "        r = r + 2\n",
    "        obs_mat = np.concatenate((obs[None,:], obs_mat[1:,:]), 0)\n",
    "        metaobs = metaobs + .001\n",
    "        mn.append(metaobs)\n",
    "        rn.append(r)\n",
    "        sn = np.concatenate((sn, obs_mat.reshape(1, -1)), 0)\n",
    "        score += r\n",
    "        frame += 1\n",
    "        still_open = env.render(\"human\")\n",
    "        if done:\n",
    "            if ep % 1000 == 0:\n",
    "                print('score', score, ' frames', frame)\n",
    "            break\n",
    "        if still_open==False:\n",
    "            crashhere\n",
    "        if not done: continue\n",
    "        if restart_delay==0:\n",
    "            print(\"score=%0.2f in %i frames\" % (score, frame))\n",
    "            if still_open!=True:      # not True in multiplayer or non-Roboschool environment\n",
    "                break\n",
    "            restart_delay = 200*2  # 2 sec at 60 fps\n",
    "        restart_delay -= 1\n",
    "        if restart_delay==0: \n",
    "            break\n",
    "    localframes.append(frame)\n",
    "    rn = np.array(rn)\n",
    "    second_half_run = len(rn)//2\n",
    "    subtract_fail = np.power(np.arange(len(rn)), 1.2)\n",
    "#     subtract_fail = np.concatenate((\n",
    "#         np.zeros_like(rn[:-second_half_run]), \n",
    "#         2 * np.power(np.arange(second_half_run), 1.2)))\n",
    "    subtract_fail = subtract_fail / subtract_fail.sum()\n",
    "#     print('rn', rn)\n",
    "#     print('subtract_fail', subtract_fail * 50)\n",
    "    rn = 3 + rn - subtract_fail * 100\n",
    "#     print('rn', rn)\n",
    "#     for i in range(15):\n",
    "#         rn[-i] = rn[-i] - (3 * (11 - i))\n",
    "    mn = np.array(mn)\n",
    "    maskn = np.ones_like(rn)\n",
    "    if ep == 0:\n",
    "        ah, sh, mh, rh, maskh = [np.expand_dims(v, 0) for v in [an, sn, mn, rn, maskn]]\n",
    "    else:\n",
    "        def get_updated_h(h, n, third_dim):\n",
    "            hshape = h.shape[1]\n",
    "            nshape = n.shape[0]\n",
    "            if third_dim:\n",
    "                if hshape > nshape:\n",
    "                    n = np.concatenate((n, np.zeros((hshape - nshape, n.shape[-1]))), 0)\n",
    "                if nshape > hshape:\n",
    "                    h = np.concatenate((h, np.zeros((\n",
    "                        h.shape[0], nshape - hshape, h.shape[-1]))), 1)\n",
    "            else:\n",
    "                if hshape > nshape:\n",
    "                    n = np.concatenate((n, np.zeros((hshape - nshape))), 0)\n",
    "                if nshape > hshape:\n",
    "                    h = np.concatenate((h, np.zeros((h.shape[0], nshape - hshape))), 1)\n",
    "            #pdb.set_trace()\n",
    "            h = np.concatenate((h, np.expand_dims(n, 0)), 0)\n",
    "            return h\n",
    "            \n",
    "        ah, sh = [get_updated_h(h, n, 1) for  h, n in zip([ah, sh], [an, sn])]\n",
    "        \n",
    "        mh, rh, maskh = [\n",
    "            get_updated_h(h, n, 0) for h, n in zip([mh, rh, maskh], [mn, rn, maskn])]\n",
    "        \n",
    "    if ep % 2 == 0 and ep > 10:\n",
    "        ah, sh, mh, rh, maskh = [v[-100000:] for v in [ah, sh, mh, rh, maskh]]\n",
    "        globalframes.append(np.mean(localframes))\n",
    "        localframes = []\n",
    "        print(globalframes[-20:])\n",
    "        batch_size = 64\n",
    "        if ep < batch_size:\n",
    "            batch_size = ep\n",
    "        num_hist = ah.shape[0]\n",
    "        total_aloss = 0\n",
    "        total_closs = 0\n",
    "        for itr in range(5):\n",
    "            if num_hist >  batch_size:\n",
    "                forced_hist = 10\n",
    "                samples = np.concatenate((\n",
    "                    np.random.choice(\n",
    "                        num_hist - forced_hist, batch_size - forced_hist, replace=False),\n",
    "                    np.arange(\n",
    "                        num_hist - forced_hist, num_hist)))\n",
    "            else:\n",
    "                np.random.choice(num_hist, batch_size, replace=False)\n",
    "            actions, states, meta, returns, mask = [\n",
    "                v[samples] for v in [ah, sh, mh, rh, maskh]]\n",
    "            feed_dict={\n",
    "                        self.obs:states,\n",
    "                        self.metaobs:meta[:,:,None],\n",
    "                        self.returns:returns,\n",
    "                        self.mask:mask}\n",
    "            _, aloss = sess.run(\n",
    "                [self.aopt, self.actor_loss],\n",
    "                feed_dict = feed_dict\n",
    "                    )\n",
    "            feed_dict[self.pi] = actions\n",
    "            feed_dict[self.obs] = states[:,:-1,:]\n",
    "            feed_dict[self.metaobs] = meta[:,:-1,None]\n",
    "            feed_dict[self.returns] = returns[:,:-1]\n",
    "            feed_dict[self.mask] = mask[:,:-1]\n",
    "            _, closs = sess.run(\n",
    "                [self.copt, self.critic_loss],\n",
    "                    feed_dict=feed_dict)\n",
    "        print('aloss', aloss, 'closs', closs)\n",
    "        print('abs action',np.abs(ah)[-1,0,:].shape, np.abs(ah)[-1,0,:].mean())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for itr in range(ep // 10):\n",
    "    #samples = np.random.choice(num_hist, batch_size, replace=False)\n",
    "    actions, states, meta, returns, mask = [v[samples] for v in [ah, sh, mh, rh, maskh]]\n",
    "    feed_dict[self.lr] = 1e-4\n",
    "    feed_dict={\n",
    "                self.obs:states,\n",
    "                self.metaobs:meta[:,:,None],\n",
    "                self.returns:returns,\n",
    "                self.mask:mask}\n",
    "    _, aloss = sess.run(\n",
    "        [self.aopt, self.actor_loss],\n",
    "        feed_dict = feed_dict\n",
    "            )\n",
    "#     feed_dict[self.lr] = 1e-5\n",
    "    feed_dict[self.pi] = actions\n",
    "    feed_dict[self.obs] = states[:,:-1,:]\n",
    "    feed_dict[self.metaobs] = meta[:,:-1,None]\n",
    "    feed_dict[self.returns] = returns[:,:-1]\n",
    "    feed_dict[self.mask] = mask[:,:-1]\n",
    "#     _, closs = sess.run(\n",
    "#         [self.copt, self.critic_loss],\n",
    "#             feed_dict=feed_dict)\n",
    "    print(aloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape, states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, aloss = sess.run(\n",
    "                [self.aopt, self.actor_loss],\n",
    "                feed_dict = feed_dict\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2',[v.shape for v in sess.run([self.advantage], feed_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(ah).mean(-1).mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itr in range(10):\n",
    "            samples = np.random.choice(num_hist, batch_size, replace=False)\n",
    "            actions, states, meta, returns = [v[samples] for v in [ah, sh, mh, rh]]\n",
    "            feed_dict={\n",
    "                        self.obs:states,\n",
    "                        self.metaobs:meta[:,:,None],\n",
    "                        self.returns:returns}\n",
    "            _, aloss = sess.run(\n",
    "                [self.aopt, self.actor_loss],\n",
    "                feed_dict = feed_dict\n",
    "                    )\n",
    "            feed_dict[self.pi] = actions\n",
    "            feed_dict[self.obs] = states[:,:-1,:]\n",
    "            feed_dict[self.metaobs] = meta[:,:-1,None]\n",
    "            _, closs = sess.run(\n",
    "                [self.copt, self.critic_loss],\n",
    "                    feed_dict=feed_dict)\n",
    "            print('aloss', aloss)\n",
    "            print('closs', closs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    v.shape for v in sess.run([self.state_value_estimate[:,:-1],\n",
    "                               self.state_value_estimate[:,1:], self.returns], feed_dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.concat((self.actions_input, self.pi), -1), feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    v.shape for v in sess.run([self.actions_input, self.pi], feed_dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    v.shape for v in sess.run([self.advantage_estimator, self.advantage], feed_dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed_dict[self.pi] = actions\n",
    "_, closs = sess.run(\n",
    "    [self.copt, self.critic_loss],\n",
    "        feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.critic_input, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.actions_input, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[v.shape for v in sess.run([self.advantage, self.advantage_estimator, self.mask], feed_dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.advantage_estimator, feed_dict).shape, sess.run(self.mask, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_mat.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_mat.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, r, d, m, s = [\n",
    "    [h[v] for h in history_sampled]\n",
    "    for v  in ['a','r', 'd', 'm', 's']]\n",
    "\n",
    "batch = {}\n",
    "vec = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vec[0]\n",
    "longest = max([len(v) for v in vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[np.array(v).shape, np.zeros((longest - len(v), len(v[0]))).shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(history_sampled[i]['s']) for i in range(len(history_sampled))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array(v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(\n",
    "        ([np.stack(np.array(v), 0).shape, np.zeros((longest - len(v), len(v[-1]))).shape]), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch[vname] = np.stack(\n",
    "    ([np.concatenate([np.stack(np.array(v), 0), np.zeros((longest - len(v), len(v[-1])))], 1) for v in vec]), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, r, d, m, s = [\n",
    "    [h[v] for h in history_sampled]\n",
    "    for v  in ['a','r', 'd', 'm', 's']]\n",
    "\n",
    "batch = {}\n",
    "\n",
    "for vname, vec in zip(['a', 's'], [a, s]):\n",
    "    longest = max([len(v) for v in vec])\n",
    "    batch[vname] = np.stack(\n",
    "        ([np.concatenate([np.stack(np.array(v), 0), np.zeros((longest - len(v), len(v[-1])))], 0) for v in vec]), 0)\n",
    "\n",
    "\n",
    "for vname, vec in zip(['r', 'd', 'm'], [r, d, m]):\n",
    "    longest = max([len(v) for v in vec])\n",
    "    batch[vname] = np.stack(\n",
    "        ([np.concatenate([np.array(v), np.zeros((longest - len(v)))], 0) for v in vec]), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, b.shape) for i, b in batch.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(history_sampled):\n",
    "    a, r, d, m, s = [\n",
    "        [h[v] for h in history_sampled]\n",
    "        for v  in ['a','r', 'd', 'm', 's']]\n",
    "\n",
    "    batch = {}\n",
    "\n",
    "    for vname, vec in zip(['a', 's'], [a, s]):\n",
    "        longest = max([len(v) for v in vec])\n",
    "        batch[vname] = np.stack(\n",
    "            ([np.concatenate([np.array(v), np.zeros((longest - len(v), len(v[0])))], 0) for v in vec]), 0)\n",
    "\n",
    "\n",
    "    for vname, vec in zip(['r', 'd', 'm'], [r, d, m]):\n",
    "        longest = max([len(v) for v in vec])\n",
    "        batch[vname] = np.stack(\n",
    "            ([np.concatenate([np.array(v), np.zeros((longest - len(v)))], 0) for v in vec]), 0)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_sampled = [history[s] for s in states_sampled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[len(h[blah]) for h in history_sampled] for blah in ['a','r', 's', 'd', 'm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[h[blah][0].shape for h in history_sampled] for blah in ['a', 's']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, r, d, m, s = [\n",
    "    [h[v] for h in history_sampled]\n",
    "    for v  in ['a','r', 'd', 'm', 's']]\n",
    "\n",
    "batch = {}\n",
    "\n",
    "for vname, vec in zip(['a', 's'], [a, s]):\n",
    "    longest = max([len(v) for v in vec])\n",
    "    batch[vname] = np.stack(\n",
    "        ([np.concatenate([np.array(v), np.zeros((longest - len(v), len(v[0])))], 0) for v in vec]), 0)\n",
    "    \n",
    "\n",
    "for vname, vec in zip(['r', 'd', 'm'], [r, d, m]):\n",
    "    longest = max([len(v) for v in vec])\n",
    "    batch[vname] = np.stack(\n",
    "        ([np.concatenate([np.array(v), np.zeros((longest - len(v)))], 0) for v in vec]), 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[b.shape for b in batch.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(x) for x in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hist = len(history)\n",
    "states_sampled = np.random.choice(num_hist, batch_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pi, history, n_steps = 100, batch_size = 5):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape, obs_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[h['r'] for h in history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
