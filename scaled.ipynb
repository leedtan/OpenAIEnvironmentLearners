{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os.path, gym\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import roboschool\n",
    "import pdb\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "GAMMA = .97\n",
    "from functools import partial\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "if 1:\n",
    "    envname = \"RoboschoolInvertedPendulum-v1\"\n",
    "    SIZE_MULT = 2\n",
    "    REWARD_MULT = 1\n",
    "    N_LAYERS = 10\n",
    "    N_DROP = 4\n",
    "    NUM_DROP_PARALLEL = 4\n",
    "    NUM_HISTORY = N_HISTORY = 2\n",
    "else:\n",
    "    envname = \"RoboschoolHumanoidFlagrun-v1\"\n",
    "    SIZE_MULT = 8\n",
    "    REWARD_MULT = 4\n",
    "    N_LAYERS = 20\n",
    "    N_DROP = 3\n",
    "    NUM_DROP_PARALLEL = 6\n",
    "    NUM_HISTORY = N_HISTORY = 6\n",
    "DROP_RATE = .3\n",
    "INIT_LEN = 120\n",
    "N_PRETRAIN = 1\n",
    "NUM_KEEP = 300\n",
    "PERCENT_CHOOSE_OPTIMAL = 2\n",
    "env = gym.make(envname)\n",
    "N_OBS, N_ACT = [v.shape[0] for v in [env.observation_space, env.action_space]]\n",
    "N_STATE = (N_OBS + N_ACT) * 2\n",
    "REWARD_IN_STATE = 0\n",
    "if REWARD_IN_STATE:\n",
    "    N_STATE += 2\n",
    "INPUT_UNITS = N_STATE * NUM_HISTORY\n",
    "STATE_DECAY = .7\n",
    "TESTING_GRAD_NORMS = 1\n",
    "ADV_ENABLED = 0\n",
    "STORED_MODELS = {}\n",
    "FCNS = {'np':{\n",
    "    'concat':np.concatenate,\n",
    "    'reshape':np.reshape,\n",
    "    'expand':np.expand_dims\n",
    "},'tf':{\n",
    "    'concat':tf.concat,\n",
    "    'reshape':tf.reshape,\n",
    "    'expand':tf.expand_dims\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb2c71a4e0>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-100, 100, .01)\n",
    "\n",
    "y = np.log(np.abs(x)+1) * np.sign(x)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_clipped_optimizer(opt_fcn,\n",
    "                            loss,\n",
    "                            clip_norm=.1,\n",
    "                            clip_single=.03,\n",
    "                            clip_global_norm=False,\n",
    "                            var_list=None):\n",
    "    if var_list is None:\n",
    "        gvs = opt_fcn.compute_gradients(loss)\n",
    "    else:\n",
    "        gvs = opt_fcn.compute_gradients(loss, var_list = var_list)\n",
    "        \n",
    "\n",
    "    if clip_global_norm:\n",
    "        gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "        capped_gs, grad_norm_total = tf.clip_by_global_norm([g for g in gs],clip_norm)\n",
    "        capped_gvs = list(zip(capped_gs, vs))\n",
    "    else:\n",
    "        grad_norm_total = tf.sqrt(\n",
    "                tf.reduce_sum([\n",
    "                        tf.reduce_sum(tf.square(grad)) for grad, var in gvs\n",
    "                        if grad is not None\n",
    "                ]))\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single, clip_single), var)\n",
    "                                    for grad, var in gvs if grad is not None]\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var)\n",
    "                                    for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "    optimizer = opt_fcn.apply_gradients(capped_gvs)\n",
    "\n",
    "    return optimizer, grad_norm_total\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self,\n",
    "                x, lshapes, output_units, namebase, tanh_out, sigmoid, squeeze = True,\n",
    "                lshapes_drop = None, drop_rate = DROP_RATE, reuse = False,\n",
    "                residual = None, \n",
    "                ):\n",
    "        self.namebase = namebase\n",
    "        self.reuse = reuse\n",
    "        self.lidx = 0\n",
    "        name_fcn = self.get_name\n",
    "        h = [tf.nn.leaky_relu(tf.layers.dense(\n",
    "            x, lshapes[0], name=self.get_name(), reuse = self.reuse))]\n",
    "        h2 = h[-1]\n",
    "        for size in lshapes:\n",
    "            h.append(tf.nn.leaky_relu(h[-1] + tf.layers.dense(\n",
    "                h2, size, name=name_fcn(), reuse = self.reuse)))\n",
    "            h2 = h[-1]#tf.concat((x, h[-1]), -1)\n",
    "        if lshapes_drop is None:\n",
    "            hout = h[-1]\n",
    "            output = tf.layers.dense(\n",
    "                hout, output_units, name=name_fcn(), reuse = self.reuse)\n",
    "            if output_units == 1 and squeeze:\n",
    "                output = tf.squeeze(output, -1)\n",
    "            self.raw_output = output\n",
    "            if residual is not None:\n",
    "                self.raw_output = self.raw_output + residual\n",
    "            if tanh_out:\n",
    "                self.output = leaky_tanh(self.raw_output)\n",
    "            elif sigmoid:\n",
    "                self.output = tf.nn.sigmoid(self.raw_output)\n",
    "            else:\n",
    "                self.output = self.raw_output\n",
    "        else:\n",
    "            drop_outs = []\n",
    "            x_drop = h[-1]\n",
    "            name_fcn = self.get_name_drop\n",
    "            self.lidx_drop = 0\n",
    "            for drop_idx, drop_try in enumerate(range(NUM_DROP_PARALLEL)):\n",
    "                h_drop = [x_drop]\n",
    "                for lsize in lshapes_drop:\n",
    "                    new_layer = tf.nn.dropout(tf.nn.leaky_relu(tf.layers.dense(\n",
    "                        h_drop[-1], size, name=name_fcn(), reuse = self.reuse)), keep_prob = 1-drop_rate)\n",
    "                    h_drop.append(new_layer)\n",
    "                    new_layer = tf.nn.leaky_relu(tf.layers.dense(\n",
    "                        h_drop[-1], size, name=name_fcn(), reuse = self.reuse))\n",
    "                    h_drop.append(new_layer)\n",
    "                output = tf.layers.dense(h_drop[-1], output_units)\n",
    "                if output_units == 1 and squeeze:\n",
    "                    output = tf.squeeze(output, -1)\n",
    "                output = output\n",
    "                if residual is not None:\n",
    "                    output = output + residual\n",
    "                if sigmoid is not None:\n",
    "                    output = tf.nn.sigmoid(output) * 0.9 - 0.05\n",
    "                drop_outs.append(output)\n",
    "                self.reuse = True\n",
    "                self.lidx_drop = 0\n",
    "            self.output = tf.stack(drop_outs, -1)\n",
    "            self.output_mean, self.variance = tf.nn.moments(self.output, -1)\n",
    "    def get_name(self):\n",
    "        self.lidx = self.lidx + 1\n",
    "        return self.namebase + str(self.lidx)\n",
    "    def get_name_drop(self):\n",
    "        self.lidx_drop = self.lidx_drop + 1\n",
    "        return self.namebase + '_drop_' + str(self.lidx_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_state(state, action, style = 'np'):\n",
    "    together = FCNS[style]['concat']((state, action), -1)\n",
    "    if style == 'tf':\n",
    "        return together\n",
    "#         expanded = FCNS[style]['expand'](\n",
    "#             together, 1)\n",
    "    else:\n",
    "        expanded = FCNS[style]['expand'](\n",
    "            together, 0)\n",
    "    return expanded\n",
    "    return FCNS[style]['reshape'](\n",
    "        together, (state.shape[0], -1, state.shape[-1] + action.shape[-1]))\n",
    "        \n",
    "    \n",
    "\n",
    "def accumulate_state(state, action, old_state, statedecay, style = 'np'):\n",
    "    new_state = make_state(state, action, style)\n",
    "    new_state_len = new_state.shape[-1]\n",
    "    if style == 'tf':\n",
    "        vel = new_state - old_state[:,:,0,:new_state_len]\n",
    "    else:\n",
    "        vel = new_state - old_state[0,:new_state_len]\n",
    "    new_state = FCNS[style]['concat']((new_state, vel), -1)\n",
    "    if style == 'tf':\n",
    "        return FCNS[style]['concat']((tf.expand_dims(new_state, 2), old_state[:,:,:-1,:]*statedecay), 2)\n",
    "    return FCNS[style]['concat']((new_state, old_state[:-1,:]*statedecay), 0)\n",
    "\n",
    "def leaky_tanh(x):\n",
    "    return tf.log(tf.abs(x)+1) * tf.sign(x)*.7\n",
    "    #return tf.nn.tanh(x*30)/10 + tf.nn.tanh(x*2)/2 + tf.nn.tanh(x/20) * 2 + x * 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_grad_norm(optimizer, loss, optname):\n",
    "    gvs = optimizer.compute_gradients(loss)\n",
    "    grad_norm = tf.reduce_mean(\n",
    "        [tf.reduce_mean(tf.square(grad)) for\n",
    "         grad, var in gvs if grad is not None and optname in var.name])\n",
    "    return grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PolicyLearner(object):\n",
    "    def __init__(self, ob_space, ac_space, take_weights_here=None, \n",
    "                 lshapes = [32 * SIZE_MULT] * N_LAYERS, config = None, \n",
    "                 lshapes_small = [16 * SIZE_MULT] * (N_LAYERS//2),\n",
    "                 lshapes_smaller = [16 * SIZE_MULT] * (N_LAYERS//2),\n",
    "                 lshapes_drop = [16 * SIZE_MULT] * N_DROP,\n",
    "                reuse = False):\n",
    "        self.sess = tf.InteractiveSession(config=config)\n",
    "        self.obs_raw = tf.placeholder(tf.float32, (None, None, NUM_HISTORY, N_STATE))\n",
    "        self.returns = tf.placeholder(tf.float32, (None, None))\n",
    "        self.returnsdecayed = tf.placeholder(tf.float32, (None, None))\n",
    "        self.mask = tf.placeholder(tf.float32, (None, None))\n",
    "        self.lr = tf.placeholder_with_default(1e-3, (None))\n",
    "        self.statesraw = tf.placeholder(tf.float32, (None, None, N_OBS))\n",
    "        self.is_train = tf.placeholder_with_default(True, (None))\n",
    "        self.is_exploit = tf.placeholder(tf.float32, (None))\n",
    "        self.is_exploit2d = tf.expand_dims(self.is_exploit, -1)\n",
    "        self.is_exploit3d = tf.expand_dims(self.is_exploit2d, -1)\n",
    "        self.statesraw_expanded = tf.expand_dims(self.statesraw, -1)\n",
    "        self.obs_raw_expanded = tf.expand_dims(self.obs_raw, -1)\n",
    "        self.maskexpanded = tf.expand_dims(self.mask, -1)\n",
    "        self.maskexpanded2 = tf.expand_dims(self.maskexpanded, -1)\n",
    "        self.obs = tf.reshape(\n",
    "            self.obs_raw, (\n",
    "                tf.shape(self.obs_raw)[0], tf.shape(self.obs_raw)[1], N_STATE * NUM_HISTORY))\n",
    "        self.actor = MLP(\n",
    "            self.obs, lshapes, N_ACT, 'a_', squeeze = False, reuse = reuse, sigmoid = False,\n",
    "        tanh_out = True)\n",
    "        self.explorer = MLP(\n",
    "            self.obs, lshapes, N_ACT, 'e_', squeeze = False, reuse = reuse, sigmoid = False,\n",
    "        tanh_out = True)\n",
    "        self.actions = self.actor.output\n",
    "        self.explorer_actions = self.explorer.output\n",
    "        self.state_value_estimator = MLP(\n",
    "            self.obs, lshapes_smaller, 1, 'v_', reuse = reuse, tanh_out = False, sigmoid = True)\n",
    "        self.state_value_estimate = self.state_value_estimator.output\n",
    "        \n",
    "        self.advantage = ((\n",
    "            self.state_value_estimate[:,1:] * GAMMA + self.returns[:,:-1]) -\n",
    "            self.state_value_estimate[:,:-1])\n",
    "        \n",
    "        self.critic_input = tf.concat((self.obs, self.actions), -1)\n",
    "        self.explorer_critic_input = tf.concat((self.obs, self.explorer_actions), -1)\n",
    "        \n",
    "        self.advantage_estimator = MLP(\n",
    "            self.critic_input, lshapes_small, 1, 'c_', lshapes_drop = lshapes_drop, reuse = reuse,\n",
    "        sigmoid = False, tanh_out = False)\n",
    "        self.model_estimator = MLP(\n",
    "            self.critic_input, lshapes_small, N_OBS, 'm_', lshapes_drop = lshapes_drop, reuse = reuse,\n",
    "            residual = self.statesraw, sigmoid = True, tanh_out = False\n",
    "        )\n",
    "        self.explorer_advantage_estimator = MLP(\n",
    "            self.explorer_critic_input, lshapes_small, 1, 'c_', lshapes_drop = lshapes_drop, reuse = True,\n",
    "        sigmoid = False, tanh_out = False)\n",
    "        self.explorer_model_estimator = MLP(\n",
    "            self.explorer_critic_input, lshapes_small, N_OBS, 'm_', lshapes_drop = lshapes_drop, reuse = True,\n",
    "            residual = self.statesraw, sigmoid = True, tanh_out = False\n",
    "        )\n",
    "        self.advantage_estimate = self.advantage_estimator.output_mean\n",
    "        self.model_estimate = self.model_estimator.output_mean\n",
    "        self.future_obs_raw = accumulate_state(\n",
    "            self.model_estimate, self.actions, self.obs_raw, STATE_DECAY, style = 'tf')\n",
    "        self.explorer_future_obs_raw = accumulate_state(\n",
    "            self.explorer_model_estimator.output_mean, self.explorer_actions,\n",
    "            self.obs_raw, STATE_DECAY, style = 'tf')\n",
    "        \n",
    "        self.future_obs = tf.reshape(self.future_obs_raw, \n",
    "            (tf.shape(self.future_obs_raw)[0],tf.shape(self.future_obs_raw)[1], N_STATE * NUM_HISTORY))\n",
    "        self.explorer_future_obs = tf.reshape(self.explorer_future_obs_raw, \n",
    "            (tf.shape(self.explorer_future_obs_raw)[0],\n",
    "             tf.shape(self.explorer_future_obs_raw)[1], N_STATE * NUM_HISTORY))\n",
    "        \n",
    "        self.future_value = MLP(\n",
    "            self.future_obs, lshapes_smaller, 1, 'v_', reuse = True, tanh_out = False, sigmoid = True)\n",
    "        \n",
    "        self.future_actor = MLP(\n",
    "            self.future_obs, lshapes, N_ACT, 'a_', reuse = True, squeeze = False,\n",
    "        sigmoid = False, tanh_out = True)\n",
    "        self.future_actions = self.future_actor.output\n",
    "        \n",
    "        self.future_critic_input = tf.concat((self.future_obs, self.future_actions), -1)\n",
    "        self.future_advantage_estimator = MLP(\n",
    "            self.future_critic_input, lshapes_small, 1, 'c_', lshapes_drop = lshapes_drop, reuse = True,\n",
    "        sigmoid = False, tanh_out = False)\n",
    "        \n",
    "        if len(self.future_actions.shape) == 2:\n",
    "            self.future_actions = tf.expand_dims(self.future_actions, -1)\n",
    "        \n",
    "        self.t_vars = tf.trainable_variables()\n",
    "        self.a_vars = [var for var in self.t_vars if 'a_' in var.name]\n",
    "        self.v_vars = [var for var in self.t_vars if 'v_' in var.name]\n",
    "        self.c_vars = [var for var in self.t_vars if 'c_' in var.name]\n",
    "        self.e_vars = [var for var in self.t_vars if 'e_' in var.name]\n",
    "        self.m_vars = [var for var in self.t_vars if 'm_' in var.name]\n",
    "        \n",
    "        self.creg, self.areg, self.vreg, self.mreg, self.ereg = [\n",
    "            tf.reduce_mean([tf.reduce_mean(tf.square(v)) for v in optvars]) * 1e3\n",
    "            for optvars in \n",
    "            [self.c_vars, self.a_vars, self.v_vars, self.m_vars, self.e_vars]]\n",
    "        \n",
    "        def mplusv(x):\n",
    "            return x.variance + x.output_mean\n",
    "        \n",
    "        self.explorer_adv_loss = -tf.reduce_mean(mplusv(\n",
    "            self.explorer_advantage_estimator) * self.mask) * 1e3\n",
    "        self.explorer_mdl_loss = -tf.reduce_mean(\n",
    "            self.explorer_model_estimator.variance * self.maskexpanded) * 1e1\n",
    "        \n",
    "        \n",
    "        self.model_state_estimate = self.model_estimator.output_mean\n",
    "        \n",
    "        self.v_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.returnsdecayed - self.state_value_estimate) * self.mask * self.is_exploit2d) * 300\n",
    "        \n",
    "        self.m_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.model_estimator.output[:,:-1] - self.statesraw_expanded[:,1:]\n",
    "        ) * self.maskexpanded2[:,:-1]) * 1000\n",
    "        \n",
    "        self.c_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.advantage_estimate[:,:-1] - self.advantage) * self.mask[:,:-1]) * 300\n",
    "        \n",
    "        self.actionmean = tf.reduce_sum(\n",
    "            self.actions * self.maskexpanded, 1) / tf.reduce_sum(\n",
    "            self.maskexpanded, 1)\n",
    "        self.actdiffsquared = tf.square(\n",
    "            self.actions - tf.expand_dims(self.actionmean, 1))\n",
    "        self.actvar = tf.reduce_sum(\n",
    "            self.actdiffsquared * self.maskexpanded, 1)/ tf.reduce_sum(\n",
    "            self.maskexpanded, 1)\n",
    "        self.actstd = self.actvar\n",
    "        \n",
    "        self.logactabs = -tf.log(tf.reduce_mean(\n",
    "            tf.abs(self.actor.raw_output) * self.maskexpanded / \n",
    "            tf.reduce_mean(self.maskexpanded)) + 1e-5) * .01\n",
    "        \n",
    "        self.actstdpenalty = -tf.reduce_mean(\n",
    "            tf.log(self.actstd + .00001)* .01 + tf.square(self.actstd)) * .0001 + self.logactabs\n",
    "        \n",
    "        self.aregmeanbyaction = tf.reduce_mean(\n",
    "            tf.square(self.actionmean)) * 1e4\n",
    "        self.a_mse = tf.reduce_mean((\n",
    "            tf.square(self.actor.raw_output) * 1e-1 + \n",
    "            tf.square(self.future_actor.raw_output) * 1e-4 +\n",
    "            tf.square(tf.square(self.actor.raw_output)) * 1e-1 + \n",
    "            tf.square(tf.square(self.future_actor.raw_output)) * 1e-4\n",
    "            ) * self.maskexpanded * 1e-2)/tf.reduce_mean(self.maskexpanded)\n",
    "        \n",
    "        self.e_mse = tf.reduce_mean((\n",
    "            tf.square(self.explorer.raw_output) * 1e-1 + \n",
    "            tf.square(tf.square(self.explorer.raw_output)) * 1e-1) *\n",
    "            self.maskexpanded)/tf.reduce_mean(self.maskexpanded)\n",
    "        \n",
    "        self.logexplorerabs = -tf.log(tf.reduce_mean(\n",
    "            tf.abs(self.explorer.raw_output) * self.maskexpanded / \n",
    "            tf.reduce_mean(self.maskexpanded)) + 1e-5) * .01\n",
    "        \n",
    "        self.explorermean = tf.reduce_sum(\n",
    "            self.explorer_actions * self.maskexpanded, 1) / tf.reduce_sum(\n",
    "            self.maskexpanded, 1)\n",
    "        self.eregmeanbyaction = tf.reduce_mean(\n",
    "            tf.square(self.explorermean)) * 1e4 + self.logexplorerabs\n",
    "            \n",
    "        self.frac_not_masked = tf.reduce_mean(self.mask)\n",
    "        \n",
    "        self.a_loss_critic = -tf.reduce_mean(\n",
    "            self.advantage_estimator.output_mean * self.mask)/self.frac_not_masked * 20000\n",
    "        self.a_loss_model = -tf.reduce_mean(\n",
    "            self.future_value.output * self.mask)/self.frac_not_masked * 1000.\n",
    "        self.a_loss_secondaction = -tf.reduce_mean(\n",
    "            self.future_advantage_estimator.output_mean * self.mask)/self.frac_not_masked * 10.\n",
    "        \n",
    "        self.grad_v = tf.square(tf.gradients(\n",
    "            self.state_value_estimator.output * self.mask, self.obs)[0])\n",
    "        self.grad_m = tf.square(tf.gradients(\n",
    "            self.model_estimator.output * self.maskexpanded2, self.critic_input)[0])\n",
    "        self.grad_c = tf.square(tf.gradients(\n",
    "            self.advantage_estimator.output * self.maskexpanded, self.critic_input)[0])\n",
    "        \n",
    "        self.grad_norm_m = tf.reduce_mean(self.grad_m) * 1e1\n",
    "        self.grad_norm_v = tf.reduce_mean(self.grad_v) * 1e-2\n",
    "        self.grad_norm_c = tf.reduce_mean(self.grad_c) * 1e-2\n",
    "    \n",
    "        slopes = tf.reduce_sum(tf.square(self.grad_c), reduction_indices=[2])\n",
    "        self.grad_c_1 = tf.reduce_mean(self.mask * (slopes - .2) ** 2) * 1e-7\n",
    "        slopes = tf.reduce_sum(tf.square(self.grad_m), reduction_indices=[2])\n",
    "        self.grad_m_1 = tf.reduce_mean(self.mask * (slopes - .2) ** 2) * 1e-3\n",
    "        \n",
    "        \n",
    "        self.critic_opt = tf.train.AdamOptimizer(self.lr, beta1= .8)\n",
    "        self.value_opt = tf.train.AdamOptimizer(self.lr, beta1= .8)\n",
    "        self.actor_opt = tf.train.AdamOptimizer(self.lr, beta1= .8)\n",
    "        self.model_opt = tf.train.AdamOptimizer(self.lr, beta1= .8)\n",
    "        self.explorer_opt = tf.train.AdamOptimizer(self.lr, beta1= .8)\n",
    "        \n",
    "        \n",
    "        self.actor_current_mult = []\n",
    "        self.critic_current_mult = []\n",
    "        self.actor_loss_mults = []\n",
    "        self.critic_loss_mults = []\n",
    "        self.critic_loss_grads = []\n",
    "        self.actor_loss_grads = []\n",
    "        self.actor_loss_targets = np.array([10., 10., .1,\n",
    "                                           10., 10.])\n",
    "        self.actor_loss_names = ['a_loss_critic', 'a_loss_model', 'a_loss_secondaction', \n",
    "                    'explorer_adv_loss', 'explorer_mdl_loss'\n",
    "                                ]\n",
    "        self.critic_loss_targets = np.array(\n",
    "            [10, 1., 10,\n",
    "             .001, .001, .001, \n",
    "            .001, .001])\n",
    "        self.critic_loss_names = [\n",
    "            'v_loss_raw','m_loss_raw', 'c_loss_raw',\n",
    "            'grad_norm_c',  'grad_norm_v', 'grad_norm_m', \n",
    "            'grad_m_1', 'grad_c_1']\n",
    "        self.aopts = [self.actor_opt] * 3 + [self.explorer_opt] * 2\n",
    "        self.aoptnames = ['a_'] * 3 + ['e_'] * 2\n",
    "        self.copts = [self.value_opt, self.model_opt, self.critic_opt, \n",
    "                      self.critic_opt, self.value_opt,\n",
    "                      self.model_opt, self.model_opt, self.critic_opt]\n",
    "        self.coptnames = [v + '_' for v in ['v', 'm', 'c', 'c', 'v', 'm', 'm', 'c']]\n",
    "        for lidx, loss in enumerate(self.actor_loss_names):\n",
    "            opt = self.aopts[lidx]\n",
    "            optname = self.aoptnames[lidx]\n",
    "            loss_mult = tf.placeholder_with_default(1., (None))\n",
    "            self.actor_loss_mults.append(loss_mult)\n",
    "            setattr(self, loss, getattr(self, loss) * loss_mult)\n",
    "            self.actor_loss_grads.append(get_grad_norm(opt, getattr(self, loss), optname))\n",
    "            self.actor_current_mult.append(1)\n",
    "        for lidx, loss in enumerate(self.critic_loss_names):\n",
    "            opt = self.copts[lidx]\n",
    "            optname = self.coptnames[lidx]\n",
    "            loss_mult = tf.placeholder_with_default(1., (None))\n",
    "            self.critic_loss_mults.append(loss_mult)\n",
    "            setattr(self, loss, getattr(self, loss) * loss_mult)\n",
    "            self.critic_loss_grads.append(get_grad_norm(opt, getattr(self, loss), optname))\n",
    "            self.critic_current_mult.append(1)\n",
    "        self.actor_current_mult = np.array(self.actor_current_mult)\n",
    "        self.critic_current_mult = np.array(self.critic_current_mult)\n",
    "        \n",
    "        self.v_loss =  self.v_loss_raw + self.vreg\n",
    "        self.m_loss = self.m_loss_raw + self.mreg\n",
    "        self.c_loss =  self.c_loss_raw + self.creg\n",
    "        self.aregtotal = self.areg + self.aregmeanbyaction + self.actstdpenalty + self.a_mse\n",
    "        self.a_loss_raw = self.a_loss_critic + self.a_loss_model + \\\n",
    "            self.a_loss_secondaction\n",
    "        self.a_loss = self.a_loss_raw + self.aregtotal\n",
    "        \n",
    "        \n",
    "        self.e_loss_minimize = self.explorer_adv_loss + self.explorer_mdl_loss + \\\n",
    "            self.e_mse + self.ereg + self.eregmeanbyaction\n",
    "        self.a_loss_minimize = self.a_loss\n",
    "        self.c_loss_minimize = self.c_loss + self.grad_norm_c + self.grad_c_1\n",
    "        self.v_loss_minimize = self.v_loss + self.grad_norm_v\n",
    "        self.m_loss_minimize = self.m_loss + self.grad_norm_m + self.grad_m_1\n",
    "        \n",
    "        \n",
    "#         if TESTING_GRAD_NORMS:\n",
    "#             self.a_grads = [\n",
    "#                 get_grad_norm(self.actor_opt, l) for l in [\n",
    "#                 self.a_loss_minimize, self.a_loss_raw, self.a_loss_critic,self.a_loss_model,\n",
    "#                 self.a_loss_secondaction,\n",
    "#                 self.areg, self.aregmeanbyaction, self.actstdpenalty, self.a_mse\n",
    "#             ]]\n",
    "#             self.v_grads = [get_grad_norm(self.value_opt, l) for l in [\n",
    "#                 self.v_loss_minimize, self.v_loss_raw, \n",
    "#                 self.vreg, self.grad_norm_v\n",
    "#             ]]\n",
    "#             self.c_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "#                 self.c_loss_minimize, self.c_loss_raw, \n",
    "#                 self.creg, self.grad_norm_c, self.grad_c_1\n",
    "\n",
    "#             ]]\n",
    "#             self.m_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "#                 self.m_loss_minimize, self.m_loss_raw, \n",
    "#                 self.mreg, self.grad_norm_m, self.grad_m_1\n",
    "#             ]]\n",
    "        \n",
    "        self.copt, self.c_norm = apply_clipped_optimizer(\n",
    "            self.critic_opt, self.c_loss_minimize, var_list = self.c_vars)\n",
    "        self.vopt, self.v_norm = apply_clipped_optimizer(\n",
    "            self.value_opt, self.v_loss_minimize, var_list = self.v_vars)\n",
    "        self.aopt, self.a_norm = apply_clipped_optimizer(\n",
    "            self.actor_opt, self.a_loss_minimize, var_list = self.a_vars)\n",
    "        self.mopt, self.m_norm = apply_clipped_optimizer(\n",
    "            self.model_opt, self.m_loss_minimize, var_list = self.m_vars)\n",
    "        self.eopt, self.e_norm = apply_clipped_optimizer(\n",
    "            self.explorer_opt, self.e_loss_minimize, var_list = self.e_vars)\n",
    "\n",
    "    def a_name(self):\n",
    "        self.a_idx += 1\n",
    "        return 'a_' + str(self.a_idx)\n",
    "    def c_name(self):\n",
    "        self.c_idx += 1\n",
    "        return 'c_' + str(self.c_idx)\n",
    "    def v_name(self):\n",
    "        self.v_idx += 1\n",
    "        return 'v_' + str(self.v_idx)\n",
    "    def m_name(self):\n",
    "        self.m_idx += 1\n",
    "        return 'm_' + str(self.m_idx)\n",
    "    def e_name(self):\n",
    "        self.e_idx += 1\n",
    "        return 'e_' + str(self.e_idx)\n",
    "    \n",
    "    def load_weights(self):\n",
    "        feed_dict = {}\n",
    "        for (var, w), ph in zip(\n",
    "            self.assigns, self.weight_assignment_placeholders):\n",
    "            feed_dict[ph] = w\n",
    "        self.sess.run(self.weight_assignment_nodes, feed_dict=feed_dict)\n",
    "\n",
    "    def act(self, obs, exploit = True):\n",
    "        # Because we need batch dimension, \n",
    "        # data[None] changes shape from [A] to [1,A]\n",
    "        if exploit:\n",
    "            act = self.actions\n",
    "        else:\n",
    "            act = self.explorer_actions\n",
    "        a = self.sess.run(\n",
    "            act, feed_dict={\n",
    "                self.obs_raw:np.reshape(obs/shdiff - shmin_g_e, (1, 1, N_HISTORY, N_STATE)),\n",
    "                self.is_train:False\n",
    "            })\n",
    "        return a[0][0]  # return first in batch\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    inter_op_parallelism_threads=0,\n",
    "    intra_op_parallelism_threads=0,\n",
    "    device_count = { \"GPU\": 0 } )\n",
    "tf.reset_default_graph()\n",
    "\n",
    "pi = PolicyLearner(env.observation_space, env.action_space, config = config)\n",
    "\n",
    "sess = pi.sess\n",
    "self = pi\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#trainer = ZooPolicyTensorflow(\n",
    "# \"mymodel1\", env.observation_space, env.action_space)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()\n",
    "\n",
    "\n",
    "# env = gym.make(envname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.actor_current_mult = np.ones_like(self.actor_current_mult,dtype=np.float32)\n",
    "self.critic_current_mult = np.ones_like(self.critic_current_mult,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#         self.advantage = ((\n",
    "#             self.stateraw_value_estimate[:,1:] * GAMMA + self.returns) -\n",
    "#             self.state_value_estimate[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ah, sh, shraw, rh, rdecayedh, maskh = [\n",
    "    np.zeros((0, 0, i)) for i in [N_ACT, INPUT_UNITS, N_OBS, 1, 1, 1]]\n",
    "isexploit = []\n",
    "globalframes = []\n",
    "localframes = []\n",
    "ep = 0\n",
    "trained = 0\n",
    "ongoing = 0\n",
    "printfreq = 20\n",
    "obj_fname = envname + str(PERCENT_CHOOSE_OPTIMAL) + 'saveobjs_unguided.pkl'\n",
    "tffile = \"tmp/\" + envname + str(PERCENT_CHOOSE_OPTIMAL) + \"unguided_trained.ckpt\"\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ongoing = 0\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ongoing:\n",
    "    \n",
    "    save_path = saver.save(sess, tffile)\n",
    "    print('saved at epoch', ep)\n",
    "    with open(obj_fname,\"wb\") as f:\n",
    "        pickle.dump(\n",
    "            [ah, sh, rh, rdecayedh, maskh, ep, globalframes, isexploit\n",
    "            ], f)\n",
    "    trained = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_actor(self, feed_dict, printon):\n",
    "    _, _, aloss = sess.run(\n",
    "        [self.aopt, self.eopt, self.a_loss],\n",
    "        feed_dict = feed_dict\n",
    "            )\n",
    "    if TESTING_GRAD_NORMS and ep % printfreq == 0 and printon and ep > INIT_LEN:\n",
    "        cur_grad_norms = sess.run(self.actor_loss_grads, feed_dict)\n",
    "        target = self.actor_loss_targets\n",
    "        problem_factor = cur_grad_norms / target\n",
    "        for fidx, factor in enumerate(problem_factor):\n",
    "            if fidx == len(problem_factor) - 1:\n",
    "                pass#pdb.set_trace()\n",
    "            if factor > 1000:\n",
    "                self.actor_current_mult[fidx] = self.actor_current_mult[fidx] / 30\n",
    "            elif factor > 100:\n",
    "                self.actor_current_mult[fidx] = self.actor_current_mult[fidx] / 10\n",
    "            elif factor > 10:\n",
    "                self.actor_current_mult[fidx] = self.actor_current_mult[fidx] / 3\n",
    "            elif factor < .001:\n",
    "                self.actor_current_mult[fidx] = self.actor_current_mult[fidx] * 30\n",
    "            elif factor < .01:\n",
    "                self.actor_current_mult[fidx] = self.actor_current_mult[fidx] * 10\n",
    "            elif factor < .1:\n",
    "                self.actor_current_mult[fidx] = self.actor_current_mult[fidx] * 3\n",
    "            self.actor_current_mult[fidx] = np.clip(self.actor_current_mult[fidx], 1e-5, 1e5)\n",
    "        print(list(zip(self.actor_loss_names, cur_grad_norms, target, self.actor_current_mult)))\n",
    "    return aloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(actor = True, value = True, n_steps = 1, forced_hist = 2):\n",
    "    for itr in range(n_steps):\n",
    "        if num_hist >  batch_size:\n",
    "            #probability = np.arange(num_hist - forced_hist)\n",
    "            probability = num_steps_per_run = (\n",
    "                maskh.shape[1] - maskh[:,::-1].argmax(1))\n",
    "            if forced_hist:\n",
    "                probability = probability[:-forced_hist]\n",
    "            probability = np.sqrt(probability)\n",
    "            probability = probability / probability.sum()\n",
    "            rnd = np.random.choice(\n",
    "                    num_hist - forced_hist, batch_size - forced_hist, \n",
    "                    replace=False, p=probability)\n",
    "            if forced_hist:\n",
    "                samples = np.concatenate((rnd,\n",
    "                    np.arange( num_hist - forced_hist, num_hist)))\n",
    "            else:\n",
    "                samples=rnd\n",
    "        else:\n",
    "            samples = np.random.choice(num_hist, num_hist, replace=False)\n",
    "        #samples = np.array([-2, -1])\n",
    "        actions, states, statesraw, returns, returnsdecayed, mask, exploit = [\n",
    "            v[samples] for v in [ah, sh, shraw, rh,rdecayedh, maskh, np.array(isexploit)]]\n",
    "        \n",
    "        feed_dict={\n",
    "                    self.returns:returns / vdiff - vmin_g_e,\n",
    "                    self.statesraw: statesraw / shrawdiff - shrawmin_g_e,\n",
    "                    self.returnsdecayed:returnsdecayed / vdiff - vmin_g_e,\n",
    "                    self.lr: .02 / np.power(ep + 20, .4),\n",
    "                    self.mask:mask,\n",
    "                    self.obs_raw: (\n",
    "                        states/shdiff - shmin_g_e).reshape(*states.shape[:2], N_HISTORY, N_STATE),\n",
    "                    self.is_exploit:exploit,\n",
    "                    self.actions:actions\n",
    "        }\n",
    "        for lph, lmult in zip(self.actor_loss_mults, self.actor_current_mult):\n",
    "            feed_dict[lph] = lmult\n",
    "        for lph, lmult in zip(self.critic_loss_mults, self.critic_current_mult):\n",
    "            feed_dict[lph] = lmult\n",
    "        self.amse_mult = 1.\n",
    "        self.emse_mult = 1.\n",
    "        if value:\n",
    "            if exploit.sum() > 1:\n",
    "                _,_, _, mloss, closs, vloss = sess.run(\n",
    "                    [self.mopt, self.copt,self.vopt, self.m_loss, self.c_loss, self.v_loss],\n",
    "                        feed_dict=feed_dict)\n",
    "            else:\n",
    "                _, mloss = sess.run(\n",
    "                    [self.mopt, self.m_loss],\n",
    "                        feed_dict=feed_dict)\n",
    "                closs = vloss = 0\n",
    "        else:\n",
    "            mloss = closs =  vloss = 0\n",
    "        del feed_dict[self.actions]\n",
    "        if actor and exploit.sum() > 1:\n",
    "            aloss = train_actor(self, feed_dict, printon = itr == 0)\n",
    "        else:\n",
    "            aloss = 0\n",
    "    if 1:\n",
    "        if ep % printfreq == 0 and exploit.sum() > 1:\n",
    "            print('aloss', aloss, 'closs', closs, 'vloss', vloss, 'mloss', mloss)\n",
    "            if TESTING_GRAD_NORMS and ep > INIT_LEN:\n",
    "                \n",
    "                cur_grad_norms = sess.run(self.critic_loss_grads, feed_dict)\n",
    "                target = self.critic_loss_targets\n",
    "                problem_factor = cur_grad_norms / target\n",
    "                for fidx, factor in enumerate(problem_factor):\n",
    "                    if factor > 1000:\n",
    "                        self.critic_current_mult[fidx] = self.critic_current_mult[fidx] / 30\n",
    "                    elif factor > 100:\n",
    "                        self.critic_current_mult[fidx] = self.critic_current_mult[fidx] / 10\n",
    "                    elif factor > 10:\n",
    "                        self.critic_current_mult[fidx] = self.critic_current_mult[fidx] / 2\n",
    "                    elif factor < .001:\n",
    "                        self.critic_current_mult[fidx] = self.critic_current_mult[fidx] * 30\n",
    "                    elif factor < .01:\n",
    "                        self.critic_current_mult[fidx] = self.critic_current_mult[fidx] * 10\n",
    "                    elif factor < .1:\n",
    "                        self.critic_current_mult[fidx] = self.critic_current_mult[fidx] * 2\n",
    "                print(list(zip(\n",
    "                    self.critic_loss_names, cur_grad_norms, target, self.critic_current_mult)))\n",
    "    if ep % printfreq == 0:\n",
    "        print(' ep, ', ep, ' avg frames',np.mean(globalframes[-20:]))\n",
    "        print('abs action',np.abs(ah)[-1,0,:].shape, np.abs(ah)[-1,0,:].mean())\n",
    "        print('max reward',np.max(rh[-10:,10:]))\n",
    "    if ep % 10000 == 0:\n",
    "        clear_output()\n",
    "    if ep % 100 == 0:\n",
    "        ysmoothed = gaussian_filter1d(globalframes, sigma=4)\n",
    "        plt.plot(ysmoothed)\n",
    "        plt.show()\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploit = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[[shrawmin_g, shrawmax_g],\n",
    "[shmin_g, shmax_g],\n",
    "[vmin_g, vmax_g]] = [[np.ones(i) * 1000, np.zeros(i) * -1000] for i in [N_OBS, N_STATE * 2, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 40.0  frames 20\n"
     ]
    }
   ],
   "source": [
    "if trained:\n",
    "    try:\n",
    "        saver.restore(sess, tffile)\n",
    "        with open(obj_fname, \"rb\") as f:\n",
    "            ah, sh, shraw, rh, rdecayedh, maskh, ep, globalframes, isexploit = pickle.load(f)\n",
    "        print('restored from save file')\n",
    "    except:\n",
    "        print('no save file detected')\n",
    "        trained = 0\n",
    "MAX_SEQ_LEN = 5000\n",
    "for ep in range(ep, 10000000):\n",
    "    if ep % 100 == 0 and trained and ep > 0:\n",
    "        save_path = saver.save(sess, tffile)\n",
    "        print('saved at epoch', ep)\n",
    "        with open(obj_fname,\"wb\") as f:\n",
    "            pickle.dump(\n",
    "                [ah[-NUM_KEEP:], sh[-NUM_KEEP:], shraw[-NUM_KEEP:],\n",
    "                 rh[-NUM_KEEP:], rdecayedh[-NUM_KEEP:], maskh[-NUM_KEEP:], ep, globalframes, isexploit[-NUM_KEEP:]\n",
    "                ], f)\n",
    "        def make_maxmin(x):\n",
    "            y = x.copy()\n",
    "            y[maskh==0] = np.nan\n",
    "            amin, amax = np.nanmin(y, (0, 1)), np.nanmax(y, (0,1))\n",
    "            return amin, amax\n",
    "        shrawmin, shrawmax = make_maxmin(shraw)\n",
    "        shmin, shmax = make_maxmin(sh)\n",
    "        vmin, vmax = make_maxmin(rdecayedh)\n",
    "        shrawmin_g, shmin_g, vmin_g = [\n",
    "            np.minimum(g, n) for g, n in zip(\n",
    "                [shrawmin_g, shmin_g, vmin_g], [shrawmin, shmin, vmin])]\n",
    "        shrawmax_g, shmax_g, vmax_g = [\n",
    "            np.maximum(g, n) for g, n in zip(\n",
    "                [shrawmax_g, shmax_g, vmax_g], [shrawmax, shmax, vmax])]\n",
    "        \n",
    "        [shrawmin_g_e, shrawmax_g_e,\n",
    "        shmin_g_e, shmax_g_e\n",
    "        ] = [np.reshape(v, (1, 1, -1)) for v in [shrawmin_g, shrawmax_g,\n",
    "                                shmin_g, shmax_g,\n",
    "                                ]]\n",
    "        vmin_g_e, vmax_g_e = [np.reshape(v, (1, 1)) for v in [vmin_g, vmax_g]]\n",
    "        \n",
    "        shrawdiff = shrawmax_g_e - shrawmin_g_e\n",
    "        shdiff = shmax_g_e - shmin_g_e\n",
    "        vdiff = vmax_g_e - vmin_g_e\n",
    "    trained = 1\n",
    "    ongoing = 1\n",
    "    \n",
    "    an, sn, snraw, rn, rdecayedn, maskn = [\n",
    "        np.zeros((0, i)) for i in [N_ACT, INPUT_UNITS, N_OBS, 1, 1, 1]]\n",
    "    frame = 0\n",
    "    score = 0\n",
    "    restart_delay = 5\n",
    "    obs = env.reset()\n",
    "    obsraw = obs\n",
    "    snraw = np.concatenate((snraw, obs.reshape(1, -1)), 0)\n",
    "    obs = np.concatenate((obs, np.zeros(N_ACT)))\n",
    "    obs = np.concatenate((obs, np.zeros_like(obs)))\n",
    "    obs_mat = np.concatenate((\n",
    "        obs[None,:],np.zeros((NUM_HISTORY-1, N_STATE))), 0)\n",
    "    rn = [0]\n",
    "    done_ctr = 0\n",
    "    sn = np.concatenate((sn, obs_mat.reshape(1, -1)), 0)\n",
    "    step_num = 0\n",
    "    step_shifted = -1\n",
    "    show_shift = 0\n",
    "    if ep <= INIT_LEN or exploit:\n",
    "        exploit = 0\n",
    "    else:\n",
    "        exploit = 1\n",
    "    if np.random.rand() > .5:\n",
    "        prob_random = PERCENT_CHOOSE_OPTIMAL#2#.95#1 - 1/np.sqrt(ep+1)\n",
    "    else:\n",
    "        prob_random = 2\n",
    "    while 1:\n",
    "        step_num += 1\n",
    "        if ep < INIT_LEN:\n",
    "            a = np.random.randn(N_ACT)\n",
    "        else:\n",
    "            a = pi.act(obs_mat.flatten(), exploit = exploit)\n",
    "            if np.random.rand() < 0.02:\n",
    "                a = np.random.randn(*a.shape)\n",
    "            if (not exploit) and np.random.rand() < .2:\n",
    "                a = np.random.randn(*a.shape) * 2\n",
    "        an = np.concatenate((an, a[None,:]), 0)\n",
    "        snraw = np.concatenate((snraw, obsraw[None,:]), 0)\n",
    "        last_obs = obs\n",
    "#         last_obsraw = obsraw\n",
    "        obs, r, done, _ = env.step(a)\n",
    "        obsraw = obs\n",
    "        r = r + 1 * REWARD_MULT\n",
    "\n",
    "        obs_mat = accumulate_state(\n",
    "            obs, a, obs_mat, STATE_DECAY, style = 'np')\n",
    "        \n",
    "        rn.append(r)\n",
    "        sn = np.concatenate((sn, obs_mat.reshape(1, -1)), 0)\n",
    "        score += r\n",
    "        frame += 1\n",
    "        if ep > INIT_LEN:\n",
    "            still_open = env.render(\"human\")\n",
    "        else:\n",
    "            still_open = 1\n",
    "        if done:\n",
    "            done_ctr += 1\n",
    "            if done_ctr > 3:\n",
    "                if ep % MAX_SEQ_LEN == 0:\n",
    "                    print('score', score, ' frames', frame)\n",
    "                break\n",
    "        if still_open==False:\n",
    "            crashhere\n",
    "        if not done: continue\n",
    "        if restart_delay==0:\n",
    "            print(\"score=%0.2f in %i frames\" % (score, frame))\n",
    "            if still_open!=True:      # not True in multiplayer or non-Roboschool \n",
    "                break\n",
    "            restart_delay = 2000*2  # 2 sec at 60 fps\n",
    "        restart_delay -= 1\n",
    "        if restart_delay==0: \n",
    "            break\n",
    "    if ep < INIT_LEN:\n",
    "        a = np.random.randn(N_ACT)\n",
    "    else:\n",
    "        a = pi.act(obs_mat.flatten())\n",
    "    an = np.concatenate((an, a[None,:]), 0)\n",
    "    localframes.append(frame)\n",
    "    rn = np.array(rn)\n",
    "    rn[-1] = rn[-1] - 20 * REWARD_MULT\n",
    "    rewards = [0]\n",
    "    for ir in rn[::-1]:\n",
    "        rewards.append(rewards[-1] * GAMMA + ir)\n",
    "    rdecayedn = np.array(rewards)[:0:-1]\n",
    "    lenrdec = len(rdecayedn)\n",
    "#     rdecayedn = rdecayedn + lenrdec\n",
    "#     rdecayedn = rdecayedn - np.arange(lenrdec) * 1.8\n",
    "    maskn = np.ones_like(rn)\n",
    "    if step_shifted > -1:\n",
    "        if step_shifted == 0:\n",
    "            raise ValueError('step shifted not allowed 0')\n",
    "        maskn[:step_shifted - 1] = 0\n",
    "    if ep == 0:\n",
    "        ah, sh, shraw, rh, rdecayedh, maskh = [\n",
    "            np.expand_dims(v, 0) for v in [an, sn, snraw, rn,rdecayedn, maskn]]\n",
    "        isexploit = [exploit]\n",
    "    else:\n",
    "        def get_updated_h(h, n, third_dim):\n",
    "            hshape = h.shape[1]\n",
    "            nshape = n.shape[0]\n",
    "            if third_dim:\n",
    "                if hshape > nshape:\n",
    "                    n = np.concatenate((n, np.zeros((hshape - nshape, n.shape[-1]))), 0)\n",
    "                if nshape > hshape:\n",
    "                    h = np.concatenate((h, np.zeros((\n",
    "                        h.shape[0], nshape - hshape, h.shape[-1]))), 1)\n",
    "            else:\n",
    "                if hshape > nshape:\n",
    "                    n = np.concatenate((n, np.zeros((hshape - nshape))), 0)\n",
    "                if nshape > hshape:\n",
    "                    h = np.concatenate((h, np.zeros((h.shape[0], nshape - hshape))), 1)\n",
    "            h = np.concatenate((h, np.expand_dims(n, 0)), 0)\n",
    "            return h\n",
    "            \n",
    "        ah, sh, shraw = [get_updated_h(h, n, 1) for  h, n in zip(\n",
    "            [ah, sh, shraw], [an, sn, snraw])]\n",
    "        \n",
    "        rh, rdecayedh, maskh = [\n",
    "            get_updated_h(h, n, 0) for h, n in zip(\n",
    "                [rh, rdecayedh, maskh], [rn, rdecayedn, maskn])]\n",
    "        isexploit.append(exploit)\n",
    "    if ep % 1 == 0 and ep > INIT_LEN:\n",
    "        ah, sh, shraw, rh,rdecayedh, maskh, isexploit = [\n",
    "            v[-NUM_KEEP:] for v in [ah, sh, shraw, rh,rdecayedh, maskh, isexploit]]\n",
    "        globalframes.append(np.mean(localframes))\n",
    "        localframes = []\n",
    "        batch_size = 32\n",
    "        if ep < batch_size:\n",
    "            batch_size = ep\n",
    "        num_hist = ah.shape[0]\n",
    "        if ep == INIT_LEN + 1 or sum(isexploit) < 2:\n",
    "            train(actor = False, n_steps = N_PRETRAIN, forced_hist=0)\n",
    "            #train(value = False, n_steps = 20, forced_hist=0)\n",
    "        elif ep < 500:\n",
    "            feed_dict = train(actor = True, value = True, n_steps = 2)\n",
    "        else:\n",
    "            feed_dict = train(actor = True, value = True, n_steps = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[v.shape, v.name] for v in feed_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.actor.output, feed_dict)[:,5:20].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(self.explorer.output, feed_dict)[:,5:20].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah[-20::2,:10].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah[-21::2,:10].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isexploit[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        self.v_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.returnsdecayed - self.state_value_estimate) * self.mask * self.is_exploit2d)\n",
    "        self.v_loss =  self.v_loss_raw + self.vreg\n",
    "        \n",
    "        self.m_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.model_estimator.output[:,:-1] - self.statesraw_expanded[:,1:]\n",
    "        ) * self.maskexpanded[:,:-1]) * 100\n",
    "        self.m_loss = self.m_loss_raw + self.mreg\n",
    "        \n",
    "        self.c_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.advantage_estimate[:,:-1] - self.advantage) * self.mask[:,:-1])\n",
    "        self.c_loss =  self.c_loss_raw + self.creg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            self.a_grads = [\n",
    "                get_grad_norm(self.actor_opt, l) for l in [\n",
    "                self.a_loss_minimize, self.a_loss_raw, self.a_loss_critic,self.a_loss_model,\n",
    "                self.a_loss_secondaction,\n",
    "                self.areg, self.aregmeanbyaction, self.actstdpenalty\n",
    "            ]]\n",
    "            self.v_grads = [get_grad_norm(self.value_opt, l) for l in [\n",
    "                self.v_loss_minimize, self.v_loss_raw, \n",
    "                self.vreg, self.grad_norm_v\n",
    "            ]]\n",
    "            self.c_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "                self.c_loss_minimize, self.c_loss_raw, \n",
    "                self.creg, self.grad_norm_c, self.grad_c_1\n",
    "\n",
    "            ]]\n",
    "            self.m_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "                self.m_loss_minimize, self.m_loss_raw, \n",
    "                self.mreg, self.grad_norm_m, self.grad_m_1\n",
    "            ]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
