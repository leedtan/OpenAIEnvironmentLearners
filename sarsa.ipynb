{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os.path, gym\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import roboschool\n",
    "import pdb\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "GAMMA = .97\n",
    "from functools import partial\n",
    "NUM_HISTORY = N_HISTORY = 6\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "if 1:\n",
    "    envname = \"RoboschoolInvertedPendulum-v1\"\n",
    "    SIZE_MULT = 1\n",
    "    REWARD_MULT = 1\n",
    "    RANDOM_STYLE_COMPLEX = 0\n",
    "    N_LAYERS = 10\n",
    "else:\n",
    "    envname = \"RoboschoolHumanoidFlagrun-v1\"\n",
    "    SIZE_MULT = 4\n",
    "    REWARD_MULT = 4\n",
    "    RANDOM_STYLE_COMPLEX = 1\n",
    "    N_LAYERS = 20\n",
    "PERCENT_CHOOSE_OPTIMAL = .8\n",
    "env = gym.make(envname)\n",
    "N_OBS, N_ACT = [v.shape[0] for v in [env.observation_space, env.action_space]]\n",
    "N_STATE = (N_OBS + N_ACT) * 2\n",
    "REWARD_IN_STATE = 0\n",
    "if REWARD_IN_STATE:\n",
    "    N_STATE += 2\n",
    "INPUT_UNITS = N_STATE * NUM_HISTORY\n",
    "STATE_DECAY = .8\n",
    "TESTING_GRAD_NORMS = 1\n",
    "ADV_ENABLED = 0\n",
    "STORED_MODELS = {}\n",
    "FCNS = {'np':{\n",
    "    'concat':np.concatenate,\n",
    "    'reshape':np.reshape,\n",
    "    'expand':np.expand_dims\n",
    "},'tf':{\n",
    "    'concat':tf.concat,\n",
    "    'reshape':tf.reshape,\n",
    "    'expand':tf.expand_dims\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_clipped_optimizer(opt_fcn,\n",
    "                            loss,\n",
    "                            clip_norm=.1,\n",
    "                            clip_single=.03,\n",
    "                            clip_global_norm=False,\n",
    "                            var_list=None):\n",
    "    if var_list is None:\n",
    "        gvs = opt_fcn.compute_gradients(loss)\n",
    "    else:\n",
    "        gvs = opt_fcn.compute_gradients(loss, var_list = var_list)\n",
    "        \n",
    "\n",
    "    if clip_global_norm:\n",
    "        gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "        capped_gs, grad_norm_total = tf.clip_by_global_norm([g for g in gs],clip_norm)\n",
    "        capped_gvs = list(zip(capped_gs, vs))\n",
    "    else:\n",
    "        grad_norm_total = tf.sqrt(\n",
    "                tf.reduce_sum([\n",
    "                        tf.reduce_sum(tf.square(grad)) for grad, var in gvs\n",
    "                        if grad is not None\n",
    "                ]))\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single, clip_single), var)\n",
    "                                    for grad, var in gvs if grad is not None]\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var)\n",
    "                                    for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "    optimizer = opt_fcn.apply_gradients(capped_gvs)\n",
    "\n",
    "    return optimizer, grad_norm_total\n",
    "\n",
    "def MLP(x, lshapes, output_units, name_fcn, is_train, \n",
    "        self = None, usemodel = False, storemodel = False,\n",
    "       scale = 1):\n",
    "    if storemodel:\n",
    "        #[[a_idx, c_idx, v_idx], name]\n",
    "        modeltype = storemodel[0]\n",
    "        STORED_MODELS[storemodel[1]] = [\n",
    "            getattr(self, modeltype), lshapes, output_units, name_fcn, is_train]\n",
    "    reuse = False\n",
    "    if usemodel:\n",
    "        modeltype = usemodel[0]\n",
    "        existing_idx, lshapes, output_units, name_fcn, is_train = STORED_MODELS[usemodel[1]]\n",
    "        old_idx = getattr(self, modeltype)\n",
    "        setattr(self, modeltype, existing_idx)\n",
    "        reuse = True\n",
    "    h = [x]\n",
    "    h.append(tf.nn.leaky_relu(tf.layers.dense(\n",
    "        h[-1], lshapes[0], name=name_fcn(), reuse = reuse)))\n",
    "    init = 0\n",
    "    for size in lshapes:\n",
    "        if not init:\n",
    "            init = 1\n",
    "            h2 = h[-1]\n",
    "        h.append(tf.nn.leaky_relu(h[-1] + tf.layers.dense(\n",
    "            h2, size, name=name_fcn(), reuse = reuse#,\n",
    "#             kernel_initializer = tf.keras.initializers.Orthogonal(gain=0.1))\n",
    "        )))\n",
    "        #h[-1] = tf.layers.batch_normalization(h[-1], momentum = .9, training = is_train)\n",
    "        h2 = tf.concat((x, h[-1]), -1)\n",
    "    hout = tf.concat((h[len(h)//2:]), -1)\n",
    "    output = tf.layers.dense(\n",
    "        hout, output_units, name=name_fcn(), reuse = reuse) \n",
    "#         kernel_initializer = tf.keras.initializers.Orthogonal(gain=0.1),\n",
    "#         bias_initializer = tf.keras.initializers.zeros())\n",
    "    #output = tf.layers.batch_normalization(output, momentum = .9, training = is_train)\n",
    "    if output_units == 1:\n",
    "        output = tf.squeeze(output, -1)\n",
    "    if usemodel:\n",
    "        setattr(self, modeltype, old_idx)\n",
    "    return output, leaky_tanh(output) * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_state(state, action, style = 'np'):\n",
    "    together = FCNS[style]['concat']((state, action), -1)\n",
    "    if style == 'tf':\n",
    "        return together\n",
    "#         expanded = FCNS[style]['expand'](\n",
    "#             together, 1)\n",
    "    else:\n",
    "        expanded = FCNS[style]['expand'](\n",
    "            together, 0)\n",
    "    return expanded\n",
    "    return FCNS[style]['reshape'](\n",
    "        together, (state.shape[0], -1, state.shape[-1] + action.shape[-1]))\n",
    "        \n",
    "    \n",
    "\n",
    "def accumulate_state(state, action, old_state, statedecay, style = 'np'):\n",
    "    new_state = make_state(state, action, style)\n",
    "    new_state_len = new_state.shape[-1]\n",
    "    if style == 'tf':\n",
    "        vel = new_state - old_state[:,:,0,:new_state_len]\n",
    "    else:\n",
    "        vel = new_state - old_state[0,:new_state_len]\n",
    "    new_state = FCNS[style]['concat']((new_state, vel), -1)\n",
    "    if style == 'tf':\n",
    "        return FCNS[style]['concat']((tf.expand_dims(new_state, 2), old_state[:,:,:-1,:]*statedecay), 2)\n",
    "    return FCNS[style]['concat']((new_state, old_state[:-1,:]*statedecay), 0)\n",
    "\n",
    "def leaky_tanh(x):\n",
    "    return tf.nn.tanh(x*30)/10 + tf.nn.tanh(x*2)/2 + tf.nn.tanh(x/10) * 2 + x * 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py:1714: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class PolicyLearner(object):\n",
    "    def __init__(self, ob_space, ac_space, take_weights_here=None, \n",
    "                 lshapes = [64 * SIZE_MULT] * N_LAYERS, config = None, \n",
    "                 lshapes_small = [32 * SIZE_MULT] * N_LAYERS,\n",
    "                 lshapes_smaller = [32 * SIZE_MULT] * N_LAYERS):\n",
    "        self.a_idx = 0\n",
    "        self.c_idx = 0\n",
    "        self.m_idx = 0\n",
    "        self.v_idx = 0\n",
    "#         self.vraw_idx = 0\n",
    "        self.sess = tf.InteractiveSession(config=config)\n",
    "        self.obs_raw = tf.placeholder(tf.float32, (None, None, NUM_HISTORY, N_STATE))\n",
    "        self.returns = tf.placeholder(tf.float32, (None, None))\n",
    "        self.returnsdecayed = tf.placeholder(tf.float32, (None, None))\n",
    "        self.mask = tf.placeholder(tf.float32, (None, None))\n",
    "        self.lr = tf.placeholder_with_default(1e-3, (None))\n",
    "        self.statesraw = tf.placeholder(tf.float32, (None, None, N_OBS))\n",
    "        self.is_train = tf.placeholder_with_default(True, (None))\n",
    "        self.obs = tf.reshape(\n",
    "            self.obs_raw, (\n",
    "                tf.shape(self.obs_raw)[0], tf.shape(self.obs_raw)[1], N_STATE * NUM_HISTORY))\n",
    "        \n",
    "        self.raw_actions, self.actions = MLP(\n",
    "            self.obs, lshapes, N_ACT, self.a_name, self.is_train, self = self,\n",
    "            storemodel = ['a_idx', 'action'])\n",
    "        if len(self.actions.shape) == 2:\n",
    "            self.actions = tf.expand_dims(self.actions, -1)\n",
    "        \n",
    "        _, self.state_value_estimate = MLP(\n",
    "            self.obs, lshapes_smaller, 1, self.v_name, self.is_train, self = self,\n",
    "            storemodel = ['v_idx', 'state_value'], scale = 2 * REWARD_MULT)\n",
    "        \n",
    "        self.advantage = ((\n",
    "            self.state_value_estimate[:,1:] * GAMMA + self.returns[:,:-1]) -\n",
    "            self.state_value_estimate[:,:-1])\n",
    "        \n",
    "        self.critic_input = tf.concat((self.obs, self.actions), -1)\n",
    "        \n",
    "        _, self.advantage_estimator = MLP(\n",
    "            self.critic_input, lshapes_small, 1, self.c_name, self.is_train, self = self,\n",
    "            storemodel = ['c_idx', 'critic'], scale = 5 * REWARD_MULT)\n",
    "        _, self.model_state_estimate = MLP(\n",
    "            self.critic_input, lshapes, N_OBS, self.m_name, self.is_train)\n",
    "        self.future_obs_raw = accumulate_state(\n",
    "            self.model_state_estimate, self.actions, self.obs_raw, STATE_DECAY, style = 'tf')\n",
    "        \n",
    "        self.future_obs = tf.reshape(self.future_obs_raw, \n",
    "            (tf.shape(self.future_obs_raw)[0],tf.shape(self.future_obs_raw)[1], N_STATE * NUM_HISTORY))\n",
    "        \n",
    "        _, self.model_state_value = MLP(\n",
    "            self.future_obs, lshapes_smaller, 1, \n",
    "            self.v_name, self.is_train, self = self,\n",
    "            usemodel = ['v_idx', 'state_value'], scale = 2 * REWARD_MULT)\n",
    "        \n",
    "\n",
    "        _, self.future_actions = MLP(\n",
    "            self.future_obs, lshapes, N_ACT, self.a_name, self.is_train, self = self,\n",
    "            usemodel = ['a_idx', 'action'])\n",
    "        if len(self.future_actions.shape) == 2:\n",
    "            self.future_actions = tf.expand_dims(self.future_actions, -1)\n",
    "        \n",
    "        self.future_critic_input = tf.concat((self.future_obs, self.future_actions), -1)\n",
    "        _, self.future_advantage_estimator = MLP(\n",
    "            self.future_critic_input, lshapes_small, 1, self.c_name, self.is_train, self = self,\n",
    "            usemodel = ['c_idx', 'critic'], scale = 5 * REWARD_MULT)\n",
    "        \n",
    "        if len(self.future_actions.shape) == 2:\n",
    "            self.future_actions = tf.expand_dims(self.future_actions, -1)\n",
    "        \n",
    "        self.t_vars = tf.trainable_variables()\n",
    "        self.a_vars = [var for var in self.t_vars if 'a_' in var.name]\n",
    "        self.v_vars = [var for var in self.t_vars if 'v_' in var.name]\n",
    "        self.c_vars = [var for var in self.t_vars if 'c_' in var.name]\n",
    "        self.m_vars = [var for var in self.t_vars if 'm_' in var.name]\n",
    "        \n",
    "        self.creg, self.areg, self.vreg, self.mreg = [\n",
    "            tf.reduce_mean([tf.reduce_mean(tf.square(v)) for v in optvars]) * 1e0\n",
    "            for optvars in \n",
    "            [self.c_vars, self.a_vars, self.v_vars, self.m_vars]]\n",
    "        self.maskexpanded = tf.expand_dims(self.mask, -1)\n",
    "        \n",
    "        self.m_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.model_state_estimate[:,:-1] - self.statesraw[:,1:]) * self.maskexpanded[:,:-1]) * 100\n",
    "        self.m_loss = self.m_loss_raw + self.mreg\n",
    "        \n",
    "        self.v_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.returnsdecayed - self.state_value_estimate) * self.mask)\n",
    "        self.v_loss =  self.v_loss_raw + self.vreg\n",
    "        \n",
    "        self.c_loss_raw = tf.reduce_mean(tf.square(\n",
    "            self.advantage_estimator[:,:-1] - self.advantage) * self.mask[:,:-1])\n",
    "        self.c_loss =  self.c_loss_raw + self.creg\n",
    "        \n",
    "        \n",
    "        self.actionmean = tf.reduce_sum(\n",
    "            self.actions * self.maskexpanded, 1) / tf.reduce_sum(\n",
    "            self.maskexpanded, 1) + tf.reduce_sum(\n",
    "            self.future_actions * self.maskexpanded, 1) / tf.reduce_sum(\n",
    "            self.maskexpanded, 1) \n",
    "        \n",
    "        \n",
    "        self.actionmean = tf.reduce_sum(\n",
    "            self.actions * self.maskexpanded, 1) / tf.reduce_sum(\n",
    "            self.maskexpanded, 1)\n",
    "        self.actdiffsquared = tf.square(\n",
    "            self.actions - tf.expand_dims(self.actionmean, 1))\n",
    "        self.actvar = tf.reduce_sum(\n",
    "            self.actdiffsquared * self.maskexpanded, 1)/ tf.reduce_sum(\n",
    "            self.maskexpanded, 1)\n",
    "        self.actstd = self.actvar\n",
    "        self.actstdpenalty = -tf.reduce_mean(tf.log(self.actstd + .00001) + tf.log(1.00001 - self.actstd)) * .001\n",
    "        \n",
    "        self.aregmeanbyaction = tf.reduce_mean(\n",
    "            tf.square(self.actionmean)) * .01\n",
    "        self.a_mse = tf.reduce_mean(tf.square(\n",
    "            self.actions + self.future_actions + self.raw_actions)) * 1e-2\n",
    "        self.aregtotal = self.areg + self.aregmeanbyaction + self.actstdpenalty + self.a_mse\n",
    "        \n",
    "        self.frac_not_masked = tf.reduce_mean(self.mask)\n",
    "        \n",
    "        self.a_loss_critic = -tf.reduce_mean(\n",
    "            self.advantage_estimator * self.mask)/self.frac_not_masked * 3\n",
    "        self.a_loss_model = -tf.reduce_mean(\n",
    "            self.model_state_value * self.mask)/self.frac_not_masked * 8\n",
    "        self.a_loss_secondaction = -tf.reduce_mean(\n",
    "            self.future_advantage_estimator * self.mask)/self.frac_not_masked * 20\n",
    "        self.a_loss_raw = self.a_loss_critic + self.a_loss_model + \\\n",
    "            self.a_loss_secondaction\n",
    "        \n",
    "        self.a_loss = self.a_loss_raw + self.aregtotal\n",
    "        \n",
    "        self.grad_v = tf.square(tf.gradients(self.state_value_estimate * self.mask, self.obs)[0])\n",
    "        self.grad_m = tf.square(tf.gradients(self.model_state_estimate * self.maskexpanded, self.critic_input)[0])\n",
    "        self.grad_c = tf.square(tf.gradients(self.advantage_estimator * self.mask, self.critic_input)[0])\n",
    "        self.grad_norm_m = tf.reduce_mean(self.grad_m) * 1e1\n",
    "        self.grad_norm_v = tf.reduce_mean(self.grad_v) * 1e0\n",
    "        self.grad_norm_c = tf.reduce_mean(self.grad_c) * 1e1\n",
    "    \n",
    "        slopes = tf.reduce_sum(tf.square(self.grad_c), reduction_indices=[2])\n",
    "        self.grad_c_1 = tf.reduce_mean(self.mask * (slopes - .1) ** 2) * 1e2\n",
    "        slopes = tf.reduce_sum(tf.square(self.grad_m), reduction_indices=[2])\n",
    "        self.grad_m_1 = tf.reduce_mean(self.mask * (slopes - .1) ** 2) * 1e2\n",
    "       \n",
    "        self.a_loss_minimize = self.a_loss\n",
    "        self.c_loss_minimize = self.c_loss + self.grad_norm_c + self.grad_c_1\n",
    "        self.v_loss_minimize = self.v_loss + self.grad_norm_v\n",
    "        self.m_loss_minimize = self.m_loss + self.grad_norm_m + self.grad_m_1\n",
    "        \n",
    "        self.critic_opt = tf.train.AdamOptimizer(self.lr/3, beta1= .8)\n",
    "        self.value_opt = tf.train.AdamOptimizer(self.lr/3, beta1= .8)\n",
    "        self.actor_opt = tf.train.AdamOptimizer(self.lr, beta1= .8)\n",
    "        self.model_opt = tf.train.AdamOptimizer(self.lr/3, beta1= .8)\n",
    "        \n",
    "        def get_grad_norm(optimizer, loss):\n",
    "            gvs = optimizer.compute_gradients(loss)\n",
    "            grad_norm = tf.reduce_mean(\n",
    "                [tf.reduce_mean(tf.square(grad)) for\n",
    "                 grad, var in gvs if grad is not None])\n",
    "            return grad_norm\n",
    "        \n",
    "        if TESTING_GRAD_NORMS:\n",
    "            self.a_grads = [\n",
    "                get_grad_norm(self.actor_opt, l) for l in [\n",
    "                self.a_loss_minimize, self.a_loss_raw, self.a_loss_critic,self.a_loss_model,\n",
    "                self.a_loss_secondaction,\n",
    "                self.areg, self.aregmeanbyaction, self.actstdpenalty\n",
    "            ]]\n",
    "            self.v_grads = [get_grad_norm(self.value_opt, l) for l in [\n",
    "                self.v_loss_minimize, self.v_loss_raw, \n",
    "                self.vreg, self.grad_norm_v\n",
    "            ]]\n",
    "            self.c_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "                self.c_loss_minimize, self.c_loss_raw, \n",
    "                self.creg, self.grad_norm_c, self.grad_c_1\n",
    "\n",
    "            ]]\n",
    "            self.m_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "                self.m_loss_minimize, self.m_loss_raw, \n",
    "                self.mreg, self.grad_norm_m, self.grad_m_1\n",
    "            ]]\n",
    "        \n",
    "        self.copt, self.c_norm = apply_clipped_optimizer(\n",
    "            self.critic_opt, self.c_loss_minimize, var_list = self.c_vars)\n",
    "        self.vopt, self.v_norm = apply_clipped_optimizer(\n",
    "            self.value_opt, self.v_loss_minimize, var_list = self.v_vars)\n",
    "        self.aopt, self.a_norm = apply_clipped_optimizer(\n",
    "            self.actor_opt, self.a_loss_minimize, var_list = self.a_vars)\n",
    "        self.mopt, self.m_norm = apply_clipped_optimizer(\n",
    "            self.model_opt, self.m_loss_minimize, var_list = self.m_vars)\n",
    "\n",
    "    def a_name(self):\n",
    "        self.a_idx += 1\n",
    "        return 'a_' + str(self.a_idx)\n",
    "    def c_name(self):\n",
    "        self.c_idx += 1\n",
    "        return 'c_' + str(self.c_idx)\n",
    "    def v_name(self):\n",
    "        self.v_idx += 1\n",
    "        return 'v_' + str(self.v_idx)\n",
    "    def m_name(self):\n",
    "        self.m_idx += 1\n",
    "        return 'm_' + str(self.m_idx)\n",
    "    \n",
    "    def load_weights(self):\n",
    "        feed_dict = {}\n",
    "        for (var, w), ph in zip(\n",
    "            self.assigns, self.weight_assignment_placeholders):\n",
    "            feed_dict[ph] = w\n",
    "        self.sess.run(self.weight_assignment_nodes, feed_dict=feed_dict)\n",
    "\n",
    "    def act(self, obs):\n",
    "        # Because we need batch dimension, \n",
    "        # data[None] changes shape from [A] to [1,A]\n",
    "        a = self.sess.run(\n",
    "            self.actions, feed_dict={\n",
    "                self.obs_raw:np.reshape(obs, (1, 1, N_HISTORY, N_STATE)),\n",
    "                self.is_train:False\n",
    "            })\n",
    "        return a[0][0]  # return first in batch\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    inter_op_parallelism_threads=0,\n",
    "    intra_op_parallelism_threads=0,\n",
    "    device_count = { \"GPU\": 0 } )\n",
    "tf.reset_default_graph()\n",
    "\n",
    "pi = PolicyLearner(env.observation_space, env.action_space, config = config)\n",
    "\n",
    "sess = pi.sess\n",
    "self = pi\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#trainer = ZooPolicyTensorflow(\n",
    "# \"mymodel1\", env.observation_space, env.action_space)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()\n",
    "\n",
    "\n",
    "# env = gym.make(envname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#         self.advantage = ((\n",
    "#             self.stateraw_value_estimate[:,1:] * GAMMA + self.returns) -\n",
    "#             self.state_value_estimate[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ah, sh, shraw, rh, rdecayedh, maskh = [\n",
    "    np.zeros((0, 0, i)) for i in [N_ACT, INPUT_UNITS, N_OBS, 1, 1, 1]]\n",
    "globalframes = []\n",
    "localframes = []\n",
    "ep = 0\n",
    "trained = 0\n",
    "ongoing = 0\n",
    "printfreq = 10\n",
    "obj_fname = envname + str(PERCENT_CHOOSE_OPTIMAL) + 'saveobjs_unguided.pkl'\n",
    "tffile = \"tmp/\" + envname + str(PERCENT_CHOOSE_OPTIMAL) + \"unguided_trained.ckpt\"\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ongoing = 0\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ongoing:\n",
    "    \n",
    "    save_path = saver.save(sess, tffile)\n",
    "    print('saved at epoch', ep)\n",
    "    with open(obj_fname,\"wb\") as f:\n",
    "        pickle.dump(\n",
    "            [ah, sh, rh, rdecayedh, maskh, ep, globalframes\n",
    "            ], f)\n",
    "    trained = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if trained:\n",
    "    try:\n",
    "        saver.restore(sess, tffile)\n",
    "        with open(obj_fname, \"rb\") as f:\n",
    "            ah, sh, shraw, rh, rdecayedh, maskh, ep, globalframes = pickle.load(f)\n",
    "        print('restored from save file')\n",
    "    except:\n",
    "        print('no save file detected')\n",
    "        trained = 0\n",
    "MAX_SEQ_LEN = 5000\n",
    "for ep in range(ep, 10000000):\n",
    "    if ep % 100 == 0 and trained and ep > 0:\n",
    "        save_path = saver.save(sess, tffile)\n",
    "        print('saved at epoch', ep)\n",
    "        with open(obj_fname,\"wb\") as f:\n",
    "            pickle.dump(\n",
    "                [ah[-1000:], sh[-1000:], shraw[-1000:],\n",
    "                 rh[-1000:], rdecayedh[-1000:], maskh[-1000:], ep, globalframes\n",
    "                ], f)\n",
    "    trained = 1\n",
    "    ongoing = 1\n",
    "    \n",
    "    an, sn, snraw, rn, rdecayedn, maskn = [\n",
    "        np.zeros((0, i)) for i in [N_ACT, INPUT_UNITS, N_OBS, 1, 1, 1]]\n",
    "    frame = 0\n",
    "    score = 0\n",
    "    restart_delay = 5\n",
    "    obs = env.reset()\n",
    "    obsraw = obs\n",
    "    snraw = np.concatenate((snraw, obs.reshape(1, -1)), 0)\n",
    "    obs = np.concatenate((obs, np.zeros(N_ACT)))\n",
    "    obs = np.concatenate((obs, np.zeros_like(obs)))\n",
    "    obs_mat = np.concatenate((\n",
    "        obs[None,:],np.zeros((NUM_HISTORY-1, N_STATE))), 0)\n",
    "    rn = [0]\n",
    "    done_ctr = 0\n",
    "    sn = np.concatenate((sn, obs_mat.reshape(1, -1)), 0)\n",
    "    step_num = 0\n",
    "    step_shifted = -1\n",
    "    show_shift = 0\n",
    "    if np.random.rand() > .5:\n",
    "        prob_random = PERCENT_CHOOSE_OPTIMAL#2#.95#1 - 1/np.sqrt(ep+1)\n",
    "    else:\n",
    "        prob_random = 2\n",
    "    while 1:\n",
    "        step_num += 1\n",
    "        a = pi.act(obs_mat.flatten())\n",
    "        if step_shifted == -1: #always act on your own\n",
    "            shifted = 1\n",
    "            if np.random.rand() > prob_random and RANDOM_STYLE_COMPLEX:\n",
    "                a = np.random.randn(*a.shape)*.01\n",
    "            elif np.random.rand() > prob_random and RANDOM_STYLE_COMPLEX:\n",
    "                a = a + np.random.randn(*a.shape) * .5\n",
    "                #a = a + np.random.randn(*a.shape)/np.sqrt(np.sqrt(ep + 2))*4\n",
    "            elif np.random.rand() > prob_random and RANDOM_STYLE_COMPLEX:\n",
    "                impact = np.random.binomial(size=a.shape, n=1, p= 0.5) * 2\n",
    "                if np.random.rand() > .5:\n",
    "                    a = a + impact\n",
    "                else:\n",
    "                    a = a - impact\n",
    "            elif np.random.rand() > prob_random and RANDOM_STYLE_COMPLEX:\n",
    "                impact = np.random.binomial(size=a.shape, n=1, p= 0.5) * 2 - 1\n",
    "                a = a + impact\n",
    "            elif np.random.rand() > prob_random and RANDOM_STYLE_COMPLEX:\n",
    "                a = a * .5\n",
    "            elif np.random.rand() > prob_random and RANDOM_STYLE_COMPLEX:\n",
    "                a = a * 2\n",
    "            elif np.random.rand() > prob_random and not RANDOM_STYLE_COMPLEX:\n",
    "                a = (np.random.binomial(size=a.shape, n=1, p= 0.5) * 2 - 1\n",
    "                    ) * np.random.randn()\n",
    "            else:\n",
    "                shifted = 0\n",
    "            if shifted:\n",
    "                step_shifted = step_num\n",
    "        an = np.concatenate((an, a[None,:]), 0)\n",
    "        snraw = np.concatenate((snraw, obsraw[None,:]), 0)\n",
    "        last_obs = obs\n",
    "#         last_obsraw = obsraw\n",
    "        obs, r, done, _ = env.step(a)\n",
    "        obsraw = obs\n",
    "        r = r + 1 * REWARD_MULT\n",
    "\n",
    "        obs_mat = accumulate_state(\n",
    "            obs, a, obs_mat, STATE_DECAY, style = 'np')\n",
    "        \n",
    "        rn.append(r)\n",
    "        sn = np.concatenate((sn, obs_mat.reshape(1, -1)), 0)\n",
    "        score += r\n",
    "        frame += 1\n",
    "        still_open = env.render(\"human\")\n",
    "        if done:\n",
    "            done_ctr += 1\n",
    "            if done_ctr > 3:\n",
    "                if ep % MAX_SEQ_LEN == 0:\n",
    "                    print('score', score, ' frames', frame)\n",
    "                break\n",
    "        if still_open==False:\n",
    "            crashhere\n",
    "        if not done: continue\n",
    "        if restart_delay==0:\n",
    "            print(\"score=%0.2f in %i frames\" % (score, frame))\n",
    "            if still_open!=True:      # not True in multiplayer or non-Roboschool \n",
    "                break\n",
    "            restart_delay = 2000*2  # 2 sec at 60 fps\n",
    "        restart_delay -= 1\n",
    "        if restart_delay==0: \n",
    "            break\n",
    "    a = pi.act(obs_mat.flatten())\n",
    "    an = np.concatenate((an, a[None,:]), 0)\n",
    "    localframes.append(frame)\n",
    "    rn = np.array(rn)\n",
    "    rn[-1] = rn[-1] - 20 * REWARD_MULT\n",
    "    rewards = [0]\n",
    "    for ir in rn[::-1]:\n",
    "        rewards.append(rewards[-1] * GAMMA + ir)\n",
    "    rdecayedn = np.array(rewards)[:0:-1]\n",
    "    lenrdec = len(rdecayedn)\n",
    "#     rdecayedn = rdecayedn + lenrdec\n",
    "#     rdecayedn = rdecayedn - np.arange(lenrdec) * 1.8\n",
    "    maskn = np.ones_like(rn)\n",
    "    if step_shifted > -1:\n",
    "        if step_shifted == 0:\n",
    "            raise ValueError('step shifted not allowed 0')\n",
    "        maskn[:step_shifted - 1] = 0\n",
    "    if ep == 0:\n",
    "        ah, sh, shraw, rh, rdecayedh, maskh = [\n",
    "            np.expand_dims(v, 0) for v in [an, sn, snraw, rn,rdecayedn, maskn]]\n",
    "    else:\n",
    "        def get_updated_h(h, n, third_dim):\n",
    "            hshape = h.shape[1]\n",
    "            nshape = n.shape[0]\n",
    "            if third_dim:\n",
    "                if hshape > nshape:\n",
    "                    n = np.concatenate((n, np.zeros((hshape - nshape, n.shape[-1]))), 0)\n",
    "                if nshape > hshape:\n",
    "                    h = np.concatenate((h, np.zeros((\n",
    "                        h.shape[0], nshape - hshape, h.shape[-1]))), 1)\n",
    "            else:\n",
    "                if hshape > nshape:\n",
    "                    n = np.concatenate((n, np.zeros((hshape - nshape))), 0)\n",
    "                if nshape > hshape:\n",
    "                    h = np.concatenate((h, np.zeros((h.shape[0], nshape - hshape))), 1)\n",
    "            h = np.concatenate((h, np.expand_dims(n, 0)), 0)\n",
    "            return h\n",
    "            \n",
    "        ah, sh, shraw = [get_updated_h(h, n, 1) for  h, n in zip(\n",
    "            [ah, sh, shraw], [an, sn, snraw])]\n",
    "        \n",
    "        rh, rdecayedh, maskh = [\n",
    "            get_updated_h(h, n, 0) for h, n in zip(\n",
    "                [rh, rdecayedh, maskh], [rn, rdecayedn, maskn])]\n",
    "        \n",
    "    if ep % 1 == 0 and ep > 20:\n",
    "        ah, sh, shraw, rh,rdecayedh, maskh = [\n",
    "            v[-400:] for v in [ah, sh, shraw, rh,rdecayedh, maskh]]\n",
    "        globalframes.append(np.mean(localframes))\n",
    "        localframes = []\n",
    "        batch_size = 16\n",
    "        if ep < batch_size:\n",
    "            batch_size = ep\n",
    "        num_hist = ah.shape[0]\n",
    "        for itr in range(1):\n",
    "            if num_hist >  batch_size:\n",
    "                forced_hist = 2\n",
    "                #probability = np.arange(num_hist - forced_hist)\n",
    "                probability = num_steps_per_run = (\n",
    "                    maskh.shape[1] - maskh[:,::-1].argmax(1))[:-forced_hist]\n",
    "                probability = np.square(probability)\n",
    "                probability = probability / probability.sum()\n",
    "                samples = np.concatenate((\n",
    "                    np.random.choice(\n",
    "                        num_hist - forced_hist, batch_size - forced_hist, \n",
    "                        replace=False, p=probability),\n",
    "                    np.arange(\n",
    "                        num_hist - forced_hist, num_hist)))\n",
    "            else:\n",
    "                samples = np.random.choice(num_hist, num_hist, replace=False)\n",
    "            actions, states, statesraw, returns, returnsdecayed, mask = [\n",
    "                v[samples] for v in [ah, sh, shraw, rh,rdecayedh, maskh]]\n",
    "            feed_dict={\n",
    "                        self.returns:returns,\n",
    "                        self.statesraw: statesraw,\n",
    "                        self.returnsdecayed:returnsdecayed,\n",
    "                        self.lr: .01 / np.power(ep + 20, .4),\n",
    "                        self.mask:mask,\n",
    "                        self.obs_raw: states.reshape(*states.shape[:2], N_HISTORY, N_STATE)\n",
    "            }\n",
    "            _, aloss = sess.run(\n",
    "                [self.aopt, self.a_loss],\n",
    "                feed_dict = feed_dict\n",
    "                    )\n",
    "            if TESTING_GRAD_NORMS and ep % printfreq == 0:\n",
    "                a_grad_norms = self.sess.run(self.a_grads, feed_dict)\n",
    "                print('a grad norms', a_grad_norms)\n",
    "            feed_dict[self.actions] = actions\n",
    "            \n",
    "            _,_, _, mloss, closs, vloss = sess.run(\n",
    "                [self.mopt, self.copt,self.vopt, self.m_loss, self.c_loss, self.v_loss],\n",
    "                    feed_dict=feed_dict)\n",
    "            if 1:\n",
    "                if ep % printfreq == 0:\n",
    "                    print('aloss', aloss, 'closs', closs, 'vloss', vloss)\n",
    "                    if TESTING_GRAD_NORMS:\n",
    "                        v_grad_norms, c_grad_norms, m_grad_norms = self.sess.run(\n",
    "                            [self.v_grads, self.c_grads, self.m_grads], feed_dict)\n",
    "                        print('v grad norms', v_grad_norms)\n",
    "                        print('c grad norms', c_grad_norms)\n",
    "                        print('m grad norms', m_grad_norms)\n",
    "        if ep % printfreq == 0:\n",
    "            print(' ep, ', ep, ' avg frames',np.mean(globalframes[-20:]))\n",
    "            print('abs action',np.abs(ah)[-1,0,:].shape, np.abs(ah)[-1,0,:].mean())\n",
    "            print('max reward',np.max(rh[-10:,10:]))\n",
    "        if ep % 10000 == 0:\n",
    "            clear_output()\n",
    "        if ep % 100 == 0:\n",
    "            ysmoothed = gaussian_filter1d(globalframes, sigma=4)\n",
    "            plt.plot(ysmoothed)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah[-1][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            self.a_grads = [\n",
    "                get_grad_norm(self.actor_opt, l) for l in [\n",
    "                self.a_loss_minimize, self.a_loss_raw, self.a_loss_critic,self.a_loss_model,\n",
    "                self.a_loss_secondaction,\n",
    "                self.areg, self.aregmeanbyaction, self.actstdpenalty\n",
    "            ]]\n",
    "            self.v_grads = [get_grad_norm(self.value_opt, l) for l in [\n",
    "                self.v_loss_minimize, self.v_loss_raw, \n",
    "                self.vreg, self.grad_norm_v\n",
    "            ]]\n",
    "            self.c_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "                self.c_loss_minimize, self.c_loss_raw, \n",
    "                self.creg, self.grad_norm_c, self.grad_c_1\n",
    "\n",
    "            ]]\n",
    "            self.m_grads = [get_grad_norm(self.critic_opt, l) for l in [\n",
    "                self.m_loss_minimize, self.m_loss_raw, \n",
    "                self.mreg, self.grad_norm_m, self.grad_m_1\n",
    "            ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.shape, statesraw.shape, returnsdecayed.shape, mask.shape, \\\n",
    "    states.reshape(*states.shape[:2], N_HISTORY, N_STATE).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.sess.run(self.actions, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.sess.run(self.obs, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.sess.run(self.a_loss_secondaction, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.sess.run(self.future_actions, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.sess.run(self.maskexpanded, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                        self.returns:returns,\n",
    "                        self.statesraw: statesraw,\n",
    "                        self.returnsdecayed:returnsdecayed,\n",
    "                        self.lr: .01 / np.power(ep + 20, .4),\n",
    "                        self.mask:mask,\n",
    "                        self.obs_raw: states.reshape(*states.shape[:2], N_HISTORY, N_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.obs_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape, obs.shape, obs_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
